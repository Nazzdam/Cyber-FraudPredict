{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063b9db8",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "# Fraud & Cyber-Threat Prediction — End-to-End Notebook\n",
    "Goal:Predict fraudulent / cyber-risky transactions using transaction metadata, behavioral features, and market stress indicators.\n",
    "\n",
    "Author: Milani Chikeka  \n",
    "\n",
    "Seed:42\n",
    "---\n",
    "Sections\n",
    "1. Setup & imports  \n",
    "2. Load dataset (Kaggle `creditcard.csv` recommended) or simulate synthetic transactions  \n",
    "3. Market stress synthetic enrichment (USD/ZAR returns, VIX proxy, repo rate changes)  \n",
    "4. Feature engineering (behavioral + transaction + stress features)  \n",
    "5. Train/test split & imbalance handling  \n",
    "6. Models: Logistic Regression baseline + LightGBM (main)  \n",
    "7. Evaluation: ROC, PR, confusion matrix, business metrics  \n",
    "8. Explainability: SHAP plots  \n",
    "9. Save model & preprocessing pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87bd9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setups and imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings  \n",
    "#import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (roc_auc_score, precision_recall_curve,average_precision_score ,confusion_matrix,classification_report)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Optional libraies and seedings\n",
    "\"\"\" try:\n",
    "    import lightgbm as lgb\n",
    "except Exception as e:\n",
    "    print(\"install lightgbm: pip install lightgbm\")\n",
    "    raise e  \"\"\"\n",
    "\n",
    "try:\n",
    "    import shap \n",
    "except Exception as e:\n",
    "    print(\"install shap: pip install shap\")\n",
    "    raise e\n",
    "\n",
    "#This is for imbalanbce handling.\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except Exception as e:\n",
    "    print(\"Install imbalanced-learn: pip install imbalanced-learn\")    \n",
    "    raise e\n",
    "\n",
    "RandomSeed=42\n",
    "np.random.mtrand.RandomState(RandomSeed)\n",
    "\n",
    "#Style plotting\n",
    "sns.set(style=\"whitegrid\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e60d9",
   "metadata": {},
   "source": [
    "#2Load the date/create synthetic data.\n",
    "-Load the Kaggle dataset `creditcard.csv`, put it in `./data/creditcard.csv`.\n",
    "-If dataset is not present, create 'synthetic' transaction datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb81502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not found. Will create a synthetic dataset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>amount</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>Browser</th>\n",
       "      <th>MerchantCategory</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14471</td>\n",
       "      <td>2020-01-01 00:44:08</td>\n",
       "      <td>44.629745</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Health</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18543</td>\n",
       "      <td>2020-01-01 01:08:44</td>\n",
       "      <td>18.118901</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11296</td>\n",
       "      <td>2020-01-01 03:20:34</td>\n",
       "      <td>227.148690</td>\n",
       "      <td>0</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Retail</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14340</td>\n",
       "      <td>2020-01-01 03:48:04</td>\n",
       "      <td>115.137749</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Travel</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16600</td>\n",
       "      <td>2020-01-01 06:23:47</td>\n",
       "      <td>27.577263</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Retail</td>\n",
       "      <td>ZA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   users           timestamp      amount  is_fraud DeviceType Browser  \\\n",
       "0  14471 2020-01-01 00:44:08   44.629745         0     Mobile  Safari   \n",
       "1  18543 2020-01-01 01:08:44   18.118901         0     Mobile  Chrome   \n",
       "2  11296 2020-01-01 03:20:34  227.148690         0    Desktop  Chrome   \n",
       "3  14340 2020-01-01 03:48:04  115.137749         0     Mobile  Chrome   \n",
       "4  16600 2020-01-01 06:23:47   27.577263         0     Tablet  Safari   \n",
       "\n",
       "  MerchantCategory Country  \n",
       "0           Health      US  \n",
       "1    Entertainment     IND  \n",
       "2           Retail      US  \n",
       "3           Travel     IND  \n",
       "4           Retail      ZA  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In this cell, the dataset is loaded and basic EDA is performed.\n",
    "DataDir=\"data\"\n",
    "os.makedirs(DataDir,exist_ok=True)\n",
    "Dataset=os.path.join(DataDir, \"./my_dataset.csv\") #Relative path to dataset\n",
    "\n",
    "if os.path.exists(Dataset):\n",
    "    print(\"Loading dataset from local directory.\")\n",
    "    df=pd.read_csv(Dataset)\n",
    "    #Kaggle datasets has \"TIME\", \"Amount\" , and \"Class\" columns.\n",
    "    #Create synthetic categorical features for demonstration.\n",
    "    df=df.reset_index(drop=True)\n",
    "    #A synthetic timeframe will be created\n",
    "    StartDate=datetime(2020,1,1)\n",
    "    df['timestamp']=df['Time'].apply(lambda x: StartDate + timedelta(seconds=int(x)))\n",
    "    #Categorical features are created\n",
    "    df['DeviceType']=np.random.choice(['Mobile','Desktop','Tablet'], size=10, p=[0.5,0.1,0.4])\n",
    "    df['Browser']=np.random.choice(['Chrome', 'Firefox', 'Safari', 'Edge'], size=10, p=[0.4,0.1,0.4,0.1])\n",
    "    df['MerchantCategory']=np.random.choice(['Retail', 'Food', 'Travel', 'Entertainment','Health'], size=10, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "    df['Country']=np.random.choice(['ZA','UK', 'US', 'CHN', 'IND'], size=10, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "    df=df.rename(columns={'Amount':'amount','Class':'is_fraud'})\n",
    "    \n",
    "else:\n",
    "    print(\"Dataset not found. Will create a synthetic dataset.\")\n",
    "    n=20000\n",
    "    #Simulate the users base\n",
    "    users=np.random.randint(1,20000, size=n)\n",
    "    StartDate=datetime(2020,1,1)\n",
    "    timestamps=[StartDate + timedelta(seconds=int(x)) \n",
    "                for x in np.random.exponential(scale=3600, size=n).cumsum()]\n",
    "    amounts=np.random.exponential(scale=100, size=n)\n",
    "#Labeling transactions as fraud or not based on amount and random noise\n",
    "IsFraud=(np.random.rand(n) < 0.002).astype(int) #Around 0.2% frauds\n",
    "DeviceType=np.random.choice(['Mobile','Desktop', 'Tablet'], size=n, p=[0.5,0.1,0.4])\n",
    "Browser=np.random.choice(['Chrome', 'Firefox', 'Safari', 'Edge'], size=n, p=[0.4,0.1,0.4,0.1])\n",
    "MerchantCategory=np.random.choice(['Retail', 'Food', 'Travel', 'Entertainment','Health'], size=n, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "Country=np.random.choice(['ZA','UK', 'US', 'CHN', 'IND'], size=n, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "\n",
    "df=pd.DataFrame({\n",
    "    'users': users,\n",
    "    'timestamp': timestamps,\n",
    "    'amount': amounts,\n",
    "    'is_fraud': IsFraud,\n",
    "    'DeviceType': DeviceType,\n",
    "    'Browser': Browser,\n",
    "    'MerchantCategory': MerchantCategory,\n",
    "    'Country': Country,\n",
    "})\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea330e2e",
   "metadata": {},
   "source": [
    " #3 Synthetic Market Stress Enrichment\n",
    "We will create a daily market stress series with:\n",
    "- USD/ZAR returns\n",
    "- VIX proxy (global vol)\n",
    "- SARB repo rate change flags\n",
    "\n",
    "Then merge the daily metrics onto each transaction by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9469cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>UsdZar</th>\n",
       "      <th>VixIndex</th>\n",
       "      <th>RepoRate</th>\n",
       "      <th>UsdZarReturn</th>\n",
       "      <th>UsdZarReturn7DayVol</th>\n",
       "      <th>UsdZar7DayMean</th>\n",
       "      <th>MarketStress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17.363157</td>\n",
       "      <td>25.992017</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.992017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>18.007726</td>\n",
       "      <td>17.467312</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.037123</td>\n",
       "      <td>0.026250</td>\n",
       "      <td>21.729665</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>18.804779</td>\n",
       "      <td>22.723656</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.044262</td>\n",
       "      <td>0.023763</td>\n",
       "      <td>22.060995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>20.164843</td>\n",
       "      <td>24.190020</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.072325</td>\n",
       "      <td>0.029785</td>\n",
       "      <td>22.593251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>21.146965</td>\n",
       "      <td>18.283866</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.048705</td>\n",
       "      <td>0.026201</td>\n",
       "      <td>21.731374</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date     UsdZar   VixIndex  RepoRate  UsdZarReturn  \\\n",
       "0 2020-01-01  17.363157  25.992017    0.0675      0.000000   \n",
       "1 2020-01-02  18.007726  17.467312    0.0675      0.037123   \n",
       "2 2020-01-03  18.804779  22.723656    0.0675      0.044262   \n",
       "3 2020-01-04  20.164843  24.190020    0.0675      0.072325   \n",
       "4 2020-01-05  21.146965  18.283866    0.0675      0.048705   \n",
       "\n",
       "   UsdZarReturn7DayVol  UsdZar7DayMean  MarketStress  \n",
       "0             0.000000       25.992017             0  \n",
       "1             0.026250       21.729665             1  \n",
       "2             0.023763       22.060995             1  \n",
       "3             0.029785       22.593251             1  \n",
       "4             0.026201       21.731374             1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create market stress data\n",
    "#create a date range that covres the transaction timestamps\n",
    "min_date=df['timestamp'].min().date()\n",
    "max_date=df['timestamp'].max().date()\n",
    "dates=pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "# Simulate USD/ZAR daily returns (random walk with occasional shocks)\n",
    "np.random.seed(42)\n",
    "UsdZarLog=np.random.normal(loc=0, scale=0.01, size=len(dates))\n",
    "#Add random shocks\n",
    "Shock=np.random.choice(len(dates), size=int(len(dates)*0.05*len(dates)), replace=True)\n",
    "UsdZarLog[Shock] += np.random.normal(loc=0.05, scale=0.02, size=len(Shock))\n",
    "\n",
    "UsdZarLog=16.76*np.exp(np.cumsum(UsdZarLog)) #Starting rate around 16.76\n",
    "\n",
    "#VIX index simulation\n",
    "VixIndex=np.abs(np.random.normal(loc=12, size=len(dates)))\n",
    "VixIndex[Shock] += np.random.normal(10, 5, size=len(Shock))\n",
    "VixIndex=np.clip(VixIndex,10,None)\n",
    "\n",
    "#repo rate simulation\n",
    "RepoRate=np.full(len(dates),0.0675 ) #starts at 6.75%\n",
    "ChangeIndixes=np.random.choice(len(dates), size=int(0.002*len(dates)), replace=False)\n",
    "for idx in ChangeIndixes:\n",
    "    RepoRate[idx:] += np.random.choice([0.25, -0.25, 0.5, -0.5]) #Changes in basis points\n",
    "\n",
    "MarketData=pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'UsdZar': UsdZarLog,\n",
    "    'VixIndex': VixIndex,\n",
    "    'RepoRate': RepoRate\n",
    "})\n",
    "\n",
    "#Compute the returns and volatility\n",
    "MarketData['UsdzarReturn']=MarketData['UsdZar'].pct_change().fillna(0)\n",
    "MarketData['UsdZarReturn7DayVol']=MarketData['UsdzarReturn'].rolling(7, min_periods=1).std().fillna(0)\n",
    "MarketData['UsdZar7DayMean']=MarketData['VixIndex'].rolling(7, min_periods=1).mean().fillna(MarketData['VixIndex'])\n",
    "#Mark the stress when UsdZar is less than -2% or VIX index is above 20 or Repo rate above 8%\n",
    "MarketData['MarketStress']=((MarketData['UsdzarReturn'].abs() > 0.02) | (MarketData['VixIndex'] > MarketData['VixIndex'].quantile(0.90))).astype(int)\n",
    "MarketData.rename(columns={'UsdzarReturn': 'UsdZarReturn'}, inplace=True)\n",
    "\n",
    "MarketData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c26c13",
   "metadata": {},
   "source": [
    "Merge market features into transactions (by date).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d598669c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>amount</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>Browser</th>\n",
       "      <th>MerchantCategory</th>\n",
       "      <th>Country</th>\n",
       "      <th>date</th>\n",
       "      <th>UsdZar</th>\n",
       "      <th>VixIndex</th>\n",
       "      <th>RepoRate</th>\n",
       "      <th>UsdZarReturn</th>\n",
       "      <th>UsdZarReturn7DayVol</th>\n",
       "      <th>UsdZar7DayMean</th>\n",
       "      <th>MarketStress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14471</td>\n",
       "      <td>2020-01-01 00:44:08</td>\n",
       "      <td>44.629745</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Health</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17.363157</td>\n",
       "      <td>25.992017</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.992017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18543</td>\n",
       "      <td>2020-01-01 01:08:44</td>\n",
       "      <td>18.118901</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>IND</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17.363157</td>\n",
       "      <td>25.992017</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.992017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11296</td>\n",
       "      <td>2020-01-01 03:20:34</td>\n",
       "      <td>227.148690</td>\n",
       "      <td>0</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Retail</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17.363157</td>\n",
       "      <td>25.992017</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.992017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14340</td>\n",
       "      <td>2020-01-01 03:48:04</td>\n",
       "      <td>115.137749</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Travel</td>\n",
       "      <td>IND</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17.363157</td>\n",
       "      <td>25.992017</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.992017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16600</td>\n",
       "      <td>2020-01-01 06:23:47</td>\n",
       "      <td>27.577263</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Retail</td>\n",
       "      <td>ZA</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17.363157</td>\n",
       "      <td>25.992017</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.992017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   users           timestamp      amount  is_fraud DeviceType Browser  \\\n",
       "0  14471 2020-01-01 00:44:08   44.629745         0     Mobile  Safari   \n",
       "1  18543 2020-01-01 01:08:44   18.118901         0     Mobile  Chrome   \n",
       "2  11296 2020-01-01 03:20:34  227.148690         0    Desktop  Chrome   \n",
       "3  14340 2020-01-01 03:48:04  115.137749         0     Mobile  Chrome   \n",
       "4  16600 2020-01-01 06:23:47   27.577263         0     Tablet  Safari   \n",
       "\n",
       "  MerchantCategory Country       date     UsdZar   VixIndex  RepoRate  \\\n",
       "0           Health      US 2020-01-01  17.363157  25.992017    0.0675   \n",
       "1    Entertainment     IND 2020-01-01  17.363157  25.992017    0.0675   \n",
       "2           Retail      US 2020-01-01  17.363157  25.992017    0.0675   \n",
       "3           Travel     IND 2020-01-01  17.363157  25.992017    0.0675   \n",
       "4           Retail      ZA 2020-01-01  17.363157  25.992017    0.0675   \n",
       "\n",
       "   UsdZarReturn  UsdZarReturn7DayVol  UsdZar7DayMean  MarketStress  \n",
       "0           0.0                  0.0       25.992017             0  \n",
       "1           0.0                  0.0       25.992017             0  \n",
       "2           0.0                  0.0       25.992017             0  \n",
       "3           0.0                  0.0       25.992017             0  \n",
       "4           0.0                  0.0       25.992017             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merge market data with transaction data by date\n",
    "df['date']=pd.to_datetime(df['timestamp'].dt.date)\n",
    "df=df.merge(MarketData, on='date', how='left')\n",
    "\n",
    "#Fill empty market data for missing dates\n",
    "df[['UsdZar', 'VixIndex', 'RepoRate', 'UsdZarReturn', 'UsdZarReturn7DayVol', 'UsdZar7DayMean', 'MarketStress']] = \\\n",
    "    df[['UsdZar', 'VixIndex', 'RepoRate', 'UsdZarReturn', 'UsdZarReturn7DayVol', 'UsdZar7DayMean', 'MarketStress']].fillna(method='ffill').fillna(0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b7cab",
   "metadata": {},
   "source": [
    "#4 Feature engineering — transactional & behavioral\n",
    "We create:\n",
    "- time features (hour, weekday)\n",
    "- log amount\n",
    "- rolling features per user (1h / 24h counts, avg amounts)\n",
    "- device-country mismatch flag\n",
    "- anomaly score: amount compared to user's historical mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d5a930e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,\n",
       "        Amount  AmountLog  hour  weekday  UserTxCount1h  UserTxCount24h  \\\n",
       " 0   66.641413   4.214220    12        0              0               0   \n",
       " 1   31.087422   3.468464     8        5              0               0   \n",
       " 2  148.332614   5.006176    15        6              0               0   \n",
       " 3  204.339807   5.324666     5        0              0               0   \n",
       " 4  161.564199   5.091073     3        6              0               0   \n",
       " \n",
       "    UserAmountMean  AmtZScore DeviceType MerchantCategory Country  \\\n",
       " 0       48.864417   0.707107     Tablet           Retail      US   \n",
       " 1       48.864417  -0.707107     Tablet           Travel     CHN   \n",
       " 2      148.332614   0.000000     Mobile    Entertainment      ZA   \n",
       " 3      204.339807   0.000000     Mobile             Food      ZA   \n",
       " 4       92.330605   0.707107    Desktop           Travel     IND   \n",
       " \n",
       "    UsdZarReturn  UsdZarReturn7DayVol   VixIndex  UsdZar7DayMean  RepoRate  \\\n",
       " 0      0.033909             0.022707  23.812631       20.337198    0.0675   \n",
       " 1      0.052424             0.011597  31.240414       24.936163    0.0675   \n",
       " 2     -0.005397             0.025110  18.212516       18.821125   -0.4325   \n",
       " 3      0.056811             0.016158  20.090583       22.672392    0.0675   \n",
       " 4      0.055316             0.020649  29.565596       25.765022    0.0675   \n",
       " \n",
       "    MarketStress  IsNightTransaction  DeviceCountryMismatch  \n",
       " 0             1                   0                      1  \n",
       " 1             1                   0                      0  \n",
       " 2             0                   0                      0  \n",
       " 3             1                   1                      0  \n",
       " 4             1                   1                      0  )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature Engineering\n",
    "# Remove duplicate columns before proceeding\n",
    "df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
    "\n",
    "# Ensure timestamp is datetime dtype, then extract time features\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "# Create Amount column to match downstream expected naming and keep log\n",
    "if 'Amount' not in df.columns:\n",
    "    df['Amount'] = df['amount']\n",
    "df['AmountLog'] = np.log1p(df['Amount'])\n",
    "\n",
    "#Sort by user and timestamp and ensure UserId exists\n",
    "if 'UserId' not in df.columns:\n",
    "    df['UserId'] = np.random.randint(1, 20000, size=len(df))\n",
    "df = df.sort_values(by=['UserId', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Rolling features (user-level)\n",
    "# Compute rolling counts in the past 1 hour and 24 hours per user using two-pointer approach\n",
    "def BuildUsersRollingFeatures(df, seconds_window, col_name):\n",
    "    Out = []\n",
    "    # iterate groups in the same order as df (df already sorted by UserId and timestamp)\n",
    "    for uid, group in df.groupby('UserId', sort=False):\n",
    "        # seconds since epoch as numpy array for fast indexing\n",
    "        Times = (group['timestamp'].astype('int64') // 1_000_000_000).to_numpy()\n",
    "        counts = []\n",
    "        left = 0\n",
    "        for i, t in enumerate(Times):\n",
    "            while left < i and (t - Times[left]) > seconds_window:\n",
    "                left += 1\n",
    "            counts.append(i - left)\n",
    "        Out.extend(counts)\n",
    "    # assign (length should match df)\n",
    "    df[col_name] = Out\n",
    "\n",
    "# create 1h and 24h user tx count columns\n",
    "BuildUsersRollingFeatures(df, seconds_window=3600, col_name='UserTxCount1h')\n",
    "BuildUsersRollingFeatures(df, seconds_window=86400, col_name='UserTxCount24h')\n",
    "\n",
    "# user historical stats (means and std) using the Amount column\n",
    "UserStats = df.groupby('UserId')['Amount'].agg(['mean','std']).rename(columns={'mean':'UserAmountMean','std':'UserAmountStd'})\n",
    "# merge using left_on UserId and right_index since UserStats uses UserId as index\n",
    "df = df.merge(UserStats, left_on='UserId', right_index=True, how='left')\n",
    "\n",
    "# If merge created suffixed columns due to prior columns, ensure canonical names exist\n",
    "# prefer explicit names if present, otherwise take suffixed variants\n",
    "if 'UserAmountMean' not in df.columns:\n",
    "    for c in ['UserAmountMean_x', 'UserAmountMean_y']:\n",
    "        if c in df.columns:\n",
    "            df['UserAmountMean'] = df[c]\n",
    "            break\n",
    "if 'UserAmountStd' not in df.columns:\n",
    "    for c in ['UserAmountStd_x', 'UserAmountStd_y']:\n",
    "        if c in df.columns:\n",
    "            df['UserAmountStd'] = df[c]\n",
    "            break\n",
    "\n",
    "# Amount anomaly score (z-score) - guard divide-by-zero and missing std\n",
    "df['UserAmountStd'] = df['UserAmountStd'].replace(0, np.nan)\n",
    "df['AmtZScore'] = (df['Amount'] - df['UserAmountMean']) / df['UserAmountStd']\n",
    "df['AmtZScore'] = df['AmtZScore'].fillna(0)\n",
    "\n",
    "# device-country mismatch flag: assume user primary country is mode country (safe mode extraction)\n",
    "UserCountry = df.groupby('UserId')['Country'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan).rename('UserPrimaryCountry')\n",
    "df = df.merge(UserCountry, on='UserId', how='left')\n",
    "# use the correct merged column name 'UserPrimaryCountry'\n",
    "df['DeviceCountryMismatch'] = (df['Country'] != df['UserPrimaryCountry']).astype(int)\n",
    "\n",
    "# Flagging for night transactions (between 12 AM to 6 AM) -- correct boolean logic\n",
    "df['IsNightTransaction'] = df['hour'].apply(lambda x: 1 if (x > 0 and x < 6) else 0)\n",
    "\n",
    "# Short-term rolling features - ensure feature list matches created columns\n",
    "FeatCots = ['Amount', 'AmountLog', 'hour', 'weekday', 'UserTxCount1h', 'UserTxCount24h',\n",
    "            'UserAmountMean','AmtZScore','DeviceType','MerchantCategory','Country',\n",
    "            'UsdZarReturn','UsdZarReturn7DayVol','VixIndex','UsdZar7DayMean','RepoRate','MarketStress','IsNightTransaction','DeviceCountryMismatch']\n",
    "\n",
    "len(df), df[FeatCots].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91e420d",
   "metadata": {},
   "source": [
    " #5 Prepare training dataset\n",
    "- Choose modeling features.\n",
    "- Encode categoricals using a ColumnTransformer pipeline.\n",
    "- Train/test split stratified by label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "367317b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (16000, 22) Test shape: (4000, 22)\n",
      "Fraud rate in train: 0.0025 in test: 0.0025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell: 6 - prepare dataset for modeling\n",
    "LabelCol='IsFradud'\n",
    "#Some datasets have different naming conventions for the label column.\n",
    "if LabelCol not in df.columns:\n",
    "    LabelCol='Class' if 'Class' in df.columns else 'is_fraud'\n",
    "    Y=df[LabelCol].astype(int)\n",
    "#Select feautures and target\n",
    "NumFeatures=['Amount', 'AmountLog', 'hour', 'weekday', 'UserTxCount1h', 'UserTxCount24h', \n",
    "             'UserAmountMean','AmtZScore','DeviceType','MerchantCategory','Country',\n",
    "             'UsdZarReturn','UsdZarReturn7DayVol','VixIndex','UsdZar7DayMean','RepoRate','MarketStress','IsNightTransaction','DeviceCountryMismatch']\n",
    "CatFeatures=['DeviceType','MerchantCategory','Country']\n",
    "X=df[NumFeatures + CatFeatures].copy()\n",
    "\n",
    "#Handle infinites and NaNs\n",
    "X=X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "#Train-test split\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X,Y, test_size=0.2, stratify=Y, random_state=RandomSeed)\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "print(\"Fraud rate in train:\", Y_train.mean(), \"in test:\", Y_test.mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f3676b",
   "metadata": {},
   "source": [
    "Preprocessing pipeline\n",
    "- Standard scale numerical features\n",
    "- One-hot encode small-cardinality categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a008d458",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Selected columns, ['Amount', 'AmountLog', 'hour', 'weekday', 'UserTxCount1h', 'UserTxCount24h', 'UserAmountMean', 'AmtZScore', 'DeviceType', 'MerchantCategory', 'Country', 'UsdZarReturn', 'UsdZarReturn7DayVol', 'VixIndex', 'UsdZar7DayMean', 'RepoRate', 'MarketStress', 'IsNightTransaction', 'DeviceCountryMismatch'], are not unique in dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      9\u001b[39m Preprocessor=ColumnTransformer(transformers=[(\u001b[33m'\u001b[39m\u001b[33mNum\u001b[39m\u001b[33m'\u001b[39m, NumericTransormer, NumFeatures),\n\u001b[32m     10\u001b[39m                                             (\u001b[33m'\u001b[39m\u001b[33mCat\u001b[39m\u001b[33m'\u001b[39m, categorical_transformer, CatFeatures)])\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Fit-transform training set to create feature matrix for simple models\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m XtrainP=\u001b[43mPreprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m XtestP=Preprocessor.transform(X_test)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTransformed feauture shape:\u001b[39m\u001b[33m\"\u001b[39m, XtrainP.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/compose/_column_transformer.py:991\u001b[39m, in \u001b[36mColumnTransformer.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    988\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_transformers()\n\u001b[32m    989\u001b[39m n_samples = _num_samples(X)\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_remainder(X)\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/compose/_column_transformer.py:545\u001b[39m, in \u001b[36mColumnTransformer._validate_column_callables\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    543\u001b[39m         columns = columns(X)\n\u001b[32m    544\u001b[39m     all_columns.append(columns)\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m     transformer_to_input_indices[name] = \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;28mself\u001b[39m._columns = all_columns\n\u001b[32m    548\u001b[39m \u001b[38;5;28mself\u001b[39m._transformer_to_input_indices = transformer_to_input_indices\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/_indexing.py:471\u001b[39m, in \u001b[36m_get_column_indices\u001b[39m\u001b[34m(X, key)\u001b[39m\n\u001b[32m    469\u001b[39m         col_idx = all_columns.get_loc(col)\n\u001b[32m    470\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers.Integral):\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    472\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSelected columns, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, are not unique in dataframe\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    473\u001b[39m             )\n\u001b[32m    474\u001b[39m         column_indices.append(col_idx)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mValueError\u001b[39m: Selected columns, ['Amount', 'AmountLog', 'hour', 'weekday', 'UserTxCount1h', 'UserTxCount24h', 'UserAmountMean', 'AmtZScore', 'DeviceType', 'MerchantCategory', 'Country', 'UsdZarReturn', 'UsdZarReturn7DayVol', 'VixIndex', 'UsdZar7DayMean', 'RepoRate', 'MarketStress', 'IsNightTransaction', 'DeviceCountryMismatch'], are not unique in dataframe"
     ]
    }
   ],
   "source": [
    "# Cell: 7 - preprocessing pipeline\n",
    "NumericTransormer=Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "Preprocessor=ColumnTransformer(transformers=[('Num', NumericTransormer, NumFeatures),\n",
    "                                            ('Cat', categorical_transformer, CatFeatures)])\n",
    "\n",
    "# Fit-transform training set to create feature matrix for simple models\n",
    "XtrainP=Preprocessor.fit_transform(X_train)\n",
    "XtestP=Preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Transformed feauture shape:\", XtrainP.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35ee5d",
   "metadata": {},
   "source": [
    "6) Handle class imbalance & Model training\n",
    "We will:\n",
    "- Train a LogisticRegression baseline with class weights.\n",
    "- Train a LightGBM model (primary).\n",
    "- Optionally use SMOTE for oversampling (toggle below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f498796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 8 - optional SMOTE toggle (use with caution)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "UseSmote=False\n",
    "\n",
    "# Ensure RandomSeed is available when cells are run out of order\n",
    "RandomSeed = globals().get(RandomSeed, 42)\n",
    "\n",
    "if UseSmote:\n",
    "    print(\"Applying SMOTE to balance the classes in the training set.\")\n",
    "    Sm = SMOTE(random_state=RandomSeed, n_jobs=-1)\n",
    "    XtrainPRes, YtrainPRes = Sm.fit_resample(XtrainP, Y_train)\n",
    "else:\n",
    "    XtrainPRes, YtrainPRes = XtrainP, Y_train\n",
    "\n",
    "print(\"Post-resample shape:\", XtrainPRes.shape, \"Fraud rate:\", YtrainPRes.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 9 - Logistic Regression baseline\n",
    "Lr=LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RandomSeed)\n",
    "Lr.fit(XtrainPRes, YtrainPRes)\n",
    "ProbLr=Lr.predict_proba(XtestP)[:,1]\n",
    "print(\"Logistic Regression AUC-ROC:\", roc_auc_score(Y_test, ProbLr))\n",
    "print(classification_report(Y_test, Lr.predict(XtestP)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0616ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 10 - LightGBM model (with sklearn wrapper)\n",
    "# We'll use plain lgb.Dataset approach via sklearn wrapper for convenience.\n",
    "lgbm=lgb.LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    objective='binary',\n",
    "    class_weight='balanced',\n",
    "    random_state=RandomSeed,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgbm.fit(XtrainPRes, YtrainPRes,\n",
    "         eval_set=[(XtestP, Y_test)],\n",
    "         eval_metric='auc',\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=100)\n",
    "\n",
    "ProbLgbm=lgbm.predict_proba(XtestP)[:,1]\n",
    "print(\"LightGBM AUC-ROC:\", roc_auc_score(Y_test, ProbLgbm))\n",
    "print(classification_report(Y_test, lgbm.predict(XtestP)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a567fd",
   "metadata": {},
   "source": [
    "## 7) Evaluation visuals & business-oriented metrics\n",
    "- ROC, PR curves\n",
    "- Confusion matrix at chosen threshold\n",
    "- Precision@K / Lift if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#ROC\n",
    "fpr, tpr, _=roc_curve(Y_test, ProbLgbm)\n",
    "roc_auc=auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f'LightGBM (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "#precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(Y_test, ProbLgbm)\n",
    "ap = average_precision_score(Y_test, ProbLgbm)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(recall, precision, label=f'LightGBM (AP = {ap:.3f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bca217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 12 - Confusion matrix at a chosen threshold\n",
    "def conf_matrix_at_threshold(y_true, probs, thresh=0.5):\n",
    "    Preds = (probs >= thresh).astype(int)\n",
    "    Cm = confusion_matrix(y_true, Preds)\n",
    "    tn, fp, fn, tp = Cm.ravel()\n",
    "    return {'tn':tn,'fp':fp,'fn':fn,'tp':tp,'cm':Cm}\n",
    "\n",
    "th = 0.5\n",
    "cm_stats = conf_matrix_at_threshold(Y_test, ProbLgbm, thresh=th)\n",
    "print(\"Threshold:\", th)\n",
    "print(cm_stats)\n",
    "sns.heatmap(cm_stats['Cm'], annot=True, fmt='d', cmap='Blues', xticklabels=['NotFraud','Fraud'], yticklabels=['NotFraud','Fraud'])\n",
    "plt.title(f'Confusion matrix @ threshold={th}')\n",
    "plt.show()\n",
    "\n",
    "# Show classification report for thresholded predictions\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "preds_th = (ProbLgbm >= th).astype(int)\n",
    "print(\"Precision:\", precision_score(Y_test, preds_th))\n",
    "print(\"Recall:\", recall_score(Y_test, preds_th))\n",
    "print(\"F1:\", f1_score(Y_test, preds_th))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1084a37",
   "metadata": {},
   "source": [
    " Business metric: Expected Loss Avoided (toy example)\n",
    "- Suppose average fraud amount is `avg_fraud_amt`.\n",
    "- False negative = missed fraud (cost = amount)\n",
    "- False positive = manual review cost = e.g., 5 USD\n",
    "Compute net saved loss vs baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63683c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 13 - toy business metric\n",
    "AvgFraudAmt=df.loc[df[LabelCol]==1, 'amount'].mean() if df[LabelCol].sum() >0 else 100\n",
    "ManuelReviewCost=5.0\n",
    "\n",
    "tn, fp, fn, tp = cm_stats['tn'], cm_stats['fp'], cm_stats['fn'], cm_stats['tp']\n",
    "ExpectedLossMissed=fn*AvgFraudAmt\n",
    "ExpectedManualReviewCost=(tp*fp) *ManuelReviewCost\n",
    "print(f\"Avg fraud amount: {AvgFraudAmt:.2f}\")\n",
    "print(f\"Missed frauds (fn): {fn}, cost approx: {ExpectedLossMissed:.2f}\")\n",
    "print(f\"Manual reviews (tp+fp): {(tp+fp)}, cost approx: {ExpectedManualReviewCost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a44eb",
   "metadata": {},
   "source": [
    "8) Explainability — SHAP\n",
    "SHAP helps explain LightGBM predictions (global and single prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d1a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 14 - SHAP explainability\n",
    "# Create a small sample to speed shap runtime\n",
    "import shap\n",
    "SampleIndices=np.random.choice(X_test.shape[0], size=min(1000, X_test.shape[0]), replace=False)\n",
    "XShapSample=X_test.iloc[SampleIndices]\n",
    "XShapSampleP=Preprocessor.transform(XShapSample)\n",
    "\n",
    "# Build feature names after preprocessing\n",
    "# numeric names + ohe names\n",
    "NumNames=NumFeatures\n",
    "ohe=Preprocessor.named_transformers_['Cat'].named_steps['ohe']\n",
    "oheNames=ohe.get_feature_names_out(CatFeatures).tolist()\n",
    "FeatureNames=NumNames + oheNames\n",
    "\n",
    "# Use TreeExplainer for LightGBM\n",
    "Explainer=shap.TreeExplainer(lgbm)\n",
    "ShapValues=Explainer.shap_values(XShapSampleP)\n",
    "\n",
    "# Summary plot (global importance)\n",
    "shap.summary_plot(ShapValues, XShapSampleP, feature_names=FeatureNames, plot_type='bar', show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a11f92",
   "metadata": {},
   "source": [
    "SHAP waterfall for a single example\n",
    "Pick a high-risk transaction from the test set and show local explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 15 - SHAP waterfall for single transaction (fixed)\n",
    "# pick the highest predicted risk transaction in the test set\n",
    "\n",
    "HighRiskIdx = np.argmax(ProbLgbm)\n",
    "\n",
    "# Use the original (preprocessed) pandas row from X_test, then transform\n",
    "SingleRow_df = X_test.iloc[[HighRiskIdx]]\n",
    "SingleRowP = Preprocessor.transform(SingleRow_df)  # shape (1, n_features)\n",
    "\n",
    "# Compute SHAP values for the single transformed row\n",
    "SingleShap = Explainer.shap_values(SingleRowP)\n",
    "\n",
    "# Handle binary-class vs single-array shap output: pick positive-class contributions\n",
    "if isinstance(SingleShap, list) and len(SingleShap) > 1:\n",
    "    shap_vals = SingleShap[1][0]  # positive class, first (and only) sample\n",
    "else:\n",
    "    # SingleShap could be array of shape (1, n) or (n,) depending on shap version\n",
    "    shap_vals = SingleShap[0] if getattr(SingleShap, 'ndim', 1) == 2 else SingleShap\n",
    "\n",
    "# Determine base value for positive class if provided as array\n",
    "ev = Explainer.expected_value\n",
    "base_value = ev[1] if (isinstance(ev, (list, np.ndarray)) and len(ev) > 1) else ev\n",
    "\n",
    "print(\"Predicted fraud probability for the highest risk transaction:\", ProbLgbm[HighRiskIdx])\n",
    "\n",
    "# Build Explanation and plot waterfall (data is the 1D feature vector)\n",
    "shap.waterfall_plot(\n",
    "    shap.Explanation(values=shap_vals,\n",
    "                     base_values=base_value,\n",
    "                     data=SingleRowP[0],\n",
    "                     feature_names=FeatureNames),\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345cdb66",
   "metadata": {},
   "source": [
    " 9) Save model & preprocessing pipeline\n",
    "Save both the trained LightGBM model and the preprocessing pipeline for later deployment (Streamlit or API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell: 16 -  Save the model and preprocessor\n",
    "OutDIr=\"models\"\n",
    "os.makedirs(OutDIr, exist_ok=True)\n",
    "\n",
    "joblib.dump(Preprocessor, os.path.join(OutDIr, \"preprocessor.joblib\"))\n",
    "joblib.dump(lgbm, os.path.join(OutDIr, \"lgbm_model.joblib\"))\n",
    "joblib.dump(Lr, os.path.join(OutDIr, \"logistic_regression_model.joblib\"))\n",
    "\n",
    "print(\"Saved preprocessor and models to ./models/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
