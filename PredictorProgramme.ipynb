{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063b9db8",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "# Fraud & Cyber-Threat Prediction — End-to-End Notebook\n",
    "Goal:Predict fraudulent / cyber-risky transactions using transaction metadata, behavioral features, and market stress indicators.\n",
    "\n",
    "Author: Milani Chikeka  \n",
    "\n",
    "Seed:42\n",
    "---\n",
    "Sections\n",
    "1. Setup & imports  \n",
    "2. Load dataset (Kaggle `creditcard.csv` recommended) or simulate synthetic transactions  \n",
    "3. Market stress synthetic enrichment (USD/ZAR returns, VIX proxy, repo rate changes)  \n",
    "4. Feature engineering (behavioral + transaction + stress features)  \n",
    "5. Train/test split & imbalance handling  \n",
    "6. Models: Logistic Regression baseline + LightGBM (main)  \n",
    "7. Evaluation: ROC, PR, confusion matrix, business metrics  \n",
    "8. Explainability: SHAP plots  \n",
    "9. Save model & preprocessing pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd9ed7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scikitlearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscikitlearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split,StratifiedKFold\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscikitlearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler,OneHotEncoder\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscikitlearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompose\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColumnTransformer\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'scikitlearn'"
     ]
    }
   ],
   "source": [
    "#Setups and imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (roc_auc_score, precision_recall_curve,average_precision_score ,confusion_matrix,classification_report)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Optional libraies and seedings\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception as e:\n",
    "    print(\"install lightgbm: pip install lightgbm\")\n",
    "raise e\n",
    "\n",
    "try:\n",
    "    import shap \n",
    "except Exception as e:\n",
    "    print(\"install shap: pip install shap\")\n",
    "    raise e\n",
    "\n",
    "#This is for imbalanbce handling.\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except Exception as e:\n",
    "    print(\"Install imbalanced-learn: pip install imbalanced-learn\")    \n",
    "    raise e\n",
    "\n",
    "RandomSeed=42\n",
    "np.random.seed(RandomSeed)\n",
    "\n",
    "#Style plotting\n",
    "sns.set(style=\"whitegrid\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e60d9",
   "metadata": {},
   "source": [
    "#2Load the date/create synthetic data.\n",
    "-Load the Kaggle dataset `creditcard.csv`, put it in `./data/creditcard.csv`.\n",
    "-If dataset is not present, create 'synthetic' transaction datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb81502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this cell, the dataset is loaded and basic EDA is performed.\n",
    "DataDir=\"data\"\n",
    "os.makedirs(DataDir,exist_ok=True)\n",
    "Dataset=os.path.join(DataDir, \"/Users/milanichikeka/Downloads/my_dataset.csv\")\n",
    "\n",
    "if os.path.exists(Dataset):\n",
    "    print(\"Loading dataset from local directory.\")\n",
    "    df=pd.read_csv(Dataset)\n",
    "    #Kaggle datasets has \"TIME\", \"Amount\" , and \"Class\" columns.\n",
    "    #Create synthetic categorical features for demonstration.\n",
    "    df=df.reset_index(drop=True)\n",
    "    #A synthetic timeframe will be created\n",
    "    StartDate=datetime(2020,1,1)\n",
    "    df['timestamp']=df['Time'].apply(lambda x: StartDate + timedelta(seconds=int(x)))\n",
    "    #Categorical features are created\n",
    "    df['DeviceType']=np.random.choice(['Mobile','Desktop','Tablet'], size=10, p=[0.5,0.1,0.4])\n",
    "    df['Browser']=np.random.choice(['Chrome', 'Firefox', 'Safari', 'Edge'], size=10, p=[0.4,0.1,0.4,0.1])\n",
    "    df['MerchantCategory']=np.random.choice(['Retail', 'Food', 'Travel', 'Entertainment','Health'], size=10, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "    df['Country']=np.random.choice(['ZA','UK', 'US', 'CHN', 'IND'], size=10, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "    df=df.rename(columns={'Amount':'amount','Class':'is_fraud'})\n",
    "    \n",
    "else:\n",
    "    print(\"Dataset not found. Will create a synthetic dataset.\")\n",
    "    n=20000\n",
    "    #Simulate the users base\n",
    "    users=np.random.randint(1,20000, size=n)\n",
    "    StartDate=datetime(2020,1,1)\n",
    "    timestamps=[StartDate + timedelta(seconds=int(x)) \n",
    "                for x in np.random.exponential(scale=3600, size=n).cumsum()]\n",
    "    amounts=np.random.exponential(scale=100, size=n)\n",
    "#Labeling transactions as fraud or not based on amount and random noise\n",
    "IsFraud=(np.random.rand(n) < 0.002).astype(int) #Around 0.2% frauds\n",
    "DeviceType=np.random.choice(['Mobile','Desktop', 'Tablet'], size=n, p=[0.5,0.1,0.4])\n",
    "Browser=np.random.choice(['Chrome', 'Firefox', 'Safari', 'Edge'], size=n, p=[0.4,0.1,0.4,0.1])\n",
    "MerchantCategory=np.random.choice(['Retail', 'Food', 'Travel', 'Entertainment','Health'], size=n, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "Country=np.random.choice(['ZA','UK', 'US', 'CHN', 'IND'], size=n, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "\n",
    "df=pd.DataFrame({\n",
    "    'users': users,\n",
    "    'timestamp': timestamps,\n",
    "    'amount': amounts,\n",
    "    'is_fraud': IsFraud,\n",
    "    'DeviceType': DeviceType,\n",
    "    'Browser': Browser,\n",
    "    'MerchantCategory': MerchantCategory,\n",
    "    'Country': Country,\n",
    "})\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea330e2e",
   "metadata": {},
   "source": [
    " #3 Synthetic Market Stress Enrichment\n",
    "We will create a daily market stress series with:\n",
    "- USD/ZAR returns\n",
    "- VIX proxy (global vol)\n",
    "- SARB repo rate change flags\n",
    "\n",
    "Then merge the daily metrics onto each transaction by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9469cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create market stress data\n",
    "#create a date range that covres the transaction timestamps\n",
    "min_date=df['timestamp'].min().date()\n",
    "max_date=df['timestamp'].max().date()\n",
    "dates=pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "# Simulate USD/ZAR daily returns (random walk with occasional shocks)\n",
    "np.random.seed(42)\n",
    "UsdZarLog=np.random.normal(loc=0, scale=0.01, size=len(dates))\n",
    "#Add random shocks\n",
    "Shock=np.random.choice(len(dates), size=int(len(dates)*0.05*len(dates)), replace=False)\n",
    "UsdZarLog[Shock] += np.random.normal(loc=0.05, scale=0.02, size=len(Shock))\n",
    "\n",
    "UsdZarLog=16.76*np.exp(np.cumsum(UsdZarLog)) #Starting rate around 16.76\n",
    "\n",
    "#VIX index simulation\n",
    "VixIndex=np.abs(np.random.normal(loc=12, size=len(dates)))\n",
    "VixIndex[Shock] += np.random.normal(10, 5, size=len(Shock))\n",
    "VixIndex=np.clip(VixIndex,10,None)\n",
    "\n",
    "#repo rate simulation\n",
    "RepoRate=np.full(len(dates),0.0675 ) #starts at 6.75%\n",
    "ChangeIndixes=np.random.choice(len(dates), size=int(0.002*len(dates)), replace=False)\n",
    "for idx in ChangeIndixes:\n",
    "    RepoRate[idx:] += np.random.choice([0.25, -0.25, 0.5, -0.5]) #Changes in basis points\n",
    "\n",
    "MarketData=pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'UsdZar': UsdZarLog,\n",
    "    'VixIndex': VixIndex,\n",
    "    'RepoRate': RepoRate\n",
    "})\n",
    "\n",
    "#Compute the returns and volatility\n",
    "MarketData['UsdzarReturn']=MarketData['UsdZar'].pct_change().fillna(0)\n",
    "MarketData['UsdZarReturn7DayVol']=MarketData['UsdzarReturn'].rolling(7, min_periods=1).std().fillna(0)\n",
    "MarketData['UsdZar7DayMean']=MarketData['VixIndex'].rolling(7, min_periods=1).mean().fillna(MarketData['VixIndex'])\n",
    "#Mark the stress when UsdZar is less than -2% or VIX index is above 20 or Repo rate above 8%\n",
    "MarketData['MarketStress']=((MarketData['UsdzarReturn'].abs() > 0.02) | (MarketData['VixIndex'] > MarketData['VixIndex'].quantile(0.90))).astype(int)\n",
    "\n",
    "MarketData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c26c13",
   "metadata": {},
   "source": [
    "Merge market features into transactions (by date).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d598669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge market data with transaction data by date\n",
    "df['date']=pd.to_datetime(df['timestamp'].dt.date)\n",
    "df=df.merge(MarketData, on='date', how='left')\n",
    "\n",
    "#Fill empty market data for missing dates\n",
    "df['UsdZar', 'VixIndex', 'RepoRate', 'UsdZarReturn', 'UsdZarReturn7DayVol', 'UsdZar7DayMean', 'MarketStress']=df[['UsdZar', 'VixIndex', 'RepoRate', 'UsdZarReturn', 'UsdZarReturn7DayVol', 'UsdZar7DayMean', 'MarketStress']].fillna(method='ffill').fillna(0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b7cab",
   "metadata": {},
   "source": [
    "#4 Feature engineering — transactional & behavioral\n",
    "We create:\n",
    "- time features (hour, weekday)\n",
    "- log amount\n",
    "- rolling features per user (1h / 24h counts, avg amounts)\n",
    "- device-country mismatch flag\n",
    "- anomaly score: amount compared to user's historical mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "df['hour']=df.to_datetime(df['timestamp']).dt.hour\n",
    "df['weekday']=df.to_datetime(df['timestamp']).dt.weekday\n",
    "df['AmountLog']=np.log(df['amount'])\n",
    "\n",
    "#Sort by user and timestamp\n",
    "if 'UserId' not in df.columns:\n",
    "    #create fake user ids if not present in the dataset\n",
    "    df['UserId']=np.random.randint(1, 20000, size=len(df))\n",
    "    df=df.sort_values(by=['UserId', 'timestamp']).reset_index(drop=True)\n",
    "        \n",
    "# Rolling features (user-level)\n",
    "# We will compute rolling counts in the past 1 hour and 24 hours using a simple groupby + expanding approach\n",
    "# For efficiency, compute per user - using windows via timestamps       \n",
    "\n",
    "def build_users_rolling_features(df, seconds_window):\n",
    "    col_name=f'UserTransCount_{seconds_window//3600}h'\n",
    "    Out=[]\n",
    "    \n",
    "    for uid, group in df.groupby('UserId'):\n",
    "        Times=group['timestamp'].astype(np.int64) // 1_000_000_000  #Convert to seconds\n",
    "        Counts=[]\n",
    "        left=0\n",
    "        for i, t in enumerate(Times):\n",
    "        #Move left pointer to maintain the window\n",
    "            while left < i and (t - Times.iloc[left]) > seconds_window:\n",
    "                left += 1\n",
    "            Counts.append(i - left) #Number of transactions in the window\n",
    "        Out.extend(Counts)\n",
    "    df[col_name]=Out\n",
    "    \n",
    "#user historical stats(means and std)    \n",
    "UserStats=df.groupby('UserId')['amount'].agg(['mean','std']).rename(columns={'mean':'UserAmountMean','std':'UserAmountStd'})\n",
    "df=df.merge(UserStats, on='UserId', right_index=True, how='left')\n",
    "\n",
    "# Amount anomaly score (z-score)\n",
    "df['AmtZScore']=(df['amount']-df['UserAmountMean'])/df['UserAmountStd'].replace(0,np.nan)\n",
    "df['AmtZScore']=df['AmtZScore'].fillna(0)\n",
    "\n",
    "# device-country mismatch flag: assume user primary country is mode country\n",
    "UserCountry=df.groupby('UserId')['Country'].agg(lambda x: x.mode()[0]).rename('UserPrimaryCountry')\n",
    "df=df.merge(UserCountry, on='UserId', right_index=True, how='left')\n",
    "df['DeviceCountryMismatch']=(df['Country'] != df['UserPrimaryCountry']).astype(int)\n",
    "\n",
    "#Flagging for night transactions (between 12 AM to 6 AM)\n",
    "df['IsNightTransaction']=df['hour'].apply(lambda x: 1 if (x >0 & x < 6) else 0)\n",
    "\n",
    "#Short-term rolling features\n",
    "FeatCots=['Amount', 'AmountLog', 'hour', 'weekday', 'user_tx_count_1h', 'user_tx_count_24h', \n",
    "             'UserAmountMean','AmtZScore','DeviceType','MerchantCategory','Country',\n",
    "             'UsdZarReturn','UsdZarReturn7DayVol','VixIndex','UsdZar7DayMean','RepoRate','MarketStress','IsNightTransaction','DeviceCountryMismatch']\n",
    "len(df), df[FeatCots].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91e420d",
   "metadata": {},
   "source": [
    " #5 Prepare training dataset\n",
    "- Choose modeling features.\n",
    "- Encode categoricals using a ColumnTransformer pipeline.\n",
    "- Train/test split stratified by label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367317b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell: 6 - prepare dataset for modeling\n",
    "LabelCol='IsFradud'\n",
    "#Some datasets have different naming conventions for the label column.\n",
    "if LabelCol not in df.columns:\n",
    "    LabelCol='Class' if 'Class' in df.columns else 'is_fraud'\n",
    "    Y=df[LabelCol].astype(int)\n",
    "#Select feautures and target\n",
    "NumFeatures=['Amount', 'AmountLog', 'hour', 'weekday', 'user_tx_count_1h', 'user_tx_count_24h', \n",
    "             'UserAmountMean','AmtZScore','DeviceType','MerchantCategory','Country',\n",
    "             'UsdZarReturn','UsdZarReturn7DayVol','VixIndex','UsdZar7DayMean','RepoRate','MarketStress','IsNightTransaction','DeviceCountryMismatch']\n",
    "CatFeatures=['device_type','merchant_category','country']\n",
    "X=df[NumFeatures + CatFeatures].copy()\n",
    "\n",
    "#Handle infinites and NaNs\n",
    "X=X.replace([np.inf, -np.inf], np.nan).fillana(0)\n",
    "\n",
    "#Train-test split\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X,Y, test_size=0.2, stratify=Y, random_state=RandomSeed)\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "print(\"Fraud rate in train:\", Y_train.mean(), \"in test:\", Y_test.mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f3676b",
   "metadata": {},
   "source": [
    "Preprocessing pipeline\n",
    "- Standard scale numerical features\n",
    "- One-hot encode small-cardinality categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a008d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 7 - preprocessing pipeline\n",
    "NumericTransormer=Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "Preprocessor=ColumnTransformer(transformers=[('Num', NumericTransormer, NumFeatures),\n",
    "                                            ('Cat', categorical_transformer, CatFeatures)])\n",
    "\n",
    "# Fit-transform training set to create feature matrix for simple models\n",
    "XtrainP=Preprocessor.fit_transform(X_train)\n",
    "XtestP=Preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Transformed feauture shape:\", XtrainP.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35ee5d",
   "metadata": {},
   "source": [
    "6) Handle class imbalance & Model training\n",
    "We will:\n",
    "- Train a LogisticRegression baseline with class weights.\n",
    "- Train a LightGBM model (primary).\n",
    "- Optionally use SMOTE for oversampling (toggle below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f498796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 8 - optional SMOTE toggle (use with caution)\n",
    "UseSmote=False\n",
    "if UseSmote:\n",
    "    print(\"Applying SMOTE to balance the classes in the training set.\")\n",
    "    Sm=SMOTE(random_state=RandomSeed, n_jobs=-1)\n",
    "    XtrainPRes, YtrainPRes=Sm.fit_resample(XtrainP, Y_train)\n",
    "else:\n",
    "    XtrainPRes, YtrainPRes=XtrainP, Y_train\n",
    "print(\"Post-resample shape:\", XtrainPRes.shape, \"Fraud rate:\", YtrainPRes.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 9 - Logistic Regression baseline\n",
    "Lr=LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RandomSeed)\n",
    "Lr.fit(XtrainPRes, YtrainPRes)\n",
    "ProbLr=Lr.predict_proba(XtestP)[:,1]\n",
    "print(\"Logistic Regression AUC-ROC:\", roc_auc_score(Y_test, ProbLr))\n",
    "print(classification_report(Y_test, Lr.predict(XtestP)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0616ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 10 - LightGBM model (with sklearn wrapper)\n",
    "# We'll use plain lgb.Dataset approach via sklearn wrapper for convenience.\n",
    "lgbm=lgb.LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    objective='binary',\n",
    "    class_weight='balanced',\n",
    "    random_state=RandomSeed,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgbm.fit(XtrainPRes, YtrainPRes,\n",
    "         eval_set=[(XtestP, Y_test)],\n",
    "         eval_metric='auc',\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=100)\n",
    "\n",
    "ProbLgbm=lgbm.predict_proba(XtestP)[:,1]\n",
    "print(\"LightGBM AUC-ROC:\", roc_auc_score(Y_test, ProbLgbm))\n",
    "print(classification_report(Y_test, lgbm.predict(XtestP)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a567fd",
   "metadata": {},
   "source": [
    "## 7) Evaluation visuals & business-oriented metrics\n",
    "- ROC, PR curves\n",
    "- Confusion matrix at chosen threshold\n",
    "- Precision@K / Lift if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#ROC\n",
    "fpr, tpr, _=roc_curve(Y_test, ProbLgbm)\n",
    "roc_auc=auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f'LightGBM (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "#precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(Y_test, ProbLgbm)\n",
    "ap = average_precision_score(Y_test, ProbLgbm)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(recall, precision, label=f'LightGBM (AP = {ap:.3f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bca217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 12 - Confusion matrix at a chosen threshold\n",
    "def conf_matrix_at_threshold(y_true, probs, thresh=0.5):\n",
    "    Preds = (probs >= thresh).astype(int)\n",
    "    Cm = confusion_matrix(y_true, Preds)\n",
    "    tn, fp, fn, tp = Cm.ravel()\n",
    "    return {'tn':tn,'fp':fp,'fn':fn,'tp':tp,'cm':cm}\n",
    "\n",
    "th = 0.5\n",
    "cm_stats = conf_matrix_at_threshold(Y_test, ProbLgbm, thresh=th)\n",
    "print(\"Threshold:\", th)\n",
    "print(cm_stats)\n",
    "sns.heatmap(cm_stats['Cm'], annot=True, fmt='d', cmap='Blues', xticklabels=['NotFraud','Fraud'], yticklabels=['NotFraud','Fraud'])\n",
    "plt.title(f'Confusion matrix @ threshold={th}')\n",
    "plt.show()\n",
    "\n",
    "# Show classification report for thresholded predictions\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "preds_th = (ProbLgbm >= th).astype(int)\n",
    "print(\"Precision:\", precision_score(Y_test, preds_th))\n",
    "print(\"Recall:\", recall_score(Y_test, preds_th))\n",
    "print(\"F1:\", f1_score(Y_test, preds_th))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1084a37",
   "metadata": {},
   "source": [
    " Business metric: Expected Loss Avoided (toy example)\n",
    "- Suppose average fraud amount is `avg_fraud_amt`.\n",
    "- False negative = missed fraud (cost = amount)\n",
    "- False positive = manual review cost = e.g., 5 USD\n",
    "Compute net saved loss vs baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63683c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 13 - toy business metric\n",
    "AvgFraudAmt=df.loc[df[LabelCol]==1, 'amount'].mean() if df[LabelCol].sum() >0 else 100\n",
    "ManuelReviewCost=5.0\n",
    "\n",
    "tn, fp, fn, tp = cm_stats['tn'], cm_stats['fp'], cm_stats['fn'], cm_stats['tp']\n",
    "ExpectedLossMissed=fn*AvgFraudAmt\n",
    "ExpectedManualReviewCost=(tp*fp) *ManuelReviewCost\n",
    "print(f\"Avg fraud amount: {AvgFraudAmt:.2f}\")\n",
    "print(f\"Missed frauds (fn): {fn}, cost approx: {ExpectedLossMissed:.2f}\")\n",
    "print(f\"Manual reviews (tp+fp): {(tp+fp)}, cost approx: {ExpectedManualReviewCost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a44eb",
   "metadata": {},
   "source": [
    "8) Explainability — SHAP\n",
    "SHAP helps explain LightGBM predictions (global and single prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d1a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 14 - SHAP explainability\n",
    "# Create a small sample to speed shap runtime\n",
    "XShapSample=XtestP.sample(n=min(1000, len(XtestP)), random_state=RandomSeed)\n",
    "XShapSampleP=Preprocessor.transform(XShapSample)\n",
    "\n",
    "# Build feature names after preprocessing\n",
    "# numeric names + ohe names\n",
    "\n",
    "NumNames=NumFeatures\n",
    "ohe=Preprocessor.named_transformers_['Cat'].named_steps['ohe']\n",
    "oheNames=ohe.get_feature_names_out(CatFeatures).tolist()\n",
    "FeatureNames=NumNames + oheNames\n",
    "\n",
    "# Use TreeExplainer for LightGBM\n",
    "Explainer=shap.TreeExplainer(lgbm)\n",
    "ShapValues=Explainer.shap_values(XShapSampleP)\n",
    "\n",
    "# Summary plot (global importance)\n",
    "shap.summary_plot(ShapValues, XShapSampleP, feature_names=FeatureNames, plot_type='bar', show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a11f92",
   "metadata": {},
   "source": [
    "SHAP waterfall for a single example\n",
    "Pick a high-risk transaction from the test set and show local explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 15 - SHAP waterfall for single transaction (fixed)\n",
    "# pick the highest predicted risk transaction in the test set\n",
    "\n",
    "HighRiskIdx = np.argmax(ProbLgbm)\n",
    "\n",
    "# Use the original (preprocessed) pandas row from X_test, then transform\n",
    "SingleRow_df = X_test.iloc[[HighRiskIdx]]\n",
    "SingleRowP = Preprocessor.transform(SingleRow_df)  # shape (1, n_features)\n",
    "\n",
    "# Compute SHAP values for the single transformed row\n",
    "SingleShap = Explainer.shap_values(SingleRowP)\n",
    "\n",
    "# Handle binary-class vs single-array shap output: pick positive-class contributions\n",
    "if isinstance(SingleShap, list) and len(SingleShap) > 1:\n",
    "    shap_vals = SingleShap[1][0]  # positive class, first (and only) sample\n",
    "else:\n",
    "    # SingleShap could be array of shape (1, n) or (n,) depending on shap version\n",
    "    shap_vals = SingleShap[0] if getattr(SingleShap, 'ndim', 1) == 2 else SingleShap\n",
    "\n",
    "# Determine base value for positive class if provided as array\n",
    "ev = Explainer.expected_value\n",
    "base_value = ev[1] if (isinstance(ev, (list, np.ndarray)) and len(ev) > 1) else ev\n",
    "\n",
    "print(\"Predicted fraud probability for the highest risk transaction:\", ProbLgbm[HighRiskIdx])\n",
    "\n",
    "# Build Explanation and plot waterfall (data is the 1D feature vector)\n",
    "shap.waterfall_plot(\n",
    "    shap.Explanation(values=shap_vals,\n",
    "                     base_values=base_value,\n",
    "                     data=SingleRowP[0],\n",
    "                     feature_names=FeatureNames),\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345cdb66",
   "metadata": {},
   "source": [
    " 9) Save model & preprocessing pipeline\n",
    "Save both the trained LightGBM model and the preprocessing pipeline for later deployment (Streamlit or API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell: 16 -  Save the model and preprocessor\n",
    "OutDIr=\"models\"\n",
    "os.makedirs(OutDIr, exist_ok=True)\n",
    "\n",
    "joblib.dump(Preprocessor, os.path.join(OutDIr, \"preprocessor.joblib\"))\n",
    "joblib.dump(lgbm, os.path.join(OutDIr, \"lgbm_model.joblib\"))\n",
    "joblib.dump(Lr, os.path.join(OutDIr, \"logistic_regression_model.joblib\"))\n",
    "\n",
    "print(\"Saved preprocessor and models to ./models/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
