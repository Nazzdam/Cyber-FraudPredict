{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063b9db8",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "# Fraud & Cyber-Threat Prediction — End-to-End Notebook\n",
    "Goal:Predict fraudulent / cyber-risky transactions using transaction metadata, behavioral features, and market stress indicators.\n",
    "\n",
    "Author: Milani Chikeka  \n",
    "\n",
    "Seed:42\n",
    "---\n",
    "Sections\n",
    "1. Setup & imports  \n",
    "2. Load dataset (Kaggle `creditcard.csv` recommended) or simulate synthetic transactions  \n",
    "3. Market stress synthetic enrichment (USD/ZAR returns, VIX proxy, repo rate changes)  \n",
    "4. Feature engineering (behavioral + transaction + stress features)  \n",
    "5. Train/test split & imbalance handling  \n",
    "6. Models: Logistic Regression baseline + LightGBM (main)  \n",
    "7. Evaluation: ROC, PR, confusion matrix, business metrics  \n",
    "8. Explainability: SHAP plots  \n",
    "9. Save model & preprocessing pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87bd9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setups and imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings  \n",
    "#import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (roc_auc_score, precision_recall_curve,average_precision_score ,confusion_matrix,classification_report)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Optional libraies and seedings\n",
    "\"\"\" try:\n",
    "    import lightgbm as lgb\n",
    "except Exception as e:\n",
    "    print(\"install lightgbm: pip install lightgbm\")\n",
    "    raise e  \"\"\"\n",
    "\n",
    "try:\n",
    "    import shap \n",
    "except Exception as e:\n",
    "    print(\"install shap: pip install shap\")\n",
    "    raise e\n",
    "\n",
    "#This is for imbalanbce handling.\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except Exception as e:\n",
    "    print(\"Install imbalanced-learn: pip install imbalanced-learn\")    \n",
    "    raise e\n",
    "\n",
    "RandomSeed=42\n",
    "np.random.mtrand.RandomState(RandomSeed)\n",
    "\n",
    "#Style plotting\n",
    "sns.set(style=\"whitegrid\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e60d9",
   "metadata": {},
   "source": [
    "#2Load the date/create synthetic data.\n",
    "-Load the Kaggle dataset `creditcard.csv`, put it in `./data/creditcard.csv`.\n",
    "-If dataset is not present, create 'synthetic' transaction datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cb81502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not found. Will create a synthetic dataset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>amount</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>Browser</th>\n",
       "      <th>MerchantCategory</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4709</td>\n",
       "      <td>2020-01-01 00:58:54</td>\n",
       "      <td>29.116379</td>\n",
       "      <td>0</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>ZA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15617</td>\n",
       "      <td>2020-01-01 03:39:53</td>\n",
       "      <td>159.980988</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Health</td>\n",
       "      <td>ZA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5696</td>\n",
       "      <td>2020-01-01 04:07:11</td>\n",
       "      <td>58.172305</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Retail</td>\n",
       "      <td>CHN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2183</td>\n",
       "      <td>2020-01-01 04:53:54</td>\n",
       "      <td>48.700885</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Retail</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7633</td>\n",
       "      <td>2020-01-01 06:24:25</td>\n",
       "      <td>176.650399</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Travel</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   users           timestamp      amount  is_fraud DeviceType Browser  \\\n",
       "0   4709 2020-01-01 00:58:54   29.116379         0    Desktop  Safari   \n",
       "1  15617 2020-01-01 03:39:53  159.980988         0     Tablet  Safari   \n",
       "2   5696 2020-01-01 04:07:11   58.172305         0     Tablet  Chrome   \n",
       "3   2183 2020-01-01 04:53:54   48.700885         0     Mobile  Chrome   \n",
       "4   7633 2020-01-01 06:24:25  176.650399         0     Mobile  Chrome   \n",
       "\n",
       "  MerchantCategory Country  \n",
       "0    Entertainment      ZA  \n",
       "1           Health      ZA  \n",
       "2           Retail     CHN  \n",
       "3           Retail      US  \n",
       "4           Travel      UK  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In this cell, the dataset is loaded and basic EDA is performed.\n",
    "DataDir=\"data\"\n",
    "os.makedirs(DataDir,exist_ok=True)\n",
    "Dataset=os.path.join(DataDir, \"./my_dataset.csv\") #Relative path to dataset\n",
    "\n",
    "if os.path.exists(Dataset):\n",
    "    print(\"Loading dataset from local directory.\")\n",
    "    df=pd.read_csv(Dataset)\n",
    "    #Kaggle datasets has \"TIME\", \"Amount\" , and \"Class\" columns.\n",
    "    #Create synthetic categorical features for demonstration.\n",
    "    df=df.reset_index(drop=True)\n",
    "    #A synthetic timeframe will be created\n",
    "    StartDate=datetime(2020,1,1)\n",
    "    df['timestamp']=df['Time'].apply(lambda x: StartDate + timedelta(seconds=int(x)))\n",
    "    #Categorical features are created\n",
    "    df['DeviceType']=np.random.choice(['Mobile','Desktop','Tablet'], size=10,p=[0.5,0.1,0.4])\n",
    "    df['Browser']=np.random.choice(['Chrome', 'Firefox', 'Safari', 'Edge'], size=10, p=[0.4,0.1,0.4,0.1])\n",
    "    df['MerchantCategory']=np.random.choice(['Retail', 'Food', 'Travel', 'Entertainment','Health'], size=10, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "    df['Country']=np.random.choice(['ZA','UK', 'US', 'CHN', 'IND'], size=10, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "    df=df.rename(columns={'Amount':'amount','Class':'is_fraud'})\n",
    "    \n",
    "else:\n",
    "    print(\"Dataset not found. Will create a synthetic dataset.\")\n",
    "    n=20000\n",
    "    #Simulate the users base\n",
    "    users=np.random.randint(1,20000, size=n)\n",
    "    StartDate=datetime(2020,1,1)\n",
    "    timestamps=[StartDate + timedelta(seconds=int(x)) \n",
    "                for x in np.random.exponential(scale=3600, size=n).cumsum()]\n",
    "    amounts=np.random.exponential(scale=100, size=n)\n",
    "#Labeling transactions as fraud or not based on amount and random noise\n",
    "IsFraud=(np.random.rand(n) < 0.002).astype(int) #Around 0.2% frauds\n",
    "DeviceType=np.random.choice(['Mobile','Desktop', 'Tablet'], size=n, p=[0.5,0.1,0.4])\n",
    "Browser=np.random.choice(['Chrome', 'Firefox', 'Safari', 'Edge'], size=n, p=[0.4,0.1,0.4,0.1])\n",
    "MerchantCategory=np.random.choice(['Retail', 'Food', 'Travel', 'Entertainment','Health'], size=n, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "Country=np.random.choice(['ZA','UK', 'US', 'CHN', 'IND'], size=n, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "\n",
    "df=pd.DataFrame({\n",
    "    'users': users,\n",
    "    'timestamp': timestamps,\n",
    "    'amount': amounts,\n",
    "    'is_fraud': IsFraud,\n",
    "    'DeviceType': DeviceType,\n",
    "    'Browser': Browser,\n",
    "    'MerchantCategory': MerchantCategory,\n",
    "    'Country': Country,\n",
    "})\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea330e2e",
   "metadata": {},
   "source": [
    " #3 Synthetic Market Stress Enrichment\n",
    "We will create a daily market stress series with:\n",
    "- USD/ZAR returns\n",
    "- VIX proxy (global vol)\n",
    "- SARB repo rate change flags\n",
    "\n",
    "Then merge the daily metrics onto each transaction by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9469cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>UsdZar</th>\n",
       "      <th>VixIndex</th>\n",
       "      <th>RepoRate</th>\n",
       "      <th>UsdZarReturn</th>\n",
       "      <th>UsdZarReturn7DayVol</th>\n",
       "      <th>UsdZar7DayMean</th>\n",
       "      <th>MarketStress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>16.867826</td>\n",
       "      <td>26.649556</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.649556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>18.099243</td>\n",
       "      <td>20.706721</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.073004</td>\n",
       "      <td>0.051622</td>\n",
       "      <td>23.678138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>19.942295</td>\n",
       "      <td>19.394113</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.101830</td>\n",
       "      <td>0.052488</td>\n",
       "      <td>22.250130</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>21.940214</td>\n",
       "      <td>23.570197</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.100185</td>\n",
       "      <td>0.047704</td>\n",
       "      <td>22.580147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>22.473068</td>\n",
       "      <td>23.617679</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.024287</td>\n",
       "      <td>0.045850</td>\n",
       "      <td>22.787653</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date     UsdZar   VixIndex  RepoRate  UsdZarReturn  \\\n",
       "0 2020-01-01  16.867826  26.649556    0.0675      0.000000   \n",
       "1 2020-01-02  18.099243  20.706721    0.0675      0.073004   \n",
       "2 2020-01-03  19.942295  19.394113    0.0675      0.101830   \n",
       "3 2020-01-04  21.940214  23.570197    0.0675      0.100185   \n",
       "4 2020-01-05  22.473068  23.617679    0.0675      0.024287   \n",
       "\n",
       "   UsdZarReturn7DayVol  UsdZar7DayMean  MarketStress  \n",
       "0             0.000000       26.649556             0  \n",
       "1             0.051622       23.678138             1  \n",
       "2             0.052488       22.250130             1  \n",
       "3             0.047704       22.580147             1  \n",
       "4             0.045850       22.787653             1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create market stress data\n",
    "#create a date range that covres the transaction timestamps\n",
    "min_date=df['timestamp'].min().date()\n",
    "max_date=df['timestamp'].max().date()\n",
    "dates=pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "# Simulate USD/ZAR daily returns (random walk with occasional shocks)\n",
    "np.random.seed(42)\n",
    "UsdZarLog=np.random.normal(loc=0, scale=0.01, size=len(dates))\n",
    "#Add random shocks\n",
    "Shock=np.random.choice(len(dates), size=int(len(dates)*0.05*len(dates)), replace=True)\n",
    "UsdZarLog[Shock] += np.random.normal(loc=0.05, scale=0.02, size=len(Shock))\n",
    "\n",
    "UsdZarLog=16.76*np.exp(np.cumsum(UsdZarLog)) #Starting rate around 16.76\n",
    "\n",
    "#VIX index simulation\n",
    "VixIndex=np.abs(np.random.normal(loc=12, size=len(dates)))\n",
    "VixIndex[Shock] += np.random.normal(10, 5, size=len(Shock))\n",
    "VixIndex=np.clip(VixIndex,10,None)\n",
    "\n",
    "#repo rate simulation\n",
    "RepoRate=np.full(len(dates),0.0675 ) #starts at 6.75%\n",
    "ChangeIndixes=np.random.choice(len(dates), size=int(0.002*len(dates)), replace=False)\n",
    "for idx in ChangeIndixes:\n",
    "    RepoRate[idx:] += np.random.choice([0.25, -0.25, 0.5, -0.5]) #Changes in basis points\n",
    "\n",
    "MarketData=pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'UsdZar': UsdZarLog,\n",
    "    'VixIndex': VixIndex,\n",
    "    'RepoRate': RepoRate\n",
    "})\n",
    "\n",
    "#Compute the returns and volatility\n",
    "MarketData['UsdzarReturn']=MarketData['UsdZar'].pct_change().fillna(0)\n",
    "MarketData['UsdZarReturn7DayVol']=MarketData['UsdzarReturn'].rolling(7, min_periods=1).std().fillna(0)\n",
    "MarketData['UsdZar7DayMean']=MarketData['VixIndex'].rolling(7, min_periods=1).mean().fillna(MarketData['VixIndex'])\n",
    "#Mark the stress when UsdZar is less than -2% or VIX index is above 20 or Repo rate above 8%\n",
    "MarketData['MarketStress']=((MarketData['UsdzarReturn'].abs() > 0.02) | (MarketData['VixIndex'] > MarketData['VixIndex'].quantile(0.90))).astype(int)\n",
    "MarketData.rename(columns={'UsdzarReturn': 'UsdZarReturn'}, inplace=True)\n",
    "\n",
    "MarketData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c26c13",
   "metadata": {},
   "source": [
    "Merge market features into transactions (by date).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d598669c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>amount</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>Browser</th>\n",
       "      <th>MerchantCategory</th>\n",
       "      <th>Country</th>\n",
       "      <th>date</th>\n",
       "      <th>UsdZar</th>\n",
       "      <th>VixIndex</th>\n",
       "      <th>RepoRate</th>\n",
       "      <th>UsdZarReturn</th>\n",
       "      <th>UsdZarReturn7DayVol</th>\n",
       "      <th>UsdZar7DayMean</th>\n",
       "      <th>MarketStress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4709</td>\n",
       "      <td>2020-01-01 00:58:54</td>\n",
       "      <td>29.116379</td>\n",
       "      <td>0</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>ZA</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>16.867826</td>\n",
       "      <td>26.649556</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.649556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15617</td>\n",
       "      <td>2020-01-01 03:39:53</td>\n",
       "      <td>159.980988</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Health</td>\n",
       "      <td>ZA</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>16.867826</td>\n",
       "      <td>26.649556</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.649556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5696</td>\n",
       "      <td>2020-01-01 04:07:11</td>\n",
       "      <td>58.172305</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Retail</td>\n",
       "      <td>CHN</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>16.867826</td>\n",
       "      <td>26.649556</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.649556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2183</td>\n",
       "      <td>2020-01-01 04:53:54</td>\n",
       "      <td>48.700885</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Retail</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>16.867826</td>\n",
       "      <td>26.649556</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.649556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7633</td>\n",
       "      <td>2020-01-01 06:24:25</td>\n",
       "      <td>176.650399</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Travel</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>16.867826</td>\n",
       "      <td>26.649556</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.649556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   users           timestamp      amount  is_fraud DeviceType Browser  \\\n",
       "0   4709 2020-01-01 00:58:54   29.116379         0    Desktop  Safari   \n",
       "1  15617 2020-01-01 03:39:53  159.980988         0     Tablet  Safari   \n",
       "2   5696 2020-01-01 04:07:11   58.172305         0     Tablet  Chrome   \n",
       "3   2183 2020-01-01 04:53:54   48.700885         0     Mobile  Chrome   \n",
       "4   7633 2020-01-01 06:24:25  176.650399         0     Mobile  Chrome   \n",
       "\n",
       "  MerchantCategory Country       date     UsdZar   VixIndex  RepoRate  \\\n",
       "0    Entertainment      ZA 2020-01-01  16.867826  26.649556    0.0675   \n",
       "1           Health      ZA 2020-01-01  16.867826  26.649556    0.0675   \n",
       "2           Retail     CHN 2020-01-01  16.867826  26.649556    0.0675   \n",
       "3           Retail      US 2020-01-01  16.867826  26.649556    0.0675   \n",
       "4           Travel      UK 2020-01-01  16.867826  26.649556    0.0675   \n",
       "\n",
       "   UsdZarReturn  UsdZarReturn7DayVol  UsdZar7DayMean  MarketStress  \n",
       "0           0.0                  0.0       26.649556             0  \n",
       "1           0.0                  0.0       26.649556             0  \n",
       "2           0.0                  0.0       26.649556             0  \n",
       "3           0.0                  0.0       26.649556             0  \n",
       "4           0.0                  0.0       26.649556             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merge market data with transaction data by date\n",
    "df['date']=pd.to_datetime(df['timestamp'].dt.date)\n",
    "df=df.merge(MarketData, on='date', how='left')\n",
    "\n",
    "#Fill empty market data for missing dates\n",
    "df[['UsdZar', 'VixIndex', 'RepoRate', 'UsdZarReturn', 'UsdZarReturn7DayVol', 'UsdZar7DayMean', 'MarketStress']] = \\\n",
    "    df[['UsdZar', 'VixIndex', 'RepoRate', 'UsdZarReturn', 'UsdZarReturn7DayVol', 'UsdZar7DayMean', 'MarketStress']].fillna(method='ffill').fillna(0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b7cab",
   "metadata": {},
   "source": [
    "#4 Feature engineering — transactional & behavioral\n",
    "We create:\n",
    "- time features (hour, weekday)\n",
    "- log amount\n",
    "- rolling features per user (1h / 24h counts, avg amounts)\n",
    "- device-country mismatch flag\n",
    "- anomaly score: amount compared to user's historical mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d5a930e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,\n",
       "        Amount  AmountLog  hour  weekday  UserTxCount1h  UserTxCount24h  \\\n",
       " 0  299.725697   5.706199    14        6              0               0   \n",
       " 1  106.787622   4.680163    19        5              0               0   \n",
       " 2   17.078902   2.894746    16        4              0               0   \n",
       " 3  163.197287   5.101069     9        2              0               0   \n",
       " 4   67.200839   4.222457    15        5              0               0   \n",
       " \n",
       "    UserAmountMean  AmtZScore DeviceType MerchantCategory Country  \\\n",
       " 0      299.725697   0.000000     Mobile           Travel      US   \n",
       " 1      106.787622   0.000000     Mobile             Food      UK   \n",
       " 2       90.138094  -0.707107     Mobile             Food      UK   \n",
       " 3       90.138094   0.707107     Mobile           Health      ZA   \n",
       " 4       83.740447  -0.707107     Tablet           Health     IND   \n",
       " \n",
       "    UsdZarReturn  UsdZarReturn7DayVol   VixIndex  UsdZar7DayMean  RepoRate  \\\n",
       " 0      0.030253             0.015091  27.342202       23.817034    0.0675   \n",
       " 1      0.076689             0.020487  16.397078       22.450832    0.0675   \n",
       " 2      0.057205             0.011182  21.609577       22.582448    0.0675   \n",
       " 3      0.053150             0.023085  24.112034       23.475417    0.0675   \n",
       " 4      0.067217             0.021375  22.205351       23.946192    0.0675   \n",
       " \n",
       "    MarketStress  IsNightTransaction  DeviceCountryMismatch  \n",
       " 0             1                   0                      0  \n",
       " 1             1                   0                      0  \n",
       " 2             1                   0                      0  \n",
       " 3             1                   0                      1  \n",
       " 4             1                   0                      0  )"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature Engineering\n",
    "# Remove duplicate columns before proceeding\n",
    "df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
    "\n",
    "# Ensure timestamp is datetime dtype, then extract time features\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "# Create Amount column to match downstream expected naming and keep log\n",
    "if 'Amount' not in df.columns:\n",
    "    df['Amount'] = df['amount']\n",
    "df['AmountLog'] = np.log1p(df['Amount'])\n",
    "\n",
    "#Sort by user and timestamp and ensure UserId exists\n",
    "if 'UserId' not in df.columns:\n",
    "    df['UserId'] = np.random.randint(1, 20000, size=len(df))\n",
    "df = df.sort_values(by=['UserId', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Rolling features (user-level)\n",
    "# Compute rolling counts in the past 1 hour and 24 hours per user using two-pointer approach\n",
    "def BuildUsersRollingFeatures(df, seconds_window, col_name):\n",
    "    Out = []\n",
    "    # iterate groups in the same order as df (df already sorted by UserId and timestamp)\n",
    "    for uid, group in df.groupby('UserId', sort=False):\n",
    "        # seconds since epoch as numpy array for fast indexing\n",
    "        Times = (group['timestamp'].astype('int64') // 1_000_000_000).to_numpy()\n",
    "        counts = []\n",
    "        left = 0\n",
    "        for i, t in enumerate(Times):\n",
    "            while left < i and (t - Times[left]) > seconds_window:\n",
    "                left += 1\n",
    "            counts.append(i - left)\n",
    "        Out.extend(counts)\n",
    "    # assign (length should match df)\n",
    "    df[col_name] = Out\n",
    "\n",
    "# create 1h and 24h user tx count columns\n",
    "BuildUsersRollingFeatures(df, seconds_window=3600, col_name='UserTxCount1h')\n",
    "BuildUsersRollingFeatures(df, seconds_window=86400, col_name='UserTxCount24h')\n",
    "\n",
    "# user historical stats (means and std) using the Amount column\n",
    "UserStats = df.groupby('UserId')['Amount'].agg(['mean','std']).rename(columns={'mean':'UserAmountMean','std':'UserAmountStd'})\n",
    "# merge using left_on UserId and right_index since UserStats uses UserId as index\n",
    "df = df.merge(UserStats, left_on='UserId', right_index=True, how='left')\n",
    "\n",
    "# If merge created suffixed columns due to prior columns, ensure canonical names exist\n",
    "# prefer explicit names if present, otherwise take suffixed variants\n",
    "if 'UserAmountMean' not in df.columns:\n",
    "    for c in ['UserAmountMean_x', 'UserAmountMean_y']:\n",
    "        if c in df.columns:\n",
    "            df['UserAmountMean'] = df[c]\n",
    "            break\n",
    "if 'UserAmountStd' not in df.columns:\n",
    "    for c in ['UserAmountStd_x', 'UserAmountStd_y']:\n",
    "        if c in df.columns:\n",
    "            df['UserAmountStd'] = df[c]\n",
    "            break\n",
    "\n",
    "# Amount anomaly score (z-score) - guard divide-by-zero and missing std\n",
    "df['UserAmountStd'] = df['UserAmountStd'].replace(0, np.nan)\n",
    "df['AmtZScore'] = (df['Amount'] - df['UserAmountMean']) / df['UserAmountStd']\n",
    "df['AmtZScore'] = df['AmtZScore'].fillna(0)\n",
    "\n",
    "# device-country mismatch flag: assume user primary country is mode country (safe mode extraction)\n",
    "UserCountry = df.groupby('UserId')['Country'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan).rename('UserPrimaryCountry')\n",
    "df = df.merge(UserCountry, on='UserId', how='left')\n",
    "# use the correct merged column name 'UserPrimaryCountry'\n",
    "df['DeviceCountryMismatch'] = (df['Country'] != df['UserPrimaryCountry']).astype(int)\n",
    "\n",
    "# Flagging for night transactions (between 12 AM to 6 AM) -- correct boolean logic\n",
    "df['IsNightTransaction'] = df['hour'].apply(lambda x: 1 if (x > 0 and x < 6) else 0)\n",
    "\n",
    "# Short-term rolling features - ensure feature list matches created columns\n",
    "FeatCots = ['Amount', 'AmountLog', 'hour', 'weekday', 'UserTxCount1h', 'UserTxCount24h',\n",
    "            'UserAmountMean','AmtZScore','DeviceType','MerchantCategory','Country',\n",
    "            'UsdZarReturn','UsdZarReturn7DayVol','VixIndex','UsdZar7DayMean','RepoRate','MarketStress','IsNightTransaction','DeviceCountryMismatch']\n",
    "\n",
    "len(df), df[FeatCots].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91e420d",
   "metadata": {},
   "source": [
    " #5 Prepare training dataset\n",
    "- Choose modeling features.\n",
    "- Encode categoricals using a ColumnTransformer pipeline.\n",
    "- Train/test split stratified by label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "367317b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (16000, 19) Test shape: (4000, 19)\n",
      "Fraud rate in train: 0.002 in test: 0.002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell: 6 - prepare dataset for modeling\n",
    "LabelCol = 'is_fraud'\n",
    "# Some datasets have different naming conventions for the label column.\n",
    "if LabelCol not in df.columns:\n",
    "    LabelCol = 'Class' if 'Class' in df.columns else 'is_fraud'\n",
    "Y = df[LabelCol].astype(int)\n",
    "# Select features and target\n",
    "NumFeatures = ['Amount', 'AmountLog', 'hour', 'weekday', 'UserTxCount1h', 'UserTxCount24h',\n",
    "               'UserAmountMean', 'AmtZScore', 'UsdZarReturn', 'UsdZarReturn7DayVol', 'VixIndex',\n",
    "               'UsdZar7DayMean', 'RepoRate', 'MarketStress', 'IsNightTransaction', 'DeviceCountryMismatch']\n",
    "CatFeatures = ['DeviceType', 'MerchantCategory', 'Country']\n",
    "X = df[NumFeatures + CatFeatures].copy()\n",
    "\n",
    "# Handle infinities and NaNs\n",
    "X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=RandomSeed)\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "print(\"Fraud rate in train:\", Y_train.mean(), \"in test:\", Y_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f3676b",
   "metadata": {},
   "source": [
    "Preprocessing pipeline\n",
    "- Standard scale numerical features\n",
    "- One-hot encode small-cardinality categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a008d458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed feauture shape: (16000, 29)\n"
     ]
    }
   ],
   "source": [
    "# Cell: 7 - preprocessing pipeline\n",
    "# Remove duplicate columns from X_train and X_test before preprocessing\n",
    "X_train = X_train.loc[:, ~X_train.columns.duplicated(keep='first')]\n",
    "X_test = X_test.loc[:, ~X_test.columns.duplicated(keep='first')]\n",
    "\n",
    "NumericTransormer=Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "CategoricalTransformer = Pipeline(steps=[\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "Preprocessor=ColumnTransformer(transformers=[('Num', NumericTransormer, NumFeatures),\n",
    "                                            ('Cat', CategoricalTransformer, CatFeatures)])\n",
    "\n",
    "# Fit-transform training set to create feature matrix for simple models\n",
    "XtrainP=Preprocessor.fit_transform(X_train)\n",
    "XtestP=Preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Transformed feauture shape:\", XtrainP.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35ee5d",
   "metadata": {},
   "source": [
    "6) Handle class imbalance & Model training\n",
    "We will:\n",
    "- Train a LogisticRegression baseline with class weights.\n",
    "- Train a LightGBM model (primary).\n",
    "- Optionally use SMOTE for oversampling (toggle below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f498796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-resample shape: (16000, 29) Fraud rate: 0.002\n"
     ]
    }
   ],
   "source": [
    "# Cell: 8 - optional SMOTE toggle (use with caution)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "UseSmote=False\n",
    "\n",
    "# Ensure RandomSeed is available when cells are run out of order\n",
    "RandomSeed = globals().get(RandomSeed, 42)\n",
    "\n",
    "if UseSmote:\n",
    "    print(\"Applying SMOTE to balance the classes in the training set.\")\n",
    "    Sm = SMOTE(random_state=RandomSeed, n_jobs=-1)\n",
    "    XtrainPRes, YtrainPRes = Sm.fit_resample(XtrainP, Y_train)\n",
    "else:\n",
    "    XtrainPRes, YtrainPRes = XtrainP, Y_train\n",
    "\n",
    "print(\"Post-resample shape:\", XtrainPRes.shape, \"Fraud rate:\", YtrainPRes.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5691c65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression AUC-ROC: 0.5350075150300602\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.66      0.79      3992\n",
      "           1       0.00      0.38      0.00         8\n",
      "\n",
      "    accuracy                           0.66      4000\n",
      "   macro avg       0.50      0.52      0.40      4000\n",
      "weighted avg       1.00      0.66      0.79      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell: 9 - Logistic Regression baseline\n",
    "Lr=LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RandomSeed)\n",
    "Lr.fit(XtrainPRes, YtrainPRes)\n",
    "ProbLr=Lr.predict_proba(XtestP)[:,1]\n",
    "print(\"Logistic Regression AUC-ROC:\", roc_auc_score(Y_test, ProbLr))\n",
    "print(classification_report(Y_test, Lr.predict(XtestP)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a0616ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell: 10 - LightGBM model (with sklearn wrapper)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# We'll use plain lgb.Dataset approach via sklearn wrapper for convenience.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlgb\u001b[39;00m\n\u001b[32m      4\u001b[39m lgbm=lgb.LGBMClassifier(\n\u001b[32m      5\u001b[39m     n_estimators=\u001b[32m1000\u001b[39m,\n\u001b[32m      6\u001b[39m     learning_rate=\u001b[32m0.05\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     13\u001b[39m lgbm.fit(XtrainPRes, YtrainPRes,\n\u001b[32m     14\u001b[39m          eval_set=[(XtestP, Y_test)],\n\u001b[32m     15\u001b[39m          eval_metric=\u001b[33m'\u001b[39m\u001b[33mauc\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     16\u001b[39m             early_stopping_rounds=\u001b[32m50\u001b[39m,\n\u001b[32m     17\u001b[39m             verbose=\u001b[32m100\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# Cell: 10 - LightGBM model (with sklearn wrapper)\n",
    "# We'll use plain lgb.Dataset approach via sklearn wrapper for convenience.\n",
    "import lightgbm as lgb\n",
    "lgbm=lgb.LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    objective='binary',\n",
    "    class_weight='balanced',\n",
    "    random_state=RandomSeed,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgbm.fit(XtrainPRes, YtrainPRes,\n",
    "         eval_set=[(XtestP, Y_test)],\n",
    "         eval_metric='auc',\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=100)\n",
    "\n",
    "ProbLgbm=lgbm.predict_proba(XtestP)[:,1]\n",
    "print(\"LightGBM AUC-ROC:\", roc_auc_score(Y_test, ProbLgbm))\n",
    "print(classification_report(Y_test, lgbm.predict(XtestP)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a567fd",
   "metadata": {},
   "source": [
    "7) Evaluation visuals & business-oriented metrics\n",
    "- ROC, PR curves\n",
    "- Confusion matrix at chosen threshold\n",
    "- Precision@K / Lift if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#ROC\n",
    "fpr, tpr, _=roc_curve(Y_test, ProbLgbm)\n",
    "roc_auc=auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f'LightGBM (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "#precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(Y_test, ProbLgbm)\n",
    "ap = average_precision_score(Y_test, ProbLgbm)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(recall, precision, label=f'LightGBM (AP = {ap:.3f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bca217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 12 - Confusion matrix at a chosen threshold\n",
    "def conf_matrix_at_threshold(y_true, probs, thresh=0.5):\n",
    "    Preds = (probs >= thresh).astype(int)\n",
    "    Cm = confusion_matrix(y_true, Preds)\n",
    "    tn, fp, fn, tp = Cm.ravel()\n",
    "    return {'tn':tn,'fp':fp,'fn':fn,'tp':tp,'cm':Cm}\n",
    "\n",
    "th = 0.5\n",
    "cm_stats = conf_matrix_at_threshold(Y_test, ProbLgbm, thresh=th)\n",
    "print(\"Threshold:\", th)\n",
    "print(cm_stats)\n",
    "sns.heatmap(cm_stats['Cm'], annot=True, fmt='d', cmap='Blues', xticklabels=['NotFraud','Fraud'], yticklabels=['NotFraud','Fraud'])\n",
    "plt.title(f'Confusion matrix @ threshold={th}')\n",
    "plt.show()\n",
    "\n",
    "# Show classification report for thresholded predictions\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "preds_th = (ProbLgbm >= th).astype(int)\n",
    "print(\"Precision:\", precision_score(Y_test, preds_th))\n",
    "print(\"Recall:\", recall_score(Y_test, preds_th))\n",
    "print(\"F1:\", f1_score(Y_test, preds_th))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1084a37",
   "metadata": {},
   "source": [
    " Business metric: Expected Loss Avoided (toy example)\n",
    "- Suppose average fraud amount is `avg_fraud_amt`.\n",
    "- False negative = missed fraud (cost = amount)\n",
    "- False positive = manual review cost = e.g., 5 USD\n",
    "Compute net saved loss vs baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63683c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 13 - toy business metric\n",
    "AvgFraudAmt=df.loc[df[LabelCol]==1, 'amount'].mean() if df[LabelCol].sum() >0 else 100\n",
    "ManuelReviewCost=5.0\n",
    "\n",
    "tn, fp, fn, tp = cm_stats['tn'], cm_stats['fp'], cm_stats['fn'], cm_stats['tp']\n",
    "ExpectedLossMissed=fn*AvgFraudAmt\n",
    "ExpectedManualReviewCost=(tp*fp) *ManuelReviewCost\n",
    "print(f\"Avg fraud amount: {AvgFraudAmt:.2f}\")\n",
    "print(f\"Missed frauds (fn): {fn}, cost approx: {ExpectedLossMissed:.2f}\")\n",
    "print(f\"Manual reviews (tp+fp): {(tp+fp)}, cost approx: {ExpectedManualReviewCost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a44eb",
   "metadata": {},
   "source": [
    "8) Explainability — SHAP\n",
    "SHAP helps explain LightGBM predictions (global and single prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d1a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 14 - SHAP explainability\n",
    "# Create a small sample to speed shap runtime\n",
    "import shap\n",
    "SampleIndices=np.random.choice(X_test.shape[0], size=min(1000, X_test.shape[0]), replace=False)\n",
    "XShapSample=X_test.iloc[SampleIndices]\n",
    "XShapSampleP=Preprocessor.transform(XShapSample)\n",
    "\n",
    "# Build feature names after preprocessing\n",
    "# numeric names + ohe names\n",
    "NumNames=NumFeatures\n",
    "ohe=Preprocessor.named_transformers_['Cat'].named_steps['ohe']\n",
    "oheNames=ohe.get_feature_names_out(CatFeatures).tolist()\n",
    "FeatureNames=NumNames + oheNames\n",
    "\n",
    "# Use TreeExplainer for LightGBM\n",
    "Explainer=shap.TreeExplainer(lgbm)\n",
    "ShapValues=Explainer.shap_values(XShapSampleP)\n",
    "\n",
    "# Summary plot (global importance)\n",
    "shap.summary_plot(ShapValues, XShapSampleP, feature_names=FeatureNames, plot_type='bar', show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a11f92",
   "metadata": {},
   "source": [
    "SHAP waterfall for a single example\n",
    "Pick a high-risk transaction from the test set and show local explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 15 - SHAP waterfall for single transaction (fixed)\n",
    "# pick the highest predicted risk transaction in the test set\n",
    "\n",
    "HighRiskIdx = np.argmax(ProbLgbm)\n",
    "\n",
    "# Use the original (preprocessed) pandas row from X_test, then transform\n",
    "SingleRow_df = X_test.iloc[[HighRiskIdx]]\n",
    "SingleRowP = Preprocessor.transform(SingleRow_df)  # shape (1, n_features)\n",
    "\n",
    "# Compute SHAP values for the single transformed row\n",
    "SingleShap = Explainer.shap_values(SingleRowP)\n",
    "\n",
    "# Handle binary-class vs single-array shap output: pick positive-class contributions\n",
    "if isinstance(SingleShap, list) and len(SingleShap) > 1:\n",
    "    shap_vals = SingleShap[1][0]  # positive class, first (and only) sample\n",
    "else:\n",
    "    # SingleShap could be array of shape (1, n) or (n,) depending on shap version\n",
    "    shap_vals = SingleShap[0] if getattr(SingleShap, 'ndim', 1) == 2 else SingleShap\n",
    "\n",
    "# Determine base value for positive class if provided as array\n",
    "ev = Explainer.expected_value\n",
    "base_value = ev[1] if (isinstance(ev, (list, np.ndarray)) and len(ev) > 1) else ev\n",
    "\n",
    "print(\"Predicted fraud probability for the highest risk transaction:\", ProbLgbm[HighRiskIdx])\n",
    "\n",
    "# Build Explanation and plot waterfall (data is the 1D feature vector)\n",
    "shap.waterfall_plot(\n",
    "    shap.Explanation(values=shap_vals,\n",
    "                     base_values=base_value,\n",
    "                     data=SingleRowP[0],\n",
    "                     feature_names=FeatureNames),\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345cdb66",
   "metadata": {},
   "source": [
    " 9) Save model & preprocessing pipeline\n",
    "Save both the trained LightGBM model and the preprocessing pipeline for later deployment (Streamlit or API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell: 16 -  Save the model and preprocessor\n",
    "OutDIr=\"models\"\n",
    "os.makedirs(OutDIr, exist_ok=True)\n",
    "\n",
    "joblib.dump(Preprocessor, os.path.join(OutDIr, \"preprocessor.joblib\"))\n",
    "joblib.dump(lgbm, os.path.join(OutDIr, \"lgbm_model.joblib\"))\n",
    "joblib.dump(Lr, os.path.join(OutDIr, \"logistic_regression_model.joblib\"))\n",
    "\n",
    "print(\"Saved preprocessor and models to ./models/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
