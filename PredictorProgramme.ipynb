{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063b9db8",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "# Fraud & Cyber-Threat Prediction — End-to-End Notebook\n",
    "Goal:Predict fraudulent / cyber-risky transactions using transaction metadata, behavioral features, and market stress indicators.\n",
    "\n",
    "Author: Milani Chikeka  \n",
    "\n",
    "Seed:42\n",
    "---\n",
    "Sections\n",
    "1. Setup & imports  \n",
    "2. Load dataset (Kaggle `creditcard.csv` recommended) or simulate synthetic transactions  \n",
    "3. Market stress synthetic enrichment (USD/ZAR returns, VIX proxy, repo rate changes)  \n",
    "4. Feature engineering (behavioral + transaction + stress features)  \n",
    "5. Train/test split & imbalance handling  \n",
    "6. Models: Logistic Regression baseline + LightGBM (main)  \n",
    "7. Evaluation: ROC, PR, confusion matrix, business metrics  \n",
    "8. Explainability: SHAP plots  \n",
    "9. Save model & preprocessing pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87bd9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setups and imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings  \n",
    "#import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (roc_auc_score, precision_recall_curve,average_precision_score ,confusion_matrix,classification_report)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Optional libraies and seedings\n",
    "\"\"\" try:\n",
    "    import lightgbm as lgb\n",
    "except Exception as e:\n",
    "    print(\"install lightgbm: pip install lightgbm\")\n",
    "    raise e  \"\"\"\n",
    "\n",
    "try:\n",
    "    import shap \n",
    "except Exception as e:\n",
    "    print(\"install shap: pip install shap\")\n",
    "    raise e\n",
    "\n",
    "#This is for imbalanbce handling.\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except Exception as e:\n",
    "    print(\"Install imbalanced-learn: pip install imbalanced-learn\")    \n",
    "    raise e\n",
    "\n",
    "RandomSeed=42\n",
    "np.random.seed(RandomSeed)\n",
    "\n",
    "#Style plotting\n",
    "sns.set(style=\"whitegrid\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e60d9",
   "metadata": {},
   "source": [
    "#2Load the date/create synthetic data.\n",
    "-Load the Kaggle dataset `creditcard.csv`, put it in `./data/creditcard.csv`.\n",
    "-If dataset is not present, create 'synthetic' transaction datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb81502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not found. Will create a synthetic dataset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>amount</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>Browser</th>\n",
       "      <th>MerchantCategory</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15796</td>\n",
       "      <td>2020-01-01 03:11:39</td>\n",
       "      <td>134.654218</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Health</td>\n",
       "      <td>ZA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>861</td>\n",
       "      <td>2020-01-01 03:55:06</td>\n",
       "      <td>134.576560</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5391</td>\n",
       "      <td>2020-01-01 07:59:38</td>\n",
       "      <td>65.825005</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Retail</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11965</td>\n",
       "      <td>2020-01-01 08:03:13</td>\n",
       "      <td>60.568024</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Travel</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11285</td>\n",
       "      <td>2020-01-01 08:55:46</td>\n",
       "      <td>154.949674</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>ZA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   users           timestamp      amount  is_fraud DeviceType Browser  \\\n",
       "0  15796 2020-01-01 03:11:39  134.654218         0     Mobile  Chrome   \n",
       "1    861 2020-01-01 03:55:06  134.576560         0     Tablet  Safari   \n",
       "2   5391 2020-01-01 07:59:38   65.825005         0     Tablet  Safari   \n",
       "3  11965 2020-01-01 08:03:13   60.568024         0     Mobile  Safari   \n",
       "4  11285 2020-01-01 08:55:46  154.949674         0     Mobile  Safari   \n",
       "\n",
       "  MerchantCategory Country  \n",
       "0           Health      ZA  \n",
       "1    Entertainment      UK  \n",
       "2           Retail      US  \n",
       "3           Travel      UK  \n",
       "4    Entertainment      ZA  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In this cell, the dataset is loaded and basic EDA is performed.\n",
    "DataDir=\"data\"\n",
    "os.makedirs(DataDir,exist_ok=True)\n",
    "Dataset=os.path.join(DataDir, \"./my_dataset.csv\") #Relative path to dataset\n",
    "\n",
    "if os.path.exists(Dataset):\n",
    "    print(\"Loading dataset from local directory.\")\n",
    "    df=pd.read_csv(Dataset)\n",
    "    #Kaggle datasets has \"TIME\", \"Amount\" , and \"Class\" columns.\n",
    "    #Create synthetic categorical features for demonstration.\n",
    "    df=df.reset_index(drop=True)\n",
    "    #A synthetic timeframe will be created\n",
    "    StartDate=datetime(2020,1,1)\n",
    "    df['timestamp']=df['Time'].apply(lambda x: StartDate + timedelta(seconds=int(x)))\n",
    "    #Categorical features are created\n",
    "    df['DeviceType']=np.random.choice(['Mobile','Desktop','Tablet'], size=10, p=[0.5,0.1,0.4])\n",
    "    df['Browser']=np.random.choice(['Chrome', 'Firefox', 'Safari', 'Edge'], size=10, p=[0.4,0.1,0.4,0.1])\n",
    "    df['MerchantCategory']=np.random.choice(['Retail', 'Food', 'Travel', 'Entertainment','Health'], size=10, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "    df['Country']=np.random.choice(['ZA','UK', 'US', 'CHN', 'IND'], size=10, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "    df=df.rename(columns={'Amount':'amount','Class':'is_fraud'})\n",
    "    \n",
    "else:\n",
    "    print(\"Dataset not found. Will create a synthetic dataset.\")\n",
    "    n=20000\n",
    "    #Simulate the users base\n",
    "    users=np.random.randint(1,20000, size=n)\n",
    "    StartDate=datetime(2020,1,1)\n",
    "    timestamps=[StartDate + timedelta(seconds=int(x)) \n",
    "                for x in np.random.exponential(scale=3600, size=n).cumsum()]\n",
    "    amounts=np.random.exponential(scale=100, size=n)\n",
    "#Labeling transactions as fraud or not based on amount and random noise\n",
    "IsFraud=(np.random.rand(n) < 0.002).astype(int) #Around 0.2% frauds\n",
    "DeviceType=np.random.choice(['Mobile','Desktop', 'Tablet'], size=n, p=[0.5,0.1,0.4])\n",
    "Browser=np.random.choice(['Chrome', 'Firefox', 'Safari', 'Edge'], size=n, p=[0.4,0.1,0.4,0.1])\n",
    "MerchantCategory=np.random.choice(['Retail', 'Food', 'Travel', 'Entertainment','Health'], size=n, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "Country=np.random.choice(['ZA','UK', 'US', 'CHN', 'IND'], size=n, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "\n",
    "df=pd.DataFrame({\n",
    "    'users': users,\n",
    "    'timestamp': timestamps,\n",
    "    'amount': amounts,\n",
    "    'is_fraud': IsFraud,\n",
    "    'DeviceType': DeviceType,\n",
    "    'Browser': Browser,\n",
    "    'MerchantCategory': MerchantCategory,\n",
    "    'Country': Country,\n",
    "})\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea330e2e",
   "metadata": {},
   "source": [
    " #3 Synthetic Market Stress Enrichment\n",
    "We will create a daily market stress series with:\n",
    "- USD/ZAR returns\n",
    "- VIX proxy (global vol)\n",
    "- SARB repo rate change flags\n",
    "\n",
    "Then merge the daily metrics onto each transaction by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9469cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>UsdZar</th>\n",
       "      <th>VixIndex</th>\n",
       "      <th>RepoRate</th>\n",
       "      <th>UsdZarReturn</th>\n",
       "      <th>UsdZarReturn7DayVol</th>\n",
       "      <th>UsdZar7DayMean</th>\n",
       "      <th>MarketStress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17.156954</td>\n",
       "      <td>21.085375</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.085375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>17.689578</td>\n",
       "      <td>19.305748</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.031044</td>\n",
       "      <td>0.021952</td>\n",
       "      <td>20.195562</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>18.786199</td>\n",
       "      <td>10.852028</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.061992</td>\n",
       "      <td>0.030996</td>\n",
       "      <td>17.081050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>20.150740</td>\n",
       "      <td>27.324698</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.072635</td>\n",
       "      <td>0.032766</td>\n",
       "      <td>19.641962</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>21.155152</td>\n",
       "      <td>16.661573</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.049845</td>\n",
       "      <td>0.028626</td>\n",
       "      <td>19.045884</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date     UsdZar   VixIndex  RepoRate  UsdZarReturn  \\\n",
       "0 2020-01-01  17.156954  21.085375    0.0675      0.000000   \n",
       "1 2020-01-02  17.689578  19.305748    0.0675      0.031044   \n",
       "2 2020-01-03  18.786199  10.852028    0.0675      0.061992   \n",
       "3 2020-01-04  20.150740  27.324698    0.0675      0.072635   \n",
       "4 2020-01-05  21.155152  16.661573    0.0675      0.049845   \n",
       "\n",
       "   UsdZarReturn7DayVol  UsdZar7DayMean  MarketStress  \n",
       "0             0.000000       21.085375             0  \n",
       "1             0.021952       20.195562             1  \n",
       "2             0.030996       17.081050             1  \n",
       "3             0.032766       19.641962             1  \n",
       "4             0.028626       19.045884             1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create market stress data\n",
    "#create a date range that covres the transaction timestamps\n",
    "min_date=df['timestamp'].min().date()\n",
    "max_date=df['timestamp'].max().date()\n",
    "dates=pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "# Simulate USD/ZAR daily returns (random walk with occasional shocks)\n",
    "np.random.seed(42)\n",
    "UsdZarLog=np.random.normal(loc=0, scale=0.01, size=len(dates))\n",
    "#Add random shocks\n",
    "Shock=np.random.choice(len(dates), size=int(len(dates)*0.05*len(dates)), replace=True)\n",
    "UsdZarLog[Shock] += np.random.normal(loc=0.05, scale=0.02, size=len(Shock))\n",
    "\n",
    "UsdZarLog=16.76*np.exp(np.cumsum(UsdZarLog)) #Starting rate around 16.76\n",
    "\n",
    "#VIX index simulation\n",
    "VixIndex=np.abs(np.random.normal(loc=12, size=len(dates)))\n",
    "VixIndex[Shock] += np.random.normal(10, 5, size=len(Shock))\n",
    "VixIndex=np.clip(VixIndex,10,None)\n",
    "\n",
    "#repo rate simulation\n",
    "RepoRate=np.full(len(dates),0.0675 ) #starts at 6.75%\n",
    "ChangeIndixes=np.random.choice(len(dates), size=int(0.002*len(dates)), replace=False)\n",
    "for idx in ChangeIndixes:\n",
    "    RepoRate[idx:] += np.random.choice([0.25, -0.25, 0.5, -0.5]) #Changes in basis points\n",
    "\n",
    "MarketData=pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'UsdZar': UsdZarLog,\n",
    "    'VixIndex': VixIndex,\n",
    "    'RepoRate': RepoRate\n",
    "})\n",
    "\n",
    "#Compute the returns and volatility\n",
    "MarketData['UsdzarReturn']=MarketData['UsdZar'].pct_change().fillna(0)\n",
    "MarketData['UsdZarReturn7DayVol']=MarketData['UsdzarReturn'].rolling(7, min_periods=1).std().fillna(0)\n",
    "MarketData['UsdZar7DayMean']=MarketData['VixIndex'].rolling(7, min_periods=1).mean().fillna(MarketData['VixIndex'])\n",
    "#Mark the stress when UsdZar is less than -2% or VIX index is above 20 or Repo rate above 8%\n",
    "MarketData['MarketStress']=((MarketData['UsdzarReturn'].abs() > 0.02) | (MarketData['VixIndex'] > MarketData['VixIndex'].quantile(0.90))).astype(int)\n",
    "MarketData.rename(columns={'UsdzarReturn': 'UsdZarReturn'}, inplace=True)\n",
    "\n",
    "MarketData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c26c13",
   "metadata": {},
   "source": [
    "Merge market features into transactions (by date).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d598669c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>amount</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>Browser</th>\n",
       "      <th>MerchantCategory</th>\n",
       "      <th>Country</th>\n",
       "      <th>date</th>\n",
       "      <th>UsdZar</th>\n",
       "      <th>VixIndex</th>\n",
       "      <th>RepoRate</th>\n",
       "      <th>UsdZarReturn</th>\n",
       "      <th>UsdZarReturn7DayVol</th>\n",
       "      <th>UsdZar7DayMean</th>\n",
       "      <th>MarketStress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15796</td>\n",
       "      <td>2020-01-01 03:11:39</td>\n",
       "      <td>134.654218</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Health</td>\n",
       "      <td>ZA</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17.156954</td>\n",
       "      <td>21.085375</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.085375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>861</td>\n",
       "      <td>2020-01-01 03:55:06</td>\n",
       "      <td>134.576560</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17.156954</td>\n",
       "      <td>21.085375</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.085375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5391</td>\n",
       "      <td>2020-01-01 07:59:38</td>\n",
       "      <td>65.825005</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Retail</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17.156954</td>\n",
       "      <td>21.085375</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.085375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11965</td>\n",
       "      <td>2020-01-01 08:03:13</td>\n",
       "      <td>60.568024</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Travel</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17.156954</td>\n",
       "      <td>21.085375</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.085375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11285</td>\n",
       "      <td>2020-01-01 08:55:46</td>\n",
       "      <td>154.949674</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>ZA</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17.156954</td>\n",
       "      <td>21.085375</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.085375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   users           timestamp      amount  is_fraud DeviceType Browser  \\\n",
       "0  15796 2020-01-01 03:11:39  134.654218         0     Mobile  Chrome   \n",
       "1    861 2020-01-01 03:55:06  134.576560         0     Tablet  Safari   \n",
       "2   5391 2020-01-01 07:59:38   65.825005         0     Tablet  Safari   \n",
       "3  11965 2020-01-01 08:03:13   60.568024         0     Mobile  Safari   \n",
       "4  11285 2020-01-01 08:55:46  154.949674         0     Mobile  Safari   \n",
       "\n",
       "  MerchantCategory Country       date     UsdZar   VixIndex  RepoRate  \\\n",
       "0           Health      ZA 2020-01-01  17.156954  21.085375    0.0675   \n",
       "1    Entertainment      UK 2020-01-01  17.156954  21.085375    0.0675   \n",
       "2           Retail      US 2020-01-01  17.156954  21.085375    0.0675   \n",
       "3           Travel      UK 2020-01-01  17.156954  21.085375    0.0675   \n",
       "4    Entertainment      ZA 2020-01-01  17.156954  21.085375    0.0675   \n",
       "\n",
       "   UsdZarReturn  UsdZarReturn7DayVol  UsdZar7DayMean  MarketStress  \n",
       "0           0.0                  0.0       21.085375             0  \n",
       "1           0.0                  0.0       21.085375             0  \n",
       "2           0.0                  0.0       21.085375             0  \n",
       "3           0.0                  0.0       21.085375             0  \n",
       "4           0.0                  0.0       21.085375             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merge market data with transaction data by date\n",
    "df['date']=pd.to_datetime(df['timestamp'].dt.date)\n",
    "df=df.merge(MarketData, on='date', how='left')\n",
    "\n",
    "#Fill empty market data for missing dates\n",
    "df[['UsdZar', 'VixIndex', 'RepoRate', 'UsdZarReturn', 'UsdZarReturn7DayVol', 'UsdZar7DayMean', 'MarketStress']] = \\\n",
    "    df[['UsdZar', 'VixIndex', 'RepoRate', 'UsdZarReturn', 'UsdZarReturn7DayVol', 'UsdZar7DayMean', 'MarketStress']].fillna(method='ffill').fillna(0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b7cab",
   "metadata": {},
   "source": [
    "#4 Feature engineering — transactional & behavioral\n",
    "We create:\n",
    "- time features (hour, weekday)\n",
    "- log amount\n",
    "- rolling features per user (1h / 24h counts, avg amounts)\n",
    "- device-country mismatch flag\n",
    "- anomaly score: amount compared to user's historical mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d5a930e",
   "metadata": {},
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "Must pass \"right_on\" OR \"right_index\".",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMergeError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# user historical stats (means and std) using the Amount column\u001b[39;00m\n\u001b[32m     40\u001b[39m UserStats = df.groupby(\u001b[33m'\u001b[39m\u001b[33mUserId\u001b[39m\u001b[33m'\u001b[39m)[\u001b[33m'\u001b[39m\u001b[33mAmount\u001b[39m\u001b[33m'\u001b[39m].agg([\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m'\u001b[39m]).rename(columns={\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mUserAmountMean\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mUserAmountStd\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m df = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUserStats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mleft\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# If merge created suffixed columns due to prior columns, ensure canonical names exist\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# prefer explicit names if present, otherwise take suffixed variants\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mUserAmountMean\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df.columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:10839\u001b[39m, in \u001b[36mDataFrame.merge\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10820\u001b[39m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m  10821\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents=\u001b[32m2\u001b[39m)\n\u001b[32m  10822\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmerge\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m  10835\u001b[39m     validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10836\u001b[39m ) -> DataFrame:\n\u001b[32m  10837\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m> \u001b[39m\u001b[32m10839\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  10840\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  10841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10848\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10849\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10853\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/reshape/merge.py:170\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[32m    156\u001b[39m         left_df,\n\u001b[32m    157\u001b[39m         right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         copy=copy,\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     op = \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result(copy=copy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/reshape/merge.py:786\u001b[39m, in \u001b[36m_MergeOperation.__init__\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    779\u001b[39m     msg = (\n\u001b[32m    780\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNot allowed to merge between different levels. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    781\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_left.columns.nlevels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m levels on the left, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    782\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_right.columns.nlevels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m on the right)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    783\u001b[39m     )\n\u001b[32m    784\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(msg)\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m \u001b[38;5;28mself\u001b[39m.left_on, \u001b[38;5;28mself\u001b[39m.right_on = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_left_right_on\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m (\n\u001b[32m    789\u001b[39m     \u001b[38;5;28mself\u001b[39m.left_join_keys,\n\u001b[32m    790\u001b[39m     \u001b[38;5;28mself\u001b[39m.right_join_keys,\n\u001b[32m   (...)\u001b[39m\u001b[32m    793\u001b[39m     right_drop,\n\u001b[32m    794\u001b[39m ) = \u001b[38;5;28mself\u001b[39m._get_merge_keys()\n\u001b[32m    796\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/reshape/merge.py:1604\u001b[39m, in \u001b[36m_MergeOperation._validate_left_right_on\u001b[39m\u001b[34m(self, left_on, right_on)\u001b[39m\n\u001b[32m   1600\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n\u001b[32m   1601\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mCan only pass argument \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mleft_on\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m OR \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mleft_index\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m not both.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1602\u001b[39m     )\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.right_index \u001b[38;5;129;01mand\u001b[39;00m right_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1604\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\u001b[33m'\u001b[39m\u001b[33mMust pass \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mright_on\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m OR \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mright_index\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1605\u001b[39m n = \u001b[38;5;28mlen\u001b[39m(left_on)\n\u001b[32m   1606\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.right_index:\n",
      "\u001b[31mMergeError\u001b[39m: Must pass \"right_on\" OR \"right_index\"."
     ]
    }
   ],
   "source": [
    "#Feature Engineering\n",
    "# Ensure timestamp is datetime dtype, then extract time features\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "# Create Amount column to match downstream expected naming and keep log\n",
    "if 'Amount' not in df.columns:\n",
    "    df['Amount'] = df['amount']\n",
    "df['AmountLog'] = np.log1p(df['Amount'])\n",
    "\n",
    "#Sort by user and timestamp and ensure UserId exists\n",
    "if 'UserId' not in df.columns:\n",
    "    df['UserId'] = np.random.randint(1, 20000, size=len(df))\n",
    "df = df.sort_values(by=['UserId', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Rolling features (user-level)\n",
    "# Compute rolling counts in the past 1 hour and 24 hours per user using two-pointer approach\n",
    "def BuildUsersRollingFeatures(df, seconds_window, col_name):\n",
    "    Out = []\n",
    "    # iterate groups in the same order as df (df already sorted by UserId and timestamp)\n",
    "    for uid, group in df.groupby('UserId', sort=False):\n",
    "        # seconds since epoch as numpy array for fast indexing\n",
    "        Times = (group['timestamp'].astype('int64') // 1_000_000_000).to_numpy()\n",
    "        counts = []\n",
    "        left = 0\n",
    "        for i, t in enumerate(Times):\n",
    "            while left < i and (t - Times[left]) > seconds_window:\n",
    "                left += 1\n",
    "            counts.append(i - left)\n",
    "        Out.extend(counts)\n",
    "    # assign (length should match df)\n",
    "    df[col_name] = Out\n",
    "\n",
    "# create 1h and 24h user tx count columns\n",
    "BuildUsersRollingFeatures(df, seconds_window=3600, col_name='UserTxCount1h')\n",
    "BuildUsersRollingFeatures(df, seconds_window=86400, col_name='UserTxCount24h')\n",
    "\n",
    "# user historical stats (means and std) using the Amount column\n",
    "UserStats = df.groupby('UserId')['Amount'].agg(['mean','std']).rename(columns={'mean':'UserAmountMean','std':'UserAmountStd'})\n",
    "df = df.merge(UserStats, left_on=True,how='left')\n",
    "\n",
    "# If merge created suffixed columns due to prior columns, ensure canonical names exist\n",
    "# prefer explicit names if present, otherwise take suffixed variants\n",
    "if 'UserAmountMean' not in df.columns:\n",
    "    for c in ['UserAmountMean_x', 'UserAmountMean_y']:\n",
    "        if c in df.columns:\n",
    "            df['UserAmountMean'] = df[c]\n",
    "            break\n",
    "if 'UserAmountStd' not in df.columns:\n",
    "    for c in ['UserAmountStd_x', 'UserAmountStd_y']:\n",
    "        if c in df.columns:\n",
    "            df['UserAmountStd'] = df[c]\n",
    "            break\n",
    "\n",
    "# Amount anomaly score (z-score) - guard divide-by-zero and missing std\n",
    "df['UserAmountStd'] = df['UserAmountStd'].replace(0, np.nan)\n",
    "df['AmtZScore'] = (df['Amount'] - df['UserAmountMean']) / df['UserAmountStd']\n",
    "df['AmtZScore'] = df['AmtZScore'].fillna(0)\n",
    "\n",
    "# device-country mismatch flag: assume user primary country is mode country (safe mode extraction)\n",
    "UserCountry = df.groupby('UserId')['Country'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan).rename('UserPrimaryCountry')\n",
    "df = df.merge(UserCountry, on='UserId', how='left')\n",
    "# use the correct merged column name 'UserPrimaryCountry'\n",
    "df['DeviceCountryMismatch'] = (df['Country'] != df['UserPrimaryCountry']).astype(int)\n",
    "\n",
    "# Flagging for night transactions (between 12 AM to 6 AM) -- correct boolean logic\n",
    "df['IsNightTransaction'] = df['hour'].apply(lambda x: 1 if (x > 0 and x < 6) else 0)\n",
    "\n",
    "# Short-term rolling features - ensure feature list matches created columns\n",
    "FeatCots = ['Amount', 'AmountLog', 'hour', 'weekday', 'UserTxCount1h', 'UserTxCount24h',\n",
    "            'UserAmountMean','AmtZScore','DeviceType','MerchantCategory','Country',\n",
    "            'UsdZarReturn','UsdZarReturn7DayVol','VixIndex','UsdZar7DayMean','RepoRate','MarketStress','IsNightTransaction','DeviceCountryMismatch']\n",
    "\n",
    "len(df), df[FeatCots].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91e420d",
   "metadata": {},
   "source": [
    " #5 Prepare training dataset\n",
    "- Choose modeling features.\n",
    "- Encode categoricals using a ColumnTransformer pipeline.\n",
    "- Train/test split stratified by label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367317b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell: 6 - prepare dataset for modeling\n",
    "LabelCol='IsFradud'\n",
    "#Some datasets have different naming conventions for the label column.\n",
    "if LabelCol not in df.columns:\n",
    "    LabelCol='Class' if 'Class' in df.columns else 'is_fraud'\n",
    "    Y=df[LabelCol].astype(int)\n",
    "#Select feautures and target\n",
    "NumFeatures=['Amount', 'AmountLog', 'hour', 'weekday', 'user_tx_count_1h', 'user_tx_count_24h', \n",
    "             'UserAmountMean','AmtZScore','DeviceType','MerchantCategory','Country',\n",
    "             'UsdZarReturn','UsdZarReturn7DayVol','VixIndex','UsdZar7DayMean','RepoRate','MarketStress','IsNightTransaction','DeviceCountryMismatch']\n",
    "CatFeatures=['device_type','merchant_category','country']\n",
    "X=df[NumFeatures + CatFeatures].copy()\n",
    "\n",
    "#Handle infinites and NaNs\n",
    "X=X.replace([np.inf, -np.inf], np.nan).fillana(0)\n",
    "\n",
    "#Train-test split\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X,Y, test_size=0.2, stratify=Y, random_state='RandomSeed')\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "print(\"Fraud rate in train:\", Y_train.mean(), \"in test:\", Y_test.mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f3676b",
   "metadata": {},
   "source": [
    "Preprocessing pipeline\n",
    "- Standard scale numerical features\n",
    "- One-hot encode small-cardinality categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a008d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 7 - preprocessing pipeline\n",
    "NumericTransormer=Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "Preprocessor=ColumnTransformer(transformers=[('Num', NumericTransormer, NumFeatures),\n",
    "                                            ('Cat', categorical_transformer, CatFeatures)])\n",
    "\n",
    "# Fit-transform training set to create feature matrix for simple models\n",
    "XtrainP=Preprocessor.fit_transform(X_train)\n",
    "XtestP=Preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Transformed feauture shape:\", XtrainP.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35ee5d",
   "metadata": {},
   "source": [
    "6) Handle class imbalance & Model training\n",
    "We will:\n",
    "- Train a LogisticRegression baseline with class weights.\n",
    "- Train a LightGBM model (primary).\n",
    "- Optionally use SMOTE for oversampling (toggle below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f498796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 8 - optional SMOTE toggle (use with caution)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "UseSmote=False\n",
    "\n",
    "# Ensure RandomSeed is available when cells are run out of order\n",
    "RandomSeed = globals().get('RandomSeed', 42)\n",
    "\n",
    "if UseSmote:\n",
    "    print(\"Applying SMOTE to balance the classes in the training set.\")\n",
    "    Sm = SMOTE(random_state=RandomSeed, n_jobs=-1)\n",
    "    XtrainPRes, YtrainPRes = Sm.fit_resample(XtrainP, Y_train)\n",
    "else:\n",
    "    XtrainPRes, YtrainPRes = XtrainP, Y_train\n",
    "\n",
    "print(\"Post-resample shape:\", XtrainPRes.shape, \"Fraud rate:\", YtrainPRes.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 9 - Logistic Regression baseline\n",
    "Lr=LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RandomSeed)\n",
    "Lr.fit(XtrainPRes, YtrainPRes)\n",
    "ProbLr=Lr.predict_proba(XtestP)[:,1]\n",
    "print(\"Logistic Regression AUC-ROC:\", roc_auc_score(Y_test, ProbLr))\n",
    "print(classification_report(Y_test, Lr.predict(XtestP)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0616ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 10 - LightGBM model (with sklearn wrapper)\n",
    "# We'll use plain lgb.Dataset approach via sklearn wrapper for convenience.\n",
    "lgbm=lgb.LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    objective='binary',\n",
    "    class_weight='balanced',\n",
    "    random_state=RandomSeed,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgbm.fit(XtrainPRes, YtrainPRes,\n",
    "         eval_set=[(XtestP, Y_test)],\n",
    "         eval_metric='auc',\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=100)\n",
    "\n",
    "ProbLgbm=lgbm.predict_proba(XtestP)[:,1]\n",
    "print(\"LightGBM AUC-ROC:\", roc_auc_score(Y_test, ProbLgbm))\n",
    "print(classification_report(Y_test, lgbm.predict(XtestP)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a567fd",
   "metadata": {},
   "source": [
    "## 7) Evaluation visuals & business-oriented metrics\n",
    "- ROC, PR curves\n",
    "- Confusion matrix at chosen threshold\n",
    "- Precision@K / Lift if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#ROC\n",
    "fpr, tpr, _=roc_curve(Y_test, ProbLgbm)\n",
    "roc_auc=auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f'LightGBM (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "#precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(Y_test, ProbLgbm)\n",
    "ap = average_precision_score(Y_test, ProbLgbm)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(recall, precision, label=f'LightGBM (AP = {ap:.3f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bca217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 12 - Confusion matrix at a chosen threshold\n",
    "def conf_matrix_at_threshold(y_true, probs, thresh=0.5):\n",
    "    Preds = (probs >= thresh).astype(int)\n",
    "    Cm = confusion_matrix(y_true, Preds)\n",
    "    tn, fp, fn, tp = Cm.ravel()\n",
    "    return {'tn':tn,'fp':fp,'fn':fn,'tp':tp,'cm':Cm}\n",
    "\n",
    "th = 0.5\n",
    "cm_stats = conf_matrix_at_threshold(Y_test, ProbLgbm, thresh=th)\n",
    "print(\"Threshold:\", th)\n",
    "print(cm_stats)\n",
    "sns.heatmap(cm_stats['Cm'], annot=True, fmt='d', cmap='Blues', xticklabels=['NotFraud','Fraud'], yticklabels=['NotFraud','Fraud'])\n",
    "plt.title(f'Confusion matrix @ threshold={th}')\n",
    "plt.show()\n",
    "\n",
    "# Show classification report for thresholded predictions\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "preds_th = (ProbLgbm >= th).astype(int)\n",
    "print(\"Precision:\", precision_score(Y_test, preds_th))\n",
    "print(\"Recall:\", recall_score(Y_test, preds_th))\n",
    "print(\"F1:\", f1_score(Y_test, preds_th))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1084a37",
   "metadata": {},
   "source": [
    " Business metric: Expected Loss Avoided (toy example)\n",
    "- Suppose average fraud amount is `avg_fraud_amt`.\n",
    "- False negative = missed fraud (cost = amount)\n",
    "- False positive = manual review cost = e.g., 5 USD\n",
    "Compute net saved loss vs baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63683c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 13 - toy business metric\n",
    "AvgFraudAmt=df.loc[df[LabelCol]==1, 'amount'].mean() if df[LabelCol].sum() >0 else 100\n",
    "ManuelReviewCost=5.0\n",
    "\n",
    "tn, fp, fn, tp = cm_stats['tn'], cm_stats['fp'], cm_stats['fn'], cm_stats['tp']\n",
    "ExpectedLossMissed=fn*AvgFraudAmt\n",
    "ExpectedManualReviewCost=(tp*fp) *ManuelReviewCost\n",
    "print(f\"Avg fraud amount: {AvgFraudAmt:.2f}\")\n",
    "print(f\"Missed frauds (fn): {fn}, cost approx: {ExpectedLossMissed:.2f}\")\n",
    "print(f\"Manual reviews (tp+fp): {(tp+fp)}, cost approx: {ExpectedManualReviewCost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a44eb",
   "metadata": {},
   "source": [
    "8) Explainability — SHAP\n",
    "SHAP helps explain LightGBM predictions (global and single prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d1a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 14 - SHAP explainability\n",
    "# Create a small sample to speed shap runtime\n",
    "import shap\n",
    "SampleIndices=np.random.choice(X_test.shape[0], size=min(1000, X_test.shape[0]), replace=False)\n",
    "XShapSample=X_test.iloc[SampleIndices]\n",
    "XShapSampleP=Preprocessor.transform(XShapSample)\n",
    "\n",
    "# Build feature names after preprocessing\n",
    "# numeric names + ohe names\n",
    "NumNames=NumFeatures\n",
    "ohe=Preprocessor.named_transformers_['Cat'].named_steps['ohe']\n",
    "oheNames=ohe.get_feature_names_out(CatFeatures).tolist()\n",
    "FeatureNames=NumNames + oheNames\n",
    "\n",
    "# Use TreeExplainer for LightGBM\n",
    "Explainer=shap.TreeExplainer(lgbm)\n",
    "ShapValues=Explainer.shap_values(XShapSampleP)\n",
    "\n",
    "# Summary plot (global importance)\n",
    "shap.summary_plot(ShapValues, XShapSampleP, feature_names=FeatureNames, plot_type='bar', show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a11f92",
   "metadata": {},
   "source": [
    "SHAP waterfall for a single example\n",
    "Pick a high-risk transaction from the test set and show local explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 15 - SHAP waterfall for single transaction (fixed)\n",
    "# pick the highest predicted risk transaction in the test set\n",
    "\n",
    "HighRiskIdx = np.argmax(ProbLgbm)\n",
    "\n",
    "# Use the original (preprocessed) pandas row from X_test, then transform\n",
    "SingleRow_df = X_test.iloc[[HighRiskIdx]]\n",
    "SingleRowP = Preprocessor.transform(SingleRow_df)  # shape (1, n_features)\n",
    "\n",
    "# Compute SHAP values for the single transformed row\n",
    "SingleShap = Explainer.shap_values(SingleRowP)\n",
    "\n",
    "# Handle binary-class vs single-array shap output: pick positive-class contributions\n",
    "if isinstance(SingleShap, list) and len(SingleShap) > 1:\n",
    "    shap_vals = SingleShap[1][0]  # positive class, first (and only) sample\n",
    "else:\n",
    "    # SingleShap could be array of shape (1, n) or (n,) depending on shap version\n",
    "    shap_vals = SingleShap[0] if getattr(SingleShap, 'ndim', 1) == 2 else SingleShap\n",
    "\n",
    "# Determine base value for positive class if provided as array\n",
    "ev = Explainer.expected_value\n",
    "base_value = ev[1] if (isinstance(ev, (list, np.ndarray)) and len(ev) > 1) else ev\n",
    "\n",
    "print(\"Predicted fraud probability for the highest risk transaction:\", ProbLgbm[HighRiskIdx])\n",
    "\n",
    "# Build Explanation and plot waterfall (data is the 1D feature vector)\n",
    "shap.waterfall_plot(\n",
    "    shap.Explanation(values=shap_vals,\n",
    "                     base_values=base_value,\n",
    "                     data=SingleRowP[0],\n",
    "                     feature_names=FeatureNames),\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345cdb66",
   "metadata": {},
   "source": [
    " 9) Save model & preprocessing pipeline\n",
    "Save both the trained LightGBM model and the preprocessing pipeline for later deployment (Streamlit or API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell: 16 -  Save the model and preprocessor\n",
    "OutDIr=\"models\"\n",
    "os.makedirs(OutDIr, exist_ok=True)\n",
    "\n",
    "joblib.dump(Preprocessor, os.path.join(OutDIr, \"preprocessor.joblib\"))\n",
    "joblib.dump(lgbm, os.path.join(OutDIr, \"lgbm_model.joblib\"))\n",
    "joblib.dump(Lr, os.path.join(OutDIr, \"logistic_regression_model.joblib\"))\n",
    "\n",
    "print(\"Saved preprocessor and models to ./models/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
