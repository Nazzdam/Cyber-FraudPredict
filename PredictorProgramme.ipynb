{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063b9db8",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "# Fraud & Cyber-Threat Prediction — End-to-End Notebook\n",
    "Goal:Predict fraudulent / cyber-risky transactions using transaction metadata, behavioral features, and market stress indicators.\n",
    "\n",
    "Author: Milani Chikeka  \n",
    "\n",
    "Seed:42\n",
    "---\n",
    "Sections\n",
    "1. Setup & imports  \n",
    "2. Load dataset (Kaggle `creditcard.csv` recommended) or simulate synthetic transactions  \n",
    "3. Market stress synthetic enrichment (USD/ZAR returns, VIX proxy, repo rate changes)  \n",
    "4. Feature engineering (behavioral + transaction + stress features)  \n",
    "5. Train/test split & imbalance handling  \n",
    "6. Models: Logistic Regression baseline + LightGBM (main)  \n",
    "7. Evaluation: ROC, PR, confusion matrix, business metrics  \n",
    "8. Explainability: SHAP plots  \n",
    "9. Save model & preprocessing pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87bd9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setups and imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings  \n",
    "#import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (roc_auc_score, precision_recall_curve,average_precision_score ,confusion_matrix,classification_report)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Optional libraies and seedings\n",
    "\"\"\" try:\n",
    "    import lightgbm as lgb\n",
    "except Exception as e:\n",
    "    print(\"install lightgbm: pip install lightgbm\")\n",
    "    raise e  \"\"\"\n",
    "\n",
    "try:\n",
    "    import shap \n",
    "except Exception as e:\n",
    "    print(\"install shap: pip install shap\")\n",
    "    raise e\n",
    "\n",
    "#This is for imbalanbce handling.\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except Exception as e:\n",
    "    print(\"Install imbalanced-learn: pip install imbalanced-learn\")    \n",
    "    raise e\n",
    "\n",
    "RandomSeed=42\n",
    "np.random.mtrand.RandomState(RandomSeed)\n",
    "\n",
    "#Style plotting\n",
    "sns.set(style=\"whitegrid\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e60d9",
   "metadata": {},
   "source": [
    "#2Load the date/create synthetic data.\n",
    "-Load the Kaggle dataset `creditcard.csv`, put it in `./data/creditcard.csv`.\n",
    "-If dataset is not present, create 'synthetic' transaction datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cb81502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not found. Will create a synthetic dataset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>amount</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>Browser</th>\n",
       "      <th>MerchantCategory</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7490</td>\n",
       "      <td>2020-01-01 01:42:32</td>\n",
       "      <td>58.326962</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>ZA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11953</td>\n",
       "      <td>2020-01-01 01:43:26</td>\n",
       "      <td>84.722107</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Travel</td>\n",
       "      <td>ZA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1134</td>\n",
       "      <td>2020-01-01 01:58:43</td>\n",
       "      <td>48.396960</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Food</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>354</td>\n",
       "      <td>2020-01-01 02:26:07</td>\n",
       "      <td>16.003870</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Food</td>\n",
       "      <td>CHN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>162</td>\n",
       "      <td>2020-01-01 02:28:43</td>\n",
       "      <td>48.281261</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   users           timestamp     amount  is_fraud DeviceType Browser  \\\n",
       "0   7490 2020-01-01 01:42:32  58.326962         0     Mobile  Chrome   \n",
       "1  11953 2020-01-01 01:43:26  84.722107         0     Tablet  Safari   \n",
       "2   1134 2020-01-01 01:58:43  48.396960         0     Mobile  Safari   \n",
       "3    354 2020-01-01 02:26:07  16.003870         0     Tablet  Safari   \n",
       "4    162 2020-01-01 02:28:43  48.281261         0     Tablet  Chrome   \n",
       "\n",
       "  MerchantCategory Country  \n",
       "0    Entertainment      ZA  \n",
       "1           Travel      ZA  \n",
       "2             Food     IND  \n",
       "3             Food     CHN  \n",
       "4    Entertainment     IND  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In this cell, the dataset is loaded and basic EDA is performed.\n",
    "DataDir=\"data\"\n",
    "os.makedirs(DataDir,exist_ok=True)\n",
    "Dataset=os.path.join(DataDir, \"./my_dataset.csv\") #Relative path to dataset\n",
    "\n",
    "if os.path.exists(Dataset):\n",
    "    print(\"Loading dataset from local directory.\")\n",
    "    df=pd.read_csv(Dataset)\n",
    "    #Kaggle datasets has \"TIME\", \"Amount\" , and \"Class\" columns.\n",
    "    #Create synthetic categorical features for demonstration.\n",
    "    df=df.reset_index(drop=True)\n",
    "    #A synthetic timeframe will be created\n",
    "    StartDate=datetime(2020,1,1)\n",
    "    df['timestamp']=df['Time'].apply(lambda x: StartDate + timedelta(seconds=int(x)))\n",
    "    #Categorical features are created\n",
    "    df['DeviceType']=np.random.choice(['Mobile','Desktop','Tablet'], size=10, p=[0.5,0.1,0.4])\n",
    "    df['Browser']=np.random.choice(['Chrome', 'Firefox', 'Safari', 'Edge'], size=10, p=[0.4,0.1,0.4,0.1])\n",
    "    df['MerchantCategory']=np.random.choice(['Retail', 'Food', 'Travel', 'Entertainment','Health'], size=10, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "    df['Country']=np.random.choice(['ZA','UK', 'US', 'CHN', 'IND'], size=10, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "    df=df.rename(columns={'Amount':'amount','Class':'is_fraud'})\n",
    "    \n",
    "else:\n",
    "    print(\"Dataset not found. Will create a synthetic dataset.\")\n",
    "    n=20000\n",
    "    #Simulate the users base\n",
    "    users=np.random.randint(1,20000, size=n)\n",
    "    StartDate=datetime(2020,1,1)\n",
    "    timestamps=[StartDate + timedelta(seconds=int(x)) \n",
    "                for x in np.random.exponential(scale=3600, size=n).cumsum()]\n",
    "    amounts=np.random.exponential(scale=100, size=n)\n",
    "#Labeling transactions as fraud or not based on amount and random noise\n",
    "IsFraud=(np.random.rand(n) < 0.002).astype(int) #Around 0.2% frauds\n",
    "DeviceType=np.random.choice(['Mobile','Desktop', 'Tablet'], size=n, p=[0.5,0.1,0.4])\n",
    "Browser=np.random.choice(['Chrome', 'Firefox', 'Safari', 'Edge'], size=n, p=[0.4,0.1,0.4,0.1])\n",
    "MerchantCategory=np.random.choice(['Retail', 'Food', 'Travel', 'Entertainment','Health'], size=n, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "Country=np.random.choice(['ZA','UK', 'US', 'CHN', 'IND'], size=n, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "\n",
    "df=pd.DataFrame({\n",
    "    'users': users,\n",
    "    'timestamp': timestamps,\n",
    "    'amount': amounts,\n",
    "    'is_fraud': IsFraud,\n",
    "    'DeviceType': DeviceType,\n",
    "    'Browser': Browser,\n",
    "    'MerchantCategory': MerchantCategory,\n",
    "    'Country': Country,\n",
    "})\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea330e2e",
   "metadata": {},
   "source": [
    " #3 Synthetic Market Stress Enrichment\n",
    "We will create a daily market stress series with:\n",
    "- USD/ZAR returns\n",
    "- VIX proxy (global vol)\n",
    "- SARB repo rate change flags\n",
    "\n",
    "Then merge the daily metrics onto each transaction by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e9469cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>UsdZar</th>\n",
       "      <th>VixIndex</th>\n",
       "      <th>RepoRate</th>\n",
       "      <th>UsdZarReturn</th>\n",
       "      <th>UsdZarReturn7DayVol</th>\n",
       "      <th>UsdZar7DayMean</th>\n",
       "      <th>MarketStress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>18.350827</td>\n",
       "      <td>18.079089</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.079089</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>19.061398</td>\n",
       "      <td>20.177443</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>0.027380</td>\n",
       "      <td>19.128266</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>20.427927</td>\n",
       "      <td>16.322595</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.071691</td>\n",
       "      <td>0.035884</td>\n",
       "      <td>18.193042</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>22.100541</td>\n",
       "      <td>24.790353</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.081879</td>\n",
       "      <td>0.036964</td>\n",
       "      <td>19.842370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>22.964764</td>\n",
       "      <td>19.737342</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.039104</td>\n",
       "      <td>0.032262</td>\n",
       "      <td>19.821365</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date     UsdZar   VixIndex  RepoRate  UsdZarReturn  \\\n",
       "0 2020-01-01  18.350827  18.079089    0.0675      0.000000   \n",
       "1 2020-01-02  19.061398  20.177443    0.0675      0.038721   \n",
       "2 2020-01-03  20.427927  16.322595    0.0675      0.071691   \n",
       "3 2020-01-04  22.100541  24.790353    0.0675      0.081879   \n",
       "4 2020-01-05  22.964764  19.737342    0.0675      0.039104   \n",
       "\n",
       "   UsdZarReturn7DayVol  UsdZar7DayMean  MarketStress  \n",
       "0             0.000000       18.079089             0  \n",
       "1             0.027380       19.128266             1  \n",
       "2             0.035884       18.193042             1  \n",
       "3             0.036964       19.842370             1  \n",
       "4             0.032262       19.821365             1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create market stress data\n",
    "#create a date range that covres the transaction timestamps\n",
    "min_date=df['timestamp'].min().date()\n",
    "max_date=df['timestamp'].max().date()\n",
    "dates=pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "# Simulate USD/ZAR daily returns (random walk with occasional shocks)\n",
    "np.random.seed(42)\n",
    "UsdZarLog=np.random.normal(loc=0, scale=0.01, size=len(dates))\n",
    "#Add random shocks\n",
    "Shock=np.random.choice(len(dates), size=int(len(dates)*0.05*len(dates)), replace=True)\n",
    "UsdZarLog[Shock] += np.random.normal(loc=0.05, scale=0.02, size=len(Shock))\n",
    "\n",
    "UsdZarLog=16.76*np.exp(np.cumsum(UsdZarLog)) #Starting rate around 16.76\n",
    "\n",
    "#VIX index simulation\n",
    "VixIndex=np.abs(np.random.normal(loc=12, size=len(dates)))\n",
    "VixIndex[Shock] += np.random.normal(10, 5, size=len(Shock))\n",
    "VixIndex=np.clip(VixIndex,10,None)\n",
    "\n",
    "#repo rate simulation\n",
    "RepoRate=np.full(len(dates),0.0675 ) #starts at 6.75%\n",
    "ChangeIndixes=np.random.choice(len(dates), size=int(0.002*len(dates)), replace=False)\n",
    "for idx in ChangeIndixes:\n",
    "    RepoRate[idx:] += np.random.choice([0.25, -0.25, 0.5, -0.5]) #Changes in basis points\n",
    "\n",
    "MarketData=pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'UsdZar': UsdZarLog,\n",
    "    'VixIndex': VixIndex,\n",
    "    'RepoRate': RepoRate\n",
    "})\n",
    "\n",
    "#Compute the returns and volatility\n",
    "MarketData['UsdzarReturn']=MarketData['UsdZar'].pct_change().fillna(0)\n",
    "MarketData['UsdZarReturn7DayVol']=MarketData['UsdzarReturn'].rolling(7, min_periods=1).std().fillna(0)\n",
    "MarketData['UsdZar7DayMean']=MarketData['VixIndex'].rolling(7, min_periods=1).mean().fillna(MarketData['VixIndex'])\n",
    "#Mark the stress when UsdZar is less than -2% or VIX index is above 20 or Repo rate above 8%\n",
    "MarketData['MarketStress']=((MarketData['UsdzarReturn'].abs() > 0.02) | (MarketData['VixIndex'] > MarketData['VixIndex'].quantile(0.90))).astype(int)\n",
    "MarketData.rename(columns={'UsdzarReturn': 'UsdZarReturn'}, inplace=True)\n",
    "\n",
    "MarketData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c26c13",
   "metadata": {},
   "source": [
    "Merge market features into transactions (by date).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d598669c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>amount</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>Browser</th>\n",
       "      <th>MerchantCategory</th>\n",
       "      <th>Country</th>\n",
       "      <th>date</th>\n",
       "      <th>UsdZar</th>\n",
       "      <th>VixIndex</th>\n",
       "      <th>RepoRate</th>\n",
       "      <th>UsdZarReturn</th>\n",
       "      <th>UsdZarReturn7DayVol</th>\n",
       "      <th>UsdZar7DayMean</th>\n",
       "      <th>MarketStress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7490</td>\n",
       "      <td>2020-01-01 01:42:32</td>\n",
       "      <td>58.326962</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>ZA</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>18.350827</td>\n",
       "      <td>18.079089</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.079089</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11953</td>\n",
       "      <td>2020-01-01 01:43:26</td>\n",
       "      <td>84.722107</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Travel</td>\n",
       "      <td>ZA</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>18.350827</td>\n",
       "      <td>18.079089</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.079089</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1134</td>\n",
       "      <td>2020-01-01 01:58:43</td>\n",
       "      <td>48.396960</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Food</td>\n",
       "      <td>IND</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>18.350827</td>\n",
       "      <td>18.079089</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.079089</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>354</td>\n",
       "      <td>2020-01-01 02:26:07</td>\n",
       "      <td>16.003870</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Food</td>\n",
       "      <td>CHN</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>18.350827</td>\n",
       "      <td>18.079089</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.079089</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>162</td>\n",
       "      <td>2020-01-01 02:28:43</td>\n",
       "      <td>48.281261</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>IND</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>18.350827</td>\n",
       "      <td>18.079089</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.079089</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   users           timestamp     amount  is_fraud DeviceType Browser  \\\n",
       "0   7490 2020-01-01 01:42:32  58.326962         0     Mobile  Chrome   \n",
       "1  11953 2020-01-01 01:43:26  84.722107         0     Tablet  Safari   \n",
       "2   1134 2020-01-01 01:58:43  48.396960         0     Mobile  Safari   \n",
       "3    354 2020-01-01 02:26:07  16.003870         0     Tablet  Safari   \n",
       "4    162 2020-01-01 02:28:43  48.281261         0     Tablet  Chrome   \n",
       "\n",
       "  MerchantCategory Country       date     UsdZar   VixIndex  RepoRate  \\\n",
       "0    Entertainment      ZA 2020-01-01  18.350827  18.079089    0.0675   \n",
       "1           Travel      ZA 2020-01-01  18.350827  18.079089    0.0675   \n",
       "2             Food     IND 2020-01-01  18.350827  18.079089    0.0675   \n",
       "3             Food     CHN 2020-01-01  18.350827  18.079089    0.0675   \n",
       "4    Entertainment     IND 2020-01-01  18.350827  18.079089    0.0675   \n",
       "\n",
       "   UsdZarReturn  UsdZarReturn7DayVol  UsdZar7DayMean  MarketStress  \n",
       "0           0.0                  0.0       18.079089             0  \n",
       "1           0.0                  0.0       18.079089             0  \n",
       "2           0.0                  0.0       18.079089             0  \n",
       "3           0.0                  0.0       18.079089             0  \n",
       "4           0.0                  0.0       18.079089             0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merge market data with transaction data by date\n",
    "df['date']=pd.to_datetime(df['timestamp'].dt.date)\n",
    "df=df.merge(MarketData, on='date', how='left')\n",
    "\n",
    "#Fill empty market data for missing dates\n",
    "df[['UsdZar', 'VixIndex', 'RepoRate', 'UsdZarReturn', 'UsdZarReturn7DayVol', 'UsdZar7DayMean', 'MarketStress']] = \\\n",
    "    df[['UsdZar', 'VixIndex', 'RepoRate', 'UsdZarReturn', 'UsdZarReturn7DayVol', 'UsdZar7DayMean', 'MarketStress']].fillna(method='ffill').fillna(0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b7cab",
   "metadata": {},
   "source": [
    "#4 Feature engineering — transactional & behavioral\n",
    "We create:\n",
    "- time features (hour, weekday)\n",
    "- log amount\n",
    "- rolling features per user (1h / 24h counts, avg amounts)\n",
    "- device-country mismatch flag\n",
    "- anomaly score: amount compared to user's historical mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d5a930e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,\n",
       "        Amount  AmountLog  hour  weekday  UserTxCount1h  UserTxCount24h  \\\n",
       " 0  350.575856   5.862425     5        6              0               0   \n",
       " 1   49.858492   3.929047     3        1              0               0   \n",
       " 2   74.093164   4.318730     4        2              0               0   \n",
       " 3   98.019721   4.595319     4        6              0               0   \n",
       " 4   75.181730   4.333122    14        3              0               0   \n",
       " \n",
       "    UserAmountMean  AmtZScore DeviceType MerchantCategory Country  \\\n",
       " 0      350.575856   0.000000    Desktop           Retail     CHN   \n",
       " 1       49.858492   0.000000     Tablet    Entertainment      US   \n",
       " 2       86.056443  -0.707107     Mobile           Retail      UK   \n",
       " 3       86.056443   0.707107     Tablet             Food     CHN   \n",
       " 4       80.909368  -0.707107     Mobile           Retail     CHN   \n",
       " \n",
       "    UsdZarReturn  UsdZarReturn7DayVol   VixIndex  UsdZar7DayMean  RepoRate  \\\n",
       " 0     -0.010913             0.032899  33.337353       24.975369    0.0675   \n",
       " 1      0.056815             0.029215  20.324761       19.984950    0.3175   \n",
       " 2      0.093860             0.021285  25.131695       19.896065    0.3175   \n",
       " 3      0.033562             0.026434  22.147811       24.374116    0.3175   \n",
       " 4      0.065887             0.025091  18.921004       20.104787    0.0675   \n",
       " \n",
       "    MarketStress  IsNightTransaction  DeviceCountryMismatch  \n",
       " 0             1                   1                      0  \n",
       " 1             1                   1                      0  \n",
       " 2             1                   1                      1  \n",
       " 3             1                   1                      0  \n",
       " 4             1                   0                      0  )"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature Engineering\n",
    "# Ensure timestamp is datetime dtype, then extract time features\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "# Create Amount column to match downstream expected naming and keep log\n",
    "if 'Amount' not in df.columns:\n",
    "    df['Amount'] = df['amount']\n",
    "df['AmountLog'] = np.log1p(df['Amount'])\n",
    "\n",
    "#Sort by user and timestamp and ensure UserId exists\n",
    "if 'UserId' not in df.columns:\n",
    "    df['UserId'] = np.random.randint(1, 20000, size=len(df))\n",
    "df = df.sort_values(by=['UserId', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Rolling features (user-level)\n",
    "# Compute rolling counts in the past 1 hour and 24 hours per user using two-pointer approach\n",
    "def BuildUsersRollingFeatures(df, seconds_window, col_name):\n",
    "    Out = []\n",
    "    # iterate groups in the same order as df (df already sorted by UserId and timestamp)\n",
    "    for uid, group in df.groupby('UserId', sort=False):\n",
    "        # seconds since epoch as numpy array for fast indexing\n",
    "        Times = (group['timestamp'].astype('int64') // 1_000_000_000).to_numpy()\n",
    "        counts = []\n",
    "        left = 0\n",
    "        for i, t in enumerate(Times):\n",
    "            while left < i and (t - Times[left]) > seconds_window:\n",
    "                left += 1\n",
    "            counts.append(i - left)\n",
    "        Out.extend(counts)\n",
    "    # assign (length should match df)\n",
    "    df[col_name] = Out\n",
    "\n",
    "# create 1h and 24h user tx count columns\n",
    "BuildUsersRollingFeatures(df, seconds_window=3600, col_name='UserTxCount1h')\n",
    "BuildUsersRollingFeatures(df, seconds_window=86400, col_name='UserTxCount24h')\n",
    "\n",
    "# user historical stats (means and std) using the Amount column\n",
    "UserStats = df.groupby('UserId')['Amount'].agg(['mean','std']).rename(columns={'mean':'UserAmountMean','std':'UserAmountStd'})\n",
    "# merge using left_on UserId and right_index since UserStats uses UserId as index\n",
    "df = df.merge(UserStats, left_on='UserId', right_index=True, how='left')\n",
    "\n",
    "# If merge created suffixed columns due to prior columns, ensure canonical names exist\n",
    "# prefer explicit names if present, otherwise take suffixed variants\n",
    "if 'UserAmountMean' not in df.columns:\n",
    "    for c in ['UserAmountMean_x', 'UserAmountMean_y']:\n",
    "        if c in df.columns:\n",
    "            df['UserAmountMean'] = df[c]\n",
    "            break\n",
    "if 'UserAmountStd' not in df.columns:\n",
    "    for c in ['UserAmountStd_x', 'UserAmountStd_y']:\n",
    "        if c in df.columns:\n",
    "            df['UserAmountStd'] = df[c]\n",
    "            break\n",
    "\n",
    "# Amount anomaly score (z-score) - guard divide-by-zero and missing std\n",
    "df['UserAmountStd'] = df['UserAmountStd'].replace(0, np.nan)\n",
    "df['AmtZScore'] = (df['Amount'] - df['UserAmountMean']) / df['UserAmountStd']\n",
    "df['AmtZScore'] = df['AmtZScore'].fillna(0)\n",
    "\n",
    "# device-country mismatch flag: assume user primary country is mode country (safe mode extraction)\n",
    "UserCountry = df.groupby('UserId')['Country'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan).rename('UserPrimaryCountry')\n",
    "df = df.merge(UserCountry, on='UserId', how='left')\n",
    "# use the correct merged column name 'UserPrimaryCountry'\n",
    "df['DeviceCountryMismatch'] = (df['Country'] != df['UserPrimaryCountry']).astype(int)\n",
    "\n",
    "# Flagging for night transactions (between 12 AM to 6 AM) -- correct boolean logic\n",
    "df['IsNightTransaction'] = df['hour'].apply(lambda x: 1 if (x > 0 and x < 6) else 0)\n",
    "\n",
    "# Short-term rolling features - ensure feature list matches created columns\n",
    "FeatCots = ['Amount', 'AmountLog', 'hour', 'weekday', 'UserTxCount1h', 'UserTxCount24h',\n",
    "            'UserAmountMean','AmtZScore','DeviceType','MerchantCategory','Country',\n",
    "            'UsdZarReturn','UsdZarReturn7DayVol','VixIndex','UsdZar7DayMean','RepoRate','MarketStress','IsNightTransaction','DeviceCountryMismatch']\n",
    "\n",
    "len(df), df[FeatCots].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91e420d",
   "metadata": {},
   "source": [
    " #5 Prepare training dataset\n",
    "- Choose modeling features.\n",
    "- Encode categoricals using a ColumnTransformer pipeline.\n",
    "- Train/test split stratified by label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "367317b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'random_state' parameter of train_test_split must be an int in the range [0, 4294967295], an instance of 'numpy.random.mtrand.RandomState' or None. Got 'RandomSeed' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidParameterError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m X=X.replace([np.inf, -np.inf], np.nan).fillna(\u001b[32m0\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m#Train-test split\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m X_train, X_test, Y_train, Y_test=\u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRandomSeed\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrain shape:\u001b[39m\u001b[33m\"\u001b[39m, X_train.shape, \u001b[33m\"\u001b[39m\u001b[33mTest shape:\u001b[39m\u001b[33m\"\u001b[39m, X_test.shape)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFraud rate in train:\u001b[39m\u001b[33m\"\u001b[39m, Y_train.mean(), \u001b[33m\"\u001b[39m\u001b[33min test:\u001b[39m\u001b[33m\"\u001b[39m, Y_test.mean())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:208\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    205\u001b[39m to_ignore += [\u001b[33m\"\u001b[39m\u001b[33mself\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcls\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    206\u001b[39m params = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params.arguments.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__qualname__\u001b[39;49m\n\u001b[32m    210\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:98\u001b[39m, in \u001b[36mvalidate_parameter_constraints\u001b[39m\u001b[34m(parameter_constraints, params, caller_name)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     93\u001b[39m     constraints_str = (\n\u001b[32m     94\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:-\u001b[32m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[32m     99\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m )\n",
      "\u001b[31mInvalidParameterError\u001b[39m: The 'random_state' parameter of train_test_split must be an int in the range [0, 4294967295], an instance of 'numpy.random.mtrand.RandomState' or None. Got 'RandomSeed' instead."
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell: 6 - prepare dataset for modeling\n",
    "LabelCol='IsFradud'\n",
    "#Some datasets have different naming conventions for the label column.\n",
    "if LabelCol not in df.columns:\n",
    "    LabelCol='Class' if 'Class' in df.columns else 'is_fraud'\n",
    "    Y=df[LabelCol].astype(int)\n",
    "#Select feautures and target\n",
    "NumFeatures=['Amount', 'AmountLog', 'hour', 'weekday', 'UserTxCount1h', 'UserTxCount24h', \n",
    "             'UserAmountMean','AmtZScore','DeviceType','MerchantCategory','Country',\n",
    "             'UsdZarReturn','UsdZarReturn7DayVol','VixIndex','UsdZar7DayMean','RepoRate','MarketStress','IsNightTransaction','DeviceCountryMismatch']\n",
    "CatFeatures=['DeviceType','MerchantCategory','Country']\n",
    "X=df[NumFeatures + CatFeatures].copy()\n",
    "\n",
    "#Handle infinites and NaNs\n",
    "X=X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "#Train-test split\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X,Y, test_size=0.2, stratify=Y, random_state='RandomSeed')\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "print(\"Fraud rate in train:\", Y_train.mean(), \"in test:\", Y_test.mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f3676b",
   "metadata": {},
   "source": [
    "Preprocessing pipeline\n",
    "- Standard scale numerical features\n",
    "- One-hot encode small-cardinality categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a008d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 7 - preprocessing pipeline\n",
    "NumericTransormer=Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "Preprocessor=ColumnTransformer(transformers=[('Num', NumericTransormer, NumFeatures),\n",
    "                                            ('Cat', categorical_transformer, CatFeatures)])\n",
    "\n",
    "# Fit-transform training set to create feature matrix for simple models\n",
    "XtrainP=Preprocessor.fit_transform(X_train)\n",
    "XtestP=Preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Transformed feauture shape:\", XtrainP.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35ee5d",
   "metadata": {},
   "source": [
    "6) Handle class imbalance & Model training\n",
    "We will:\n",
    "- Train a LogisticRegression baseline with class weights.\n",
    "- Train a LightGBM model (primary).\n",
    "- Optionally use SMOTE for oversampling (toggle below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f498796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 8 - optional SMOTE toggle (use with caution)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "UseSmote=False\n",
    "\n",
    "# Ensure RandomSeed is available when cells are run out of order\n",
    "RandomSeed = globals().get('RandomSeed', 42)\n",
    "\n",
    "if UseSmote:\n",
    "    print(\"Applying SMOTE to balance the classes in the training set.\")\n",
    "    Sm = SMOTE(random_state=RandomSeed, n_jobs=-1)\n",
    "    XtrainPRes, YtrainPRes = Sm.fit_resample(XtrainP, Y_train)\n",
    "else:\n",
    "    XtrainPRes, YtrainPRes = XtrainP, Y_train\n",
    "\n",
    "print(\"Post-resample shape:\", XtrainPRes.shape, \"Fraud rate:\", YtrainPRes.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 9 - Logistic Regression baseline\n",
    "Lr=LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RandomSeed)\n",
    "Lr.fit(XtrainPRes, YtrainPRes)\n",
    "ProbLr=Lr.predict_proba(XtestP)[:,1]\n",
    "print(\"Logistic Regression AUC-ROC:\", roc_auc_score(Y_test, ProbLr))\n",
    "print(classification_report(Y_test, Lr.predict(XtestP)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0616ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 10 - LightGBM model (with sklearn wrapper)\n",
    "# We'll use plain lgb.Dataset approach via sklearn wrapper for convenience.\n",
    "lgbm=lgb.LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    objective='binary',\n",
    "    class_weight='balanced',\n",
    "    random_state=RandomSeed,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgbm.fit(XtrainPRes, YtrainPRes,\n",
    "         eval_set=[(XtestP, Y_test)],\n",
    "         eval_metric='auc',\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=100)\n",
    "\n",
    "ProbLgbm=lgbm.predict_proba(XtestP)[:,1]\n",
    "print(\"LightGBM AUC-ROC:\", roc_auc_score(Y_test, ProbLgbm))\n",
    "print(classification_report(Y_test, lgbm.predict(XtestP)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a567fd",
   "metadata": {},
   "source": [
    "## 7) Evaluation visuals & business-oriented metrics\n",
    "- ROC, PR curves\n",
    "- Confusion matrix at chosen threshold\n",
    "- Precision@K / Lift if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#ROC\n",
    "fpr, tpr, _=roc_curve(Y_test, ProbLgbm)\n",
    "roc_auc=auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f'LightGBM (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "#precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(Y_test, ProbLgbm)\n",
    "ap = average_precision_score(Y_test, ProbLgbm)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(recall, precision, label=f'LightGBM (AP = {ap:.3f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bca217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 12 - Confusion matrix at a chosen threshold\n",
    "def conf_matrix_at_threshold(y_true, probs, thresh=0.5):\n",
    "    Preds = (probs >= thresh).astype(int)\n",
    "    Cm = confusion_matrix(y_true, Preds)\n",
    "    tn, fp, fn, tp = Cm.ravel()\n",
    "    return {'tn':tn,'fp':fp,'fn':fn,'tp':tp,'cm':Cm}\n",
    "\n",
    "th = 0.5\n",
    "cm_stats = conf_matrix_at_threshold(Y_test, ProbLgbm, thresh=th)\n",
    "print(\"Threshold:\", th)\n",
    "print(cm_stats)\n",
    "sns.heatmap(cm_stats['Cm'], annot=True, fmt='d', cmap='Blues', xticklabels=['NotFraud','Fraud'], yticklabels=['NotFraud','Fraud'])\n",
    "plt.title(f'Confusion matrix @ threshold={th}')\n",
    "plt.show()\n",
    "\n",
    "# Show classification report for thresholded predictions\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "preds_th = (ProbLgbm >= th).astype(int)\n",
    "print(\"Precision:\", precision_score(Y_test, preds_th))\n",
    "print(\"Recall:\", recall_score(Y_test, preds_th))\n",
    "print(\"F1:\", f1_score(Y_test, preds_th))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1084a37",
   "metadata": {},
   "source": [
    " Business metric: Expected Loss Avoided (toy example)\n",
    "- Suppose average fraud amount is `avg_fraud_amt`.\n",
    "- False negative = missed fraud (cost = amount)\n",
    "- False positive = manual review cost = e.g., 5 USD\n",
    "Compute net saved loss vs baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63683c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 13 - toy business metric\n",
    "AvgFraudAmt=df.loc[df[LabelCol]==1, 'amount'].mean() if df[LabelCol].sum() >0 else 100\n",
    "ManuelReviewCost=5.0\n",
    "\n",
    "tn, fp, fn, tp = cm_stats['tn'], cm_stats['fp'], cm_stats['fn'], cm_stats['tp']\n",
    "ExpectedLossMissed=fn*AvgFraudAmt\n",
    "ExpectedManualReviewCost=(tp*fp) *ManuelReviewCost\n",
    "print(f\"Avg fraud amount: {AvgFraudAmt:.2f}\")\n",
    "print(f\"Missed frauds (fn): {fn}, cost approx: {ExpectedLossMissed:.2f}\")\n",
    "print(f\"Manual reviews (tp+fp): {(tp+fp)}, cost approx: {ExpectedManualReviewCost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a44eb",
   "metadata": {},
   "source": [
    "8) Explainability — SHAP\n",
    "SHAP helps explain LightGBM predictions (global and single prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d1a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 14 - SHAP explainability\n",
    "# Create a small sample to speed shap runtime\n",
    "import shap\n",
    "SampleIndices=np.random.choice(X_test.shape[0], size=min(1000, X_test.shape[0]), replace=False)\n",
    "XShapSample=X_test.iloc[SampleIndices]\n",
    "XShapSampleP=Preprocessor.transform(XShapSample)\n",
    "\n",
    "# Build feature names after preprocessing\n",
    "# numeric names + ohe names\n",
    "NumNames=NumFeatures\n",
    "ohe=Preprocessor.named_transformers_['Cat'].named_steps['ohe']\n",
    "oheNames=ohe.get_feature_names_out(CatFeatures).tolist()\n",
    "FeatureNames=NumNames + oheNames\n",
    "\n",
    "# Use TreeExplainer for LightGBM\n",
    "Explainer=shap.TreeExplainer(lgbm)\n",
    "ShapValues=Explainer.shap_values(XShapSampleP)\n",
    "\n",
    "# Summary plot (global importance)\n",
    "shap.summary_plot(ShapValues, XShapSampleP, feature_names=FeatureNames, plot_type='bar', show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a11f92",
   "metadata": {},
   "source": [
    "SHAP waterfall for a single example\n",
    "Pick a high-risk transaction from the test set and show local explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 15 - SHAP waterfall for single transaction (fixed)\n",
    "# pick the highest predicted risk transaction in the test set\n",
    "\n",
    "HighRiskIdx = np.argmax(ProbLgbm)\n",
    "\n",
    "# Use the original (preprocessed) pandas row from X_test, then transform\n",
    "SingleRow_df = X_test.iloc[[HighRiskIdx]]\n",
    "SingleRowP = Preprocessor.transform(SingleRow_df)  # shape (1, n_features)\n",
    "\n",
    "# Compute SHAP values for the single transformed row\n",
    "SingleShap = Explainer.shap_values(SingleRowP)\n",
    "\n",
    "# Handle binary-class vs single-array shap output: pick positive-class contributions\n",
    "if isinstance(SingleShap, list) and len(SingleShap) > 1:\n",
    "    shap_vals = SingleShap[1][0]  # positive class, first (and only) sample\n",
    "else:\n",
    "    # SingleShap could be array of shape (1, n) or (n,) depending on shap version\n",
    "    shap_vals = SingleShap[0] if getattr(SingleShap, 'ndim', 1) == 2 else SingleShap\n",
    "\n",
    "# Determine base value for positive class if provided as array\n",
    "ev = Explainer.expected_value\n",
    "base_value = ev[1] if (isinstance(ev, (list, np.ndarray)) and len(ev) > 1) else ev\n",
    "\n",
    "print(\"Predicted fraud probability for the highest risk transaction:\", ProbLgbm[HighRiskIdx])\n",
    "\n",
    "# Build Explanation and plot waterfall (data is the 1D feature vector)\n",
    "shap.waterfall_plot(\n",
    "    shap.Explanation(values=shap_vals,\n",
    "                     base_values=base_value,\n",
    "                     data=SingleRowP[0],\n",
    "                     feature_names=FeatureNames),\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345cdb66",
   "metadata": {},
   "source": [
    " 9) Save model & preprocessing pipeline\n",
    "Save both the trained LightGBM model and the preprocessing pipeline for later deployment (Streamlit or API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell: 16 -  Save the model and preprocessor\n",
    "OutDIr=\"models\"\n",
    "os.makedirs(OutDIr, exist_ok=True)\n",
    "\n",
    "joblib.dump(Preprocessor, os.path.join(OutDIr, \"preprocessor.joblib\"))\n",
    "joblib.dump(lgbm, os.path.join(OutDIr, \"lgbm_model.joblib\"))\n",
    "joblib.dump(Lr, os.path.join(OutDIr, \"logistic_regression_model.joblib\"))\n",
    "\n",
    "print(\"Saved preprocessor and models to ./models/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
