{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063b9db8",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "# Fraud & Cyber-Threat Prediction — End-to-End Notebook\n",
    "Goal:Predict fraudulent / cyber-risky transactions using transaction metadata, behavioral features, and market stress indicators.\n",
    "\n",
    "Author: Milani Chikeka  \n",
    "\n",
    "Seed:42\n",
    "---\n",
    "Sections\n",
    "1. Setup & imports  \n",
    "2. Load dataset (Kaggle `creditcard.csv` recommended) or simulate synthetic transactions  \n",
    "3. Market stress synthetic enrichment (USD/ZAR returns, VIX proxy, repo rate changes)  \n",
    "4. Feature engineering (behavioral + transaction + stress features)  \n",
    "5. Train/test split & imbalance handling  \n",
    "6. Models: Logistic Regression baseline + LightGBM (main)  \n",
    "7. Evaluation: ROC, PR, confusion matrix, business metrics  \n",
    "8. Explainability: SHAP plots  \n",
    "9. Save model & preprocessing pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87bd9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setups and imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings  \n",
    "#import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (roc_auc_score, precision_recall_curve,average_precision_score ,confusion_matrix,classification_report)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Optional libraies and seedings\n",
    "\"\"\" try:\n",
    "    import lightgbm as lgb\n",
    "except Exception as e:\n",
    "    print(\"install lightgbm: pip install lightgbm\")\n",
    "    raise e  \"\"\"\n",
    "\n",
    "try:\n",
    "    import shap \n",
    "except Exception as e:\n",
    "    print(\"install shap: pip install shap\")\n",
    "    raise e\n",
    "\n",
    "#This is for imbalanbce handling.\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "except Exception as e:\n",
    "    print(\"Install imbalanced-learn: pip install imbalanced-learn\")    \n",
    "    raise e\n",
    "\n",
    "RandomSeed=42\n",
    "np.random.mtrand.RandomState(RandomSeed)\n",
    "\n",
    "#Style plotting\n",
    "sns.set(style=\"whitegrid\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e60d9",
   "metadata": {},
   "source": [
    "#2Load the date/create synthetic data.\n",
    "-Load the Kaggle dataset `creditcard.csv`, put it in `./data/creditcard.csv`.\n",
    "-If dataset is not present, create 'synthetic' transaction datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cb81502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not found. Will create a synthetic dataset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>amount</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>Browser</th>\n",
       "      <th>MerchantCategory</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13477</td>\n",
       "      <td>2020-01-01 01:46:15</td>\n",
       "      <td>85.203985</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Retail</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3536</td>\n",
       "      <td>2020-01-01 03:45:05</td>\n",
       "      <td>137.682193</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Travel</td>\n",
       "      <td>ZA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2591</td>\n",
       "      <td>2020-01-01 04:30:47</td>\n",
       "      <td>169.633725</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Retail</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12699</td>\n",
       "      <td>2020-01-01 05:53:39</td>\n",
       "      <td>159.192126</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Travel</td>\n",
       "      <td>ZA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2497</td>\n",
       "      <td>2020-01-01 07:36:13</td>\n",
       "      <td>18.125363</td>\n",
       "      <td>0</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Retail</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   users           timestamp      amount  is_fraud DeviceType  Browser  \\\n",
       "0  13477 2020-01-01 01:46:15   85.203985         0     Tablet   Chrome   \n",
       "1   3536 2020-01-01 03:45:05  137.682193         0     Tablet   Safari   \n",
       "2   2591 2020-01-01 04:30:47  169.633725         0     Mobile  Firefox   \n",
       "3  12699 2020-01-01 05:53:39  159.192126         0     Tablet   Safari   \n",
       "4   2497 2020-01-01 07:36:13   18.125363         0    Desktop   Chrome   \n",
       "\n",
       "  MerchantCategory Country  \n",
       "0           Retail     IND  \n",
       "1           Travel      ZA  \n",
       "2           Retail      US  \n",
       "3           Travel      ZA  \n",
       "4           Retail     IND  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In this cell, the dataset is loaded and basic EDA is performed.\n",
    "DataDir=\"data\"\n",
    "os.makedirs(DataDir,exist_ok=True)\n",
    "Dataset=os.path.join(DataDir, \"./my_dataset.csv\") #Relative path to dataset\n",
    "\n",
    "if os.path.exists(Dataset):\n",
    "    print(\"Loading dataset from local directory.\")\n",
    "    df=pd.read_csv(Dataset)\n",
    "    #Kaggle datasets has \"TIME\", \"Amount\" , and \"Class\" columns.\n",
    "    #Create synthetic categorical features for demonstration.\n",
    "    df=df.reset_index(drop=True)\n",
    "    #A synthetic timeframe will be created\n",
    "    StartDate=datetime(2020,1,1)\n",
    "    df['timestamp']=df['Time'].apply(lambda x: StartDate + timedelta(seconds=int(x)))\n",
    "    #Categorical features are created\n",
    "    df['DeviceType']=np.random.choice(['Mobile','Desktop','Tablet'], size=10, p=[0.5,0.1,0.4])\n",
    "    df['Browser']=np.random.choice(['Chrome', 'Firefox', 'Safari', 'Edge'], size=10, p=[0.4,0.1,0.4,0.1])\n",
    "    df['MerchantCategory']=np.random.choice(['Retail', 'Food', 'Travel', 'Entertainment','Health'], size=10, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "    df['Country']=np.random.choice(['ZA','UK', 'US', 'CHN', 'IND'], size=10, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "    df=df.rename(columns={'Amount':'amount','Class':'is_fraud'})\n",
    "    \n",
    "else:\n",
    "    print(\"Dataset not found. Will create a synthetic dataset.\")\n",
    "    n=20000\n",
    "    #Simulate the users base\n",
    "    users=np.random.randint(1,20000, size=n)\n",
    "    StartDate=datetime(2020,1,1)\n",
    "    timestamps=[StartDate + timedelta(seconds=int(x)) \n",
    "                for x in np.random.exponential(scale=3600, size=n).cumsum()]\n",
    "    amounts=np.random.exponential(scale=100, size=n)\n",
    "#Labeling transactions as fraud or not based on amount and random noise\n",
    "IsFraud=(np.random.rand(n) < 0.002).astype(int) #Around 0.2% frauds\n",
    "DeviceType=np.random.choice(['Mobile','Desktop', 'Tablet'], size=n, p=[0.5,0.1,0.4])\n",
    "Browser=np.random.choice(['Chrome', 'Firefox', 'Safari', 'Edge'], size=n, p=[0.4,0.1,0.4,0.1])\n",
    "MerchantCategory=np.random.choice(['Retail', 'Food', 'Travel', 'Entertainment','Health'], size=n, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "Country=np.random.choice(['ZA','UK', 'US', 'CHN', 'IND'], size=n, p=[0.3,0.2,0.2,0.2,0.1])\n",
    "\n",
    "df=pd.DataFrame({\n",
    "    'users': users,\n",
    "    'timestamp': timestamps,\n",
    "    'amount': amounts,\n",
    "    'is_fraud': IsFraud,\n",
    "    'DeviceType': DeviceType,\n",
    "    'Browser': Browser,\n",
    "    'MerchantCategory': MerchantCategory,\n",
    "    'Country': Country,\n",
    "})\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea330e2e",
   "metadata": {},
   "source": [
    " #3 Synthetic Market Stress Enrichment\n",
    "We will create a daily market stress series with:\n",
    "- USD/ZAR returns\n",
    "- VIX proxy (global vol)\n",
    "- SARB repo rate change flags\n",
    "\n",
    "Then merge the daily metrics onto each transaction by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9469cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>UsdZar</th>\n",
       "      <th>VixIndex</th>\n",
       "      <th>RepoRate</th>\n",
       "      <th>UsdZarReturn</th>\n",
       "      <th>UsdZarReturn7DayVol</th>\n",
       "      <th>UsdZar7DayMean</th>\n",
       "      <th>MarketStress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17.725266</td>\n",
       "      <td>23.807031</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.807031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>18.621960</td>\n",
       "      <td>24.755790</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.050589</td>\n",
       "      <td>0.035771</td>\n",
       "      <td>24.281411</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>20.470605</td>\n",
       "      <td>16.633737</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.099272</td>\n",
       "      <td>0.049639</td>\n",
       "      <td>21.732186</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>22.262195</td>\n",
       "      <td>23.085662</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.087520</td>\n",
       "      <td>0.044671</td>\n",
       "      <td>22.070555</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>23.094689</td>\n",
       "      <td>16.502542</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.037395</td>\n",
       "      <td>0.039912</td>\n",
       "      <td>20.956953</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date     UsdZar   VixIndex  RepoRate  UsdZarReturn  \\\n",
       "0 2020-01-01  17.725266  23.807031    0.0675      0.000000   \n",
       "1 2020-01-02  18.621960  24.755790    0.0675      0.050589   \n",
       "2 2020-01-03  20.470605  16.633737    0.0675      0.099272   \n",
       "3 2020-01-04  22.262195  23.085662    0.0675      0.087520   \n",
       "4 2020-01-05  23.094689  16.502542    0.0675      0.037395   \n",
       "\n",
       "   UsdZarReturn7DayVol  UsdZar7DayMean  MarketStress  \n",
       "0             0.000000       23.807031             0  \n",
       "1             0.035771       24.281411             1  \n",
       "2             0.049639       21.732186             1  \n",
       "3             0.044671       22.070555             1  \n",
       "4             0.039912       20.956953             1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create market stress data\n",
    "#create a date range that covres the transaction timestamps\n",
    "min_date=df['timestamp'].min().date()\n",
    "max_date=df['timestamp'].max().date()\n",
    "dates=pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "# Simulate USD/ZAR daily returns (random walk with occasional shocks)\n",
    "np.random.seed(42)\n",
    "UsdZarLog=np.random.normal(loc=0, scale=0.01, size=len(dates))\n",
    "#Add random shocks\n",
    "Shock=np.random.choice(len(dates), size=int(len(dates)*0.05*len(dates)), replace=True)\n",
    "UsdZarLog[Shock] += np.random.normal(loc=0.05, scale=0.02, size=len(Shock))\n",
    "\n",
    "UsdZarLog=16.76*np.exp(np.cumsum(UsdZarLog)) #Starting rate around 16.76\n",
    "\n",
    "#VIX index simulation\n",
    "VixIndex=np.abs(np.random.normal(loc=12, size=len(dates)))\n",
    "VixIndex[Shock] += np.random.normal(10, 5, size=len(Shock))\n",
    "VixIndex=np.clip(VixIndex,10,None)\n",
    "\n",
    "#repo rate simulation\n",
    "RepoRate=np.full(len(dates),0.0675 ) #starts at 6.75%\n",
    "ChangeIndixes=np.random.choice(len(dates), size=int(0.002*len(dates)), replace=False)\n",
    "for idx in ChangeIndixes:\n",
    "    RepoRate[idx:] += np.random.choice([0.25, -0.25, 0.5, -0.5]) #Changes in basis points\n",
    "\n",
    "MarketData=pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'UsdZar': UsdZarLog,\n",
    "    'VixIndex': VixIndex,\n",
    "    'RepoRate': RepoRate\n",
    "})\n",
    "\n",
    "#Compute the returns and volatility\n",
    "MarketData['UsdzarReturn']=MarketData['UsdZar'].pct_change().fillna(0)\n",
    "MarketData['UsdZarReturn7DayVol']=MarketData['UsdzarReturn'].rolling(7, min_periods=1).std().fillna(0)\n",
    "MarketData['UsdZar7DayMean']=MarketData['VixIndex'].rolling(7, min_periods=1).mean().fillna(MarketData['VixIndex'])\n",
    "#Mark the stress when UsdZar is less than -2% or VIX index is above 20 or Repo rate above 8%\n",
    "MarketData['MarketStress']=((MarketData['UsdzarReturn'].abs() > 0.02) | (MarketData['VixIndex'] > MarketData['VixIndex'].quantile(0.90))).astype(int)\n",
    "MarketData.rename(columns={'UsdzarReturn': 'UsdZarReturn'}, inplace=True)\n",
    "\n",
    "MarketData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c26c13",
   "metadata": {},
   "source": [
    "Merge market features into transactions (by date).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d598669c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>amount</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>Browser</th>\n",
       "      <th>MerchantCategory</th>\n",
       "      <th>Country</th>\n",
       "      <th>date</th>\n",
       "      <th>UsdZar</th>\n",
       "      <th>VixIndex</th>\n",
       "      <th>RepoRate</th>\n",
       "      <th>UsdZarReturn</th>\n",
       "      <th>UsdZarReturn7DayVol</th>\n",
       "      <th>UsdZar7DayMean</th>\n",
       "      <th>MarketStress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13477</td>\n",
       "      <td>2020-01-01 01:46:15</td>\n",
       "      <td>85.203985</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Retail</td>\n",
       "      <td>IND</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17.725266</td>\n",
       "      <td>23.807031</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.807031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3536</td>\n",
       "      <td>2020-01-01 03:45:05</td>\n",
       "      <td>137.682193</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Travel</td>\n",
       "      <td>ZA</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17.725266</td>\n",
       "      <td>23.807031</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.807031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2591</td>\n",
       "      <td>2020-01-01 04:30:47</td>\n",
       "      <td>169.633725</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>Retail</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17.725266</td>\n",
       "      <td>23.807031</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.807031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12699</td>\n",
       "      <td>2020-01-01 05:53:39</td>\n",
       "      <td>159.192126</td>\n",
       "      <td>0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Safari</td>\n",
       "      <td>Travel</td>\n",
       "      <td>ZA</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17.725266</td>\n",
       "      <td>23.807031</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.807031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2497</td>\n",
       "      <td>2020-01-01 07:36:13</td>\n",
       "      <td>18.125363</td>\n",
       "      <td>0</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>Retail</td>\n",
       "      <td>IND</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17.725266</td>\n",
       "      <td>23.807031</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.807031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   users           timestamp      amount  is_fraud DeviceType  Browser  \\\n",
       "0  13477 2020-01-01 01:46:15   85.203985         0     Tablet   Chrome   \n",
       "1   3536 2020-01-01 03:45:05  137.682193         0     Tablet   Safari   \n",
       "2   2591 2020-01-01 04:30:47  169.633725         0     Mobile  Firefox   \n",
       "3  12699 2020-01-01 05:53:39  159.192126         0     Tablet   Safari   \n",
       "4   2497 2020-01-01 07:36:13   18.125363         0    Desktop   Chrome   \n",
       "\n",
       "  MerchantCategory Country       date     UsdZar   VixIndex  RepoRate  \\\n",
       "0           Retail     IND 2020-01-01  17.725266  23.807031    0.0675   \n",
       "1           Travel      ZA 2020-01-01  17.725266  23.807031    0.0675   \n",
       "2           Retail      US 2020-01-01  17.725266  23.807031    0.0675   \n",
       "3           Travel      ZA 2020-01-01  17.725266  23.807031    0.0675   \n",
       "4           Retail     IND 2020-01-01  17.725266  23.807031    0.0675   \n",
       "\n",
       "   UsdZarReturn  UsdZarReturn7DayVol  UsdZar7DayMean  MarketStress  \n",
       "0           0.0                  0.0       23.807031             0  \n",
       "1           0.0                  0.0       23.807031             0  \n",
       "2           0.0                  0.0       23.807031             0  \n",
       "3           0.0                  0.0       23.807031             0  \n",
       "4           0.0                  0.0       23.807031             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merge market data with transaction data by date\n",
    "df['date']=pd.to_datetime(df['timestamp'].dt.date)\n",
    "df=df.merge(MarketData, on='date', how='left')\n",
    "\n",
    "#Fill empty market data for missing dates\n",
    "df[['UsdZar', 'VixIndex', 'RepoRate', 'UsdZarReturn', 'UsdZarReturn7DayVol', 'UsdZar7DayMean', 'MarketStress']] = \\\n",
    "    df[['UsdZar', 'VixIndex', 'RepoRate', 'UsdZarReturn', 'UsdZarReturn7DayVol', 'UsdZar7DayMean', 'MarketStress']].fillna(method='ffill').fillna(0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b7cab",
   "metadata": {},
   "source": [
    "#4 Feature engineering — transactional & behavioral\n",
    "We create:\n",
    "- time features (hour, weekday)\n",
    "- log amount\n",
    "- rolling features per user (1h / 24h counts, avg amounts)\n",
    "- device-country mismatch flag\n",
    "- anomaly score: amount compared to user's historical mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d5a930e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,\n",
       "        Amount  AmountLog  hour  weekday  UserTxCount1h  UserTxCount24h  \\\n",
       " 0   87.705134   4.485318    17        1              0               0   \n",
       " 1   14.591422   2.746721     6        3              0               0   \n",
       " 2  201.224943   5.309381    11        3              0               0   \n",
       " 3  521.058791   6.257780    20        6              0               0   \n",
       " 4   37.237430   3.643815     9        5              0               0   \n",
       " \n",
       "    UserAmountMean  AmtZScore DeviceType MerchantCategory Country  \\\n",
       " 0       87.705134   0.000000     Mobile    Entertainment      US   \n",
       " 1       14.591422   0.000000     Mobile           Retail      US   \n",
       " 2      361.141867  -0.707107     Tablet    Entertainment      US   \n",
       " 3      361.141867   0.707107    Desktop           Retail     IND   \n",
       " 4       44.917405  -0.707107     Mobile           Retail      UK   \n",
       " \n",
       "    UsdZarReturn  UsdZarReturn7DayVol   VixIndex  UsdZar7DayMean  RepoRate  \\\n",
       " 0      0.028395             0.019923  27.064303       22.789020    0.0675   \n",
       " 1      0.031167             0.023078  24.187966       23.525089    0.3175   \n",
       " 2      0.014536             0.031480  20.786710       19.816028    0.0675   \n",
       " 3      0.065987             0.012839  25.064987       20.137922    0.3175   \n",
       " 4      0.065413             0.017777  18.229856       21.813991    0.3175   \n",
       " \n",
       "    MarketStress  IsNightTransaction  DeviceCountryMismatch  \n",
       " 0             1                   0                      0  \n",
       " 1             1                   0                      0  \n",
       " 2             0                   0                      1  \n",
       " 3             1                   0                      0  \n",
       " 4             1                   0                      1  )"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature Engineering\n",
    "# Remove duplicate columns before proceeding\n",
    "df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
    "\n",
    "# Ensure timestamp is datetime dtype, then extract time features\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['weekday'] = df['timestamp'].dt.weekday\n",
    "\n",
    "# Create Amount column to match downstream expected naming and keep log\n",
    "if 'Amount' not in df.columns:\n",
    "    df['Amount'] = df['amount']\n",
    "df['AmountLog'] = np.log1p(df['Amount'])\n",
    "\n",
    "#Sort by user and timestamp and ensure UserId exists\n",
    "if 'UserId' not in df.columns:\n",
    "    df['UserId'] = np.random.randint(1, 20000, size=len(df))\n",
    "df = df.sort_values(by=['UserId', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Rolling features (user-level)\n",
    "# Compute rolling counts in the past 1 hour and 24 hours per user using two-pointer approach\n",
    "def BuildUsersRollingFeatures(df, seconds_window, col_name):\n",
    "    Out = []\n",
    "    # iterate groups in the same order as df (df already sorted by UserId and timestamp)\n",
    "    for uid, group in df.groupby('UserId', sort=False):\n",
    "        # seconds since epoch as numpy array for fast indexing\n",
    "        Times = (group['timestamp'].astype('int64') // 1_000_000_000).to_numpy()\n",
    "        counts = []\n",
    "        left = 0\n",
    "        for i, t in enumerate(Times):\n",
    "            while left < i and (t - Times[left]) > seconds_window:\n",
    "                left += 1\n",
    "            counts.append(i - left)\n",
    "        Out.extend(counts)\n",
    "    # assign (length should match df)\n",
    "    df[col_name] = Out\n",
    "\n",
    "# create 1h and 24h user tx count columns\n",
    "BuildUsersRollingFeatures(df, seconds_window=3600, col_name='UserTxCount1h')\n",
    "BuildUsersRollingFeatures(df, seconds_window=86400, col_name='UserTxCount24h')\n",
    "\n",
    "# user historical stats (means and std) using the Amount column\n",
    "UserStats = df.groupby('UserId')['Amount'].agg(['mean','std']).rename(columns={'mean':'UserAmountMean','std':'UserAmountStd'})\n",
    "# merge using left_on UserId and right_index since UserStats uses UserId as index\n",
    "df = df.merge(UserStats, left_on='UserId', right_index=True, how='left')\n",
    "\n",
    "# If merge created suffixed columns due to prior columns, ensure canonical names exist\n",
    "# prefer explicit names if present, otherwise take suffixed variants\n",
    "if 'UserAmountMean' not in df.columns:\n",
    "    for c in ['UserAmountMean_x', 'UserAmountMean_y']:\n",
    "        if c in df.columns:\n",
    "            df['UserAmountMean'] = df[c]\n",
    "            break\n",
    "if 'UserAmountStd' not in df.columns:\n",
    "    for c in ['UserAmountStd_x', 'UserAmountStd_y']:\n",
    "        if c in df.columns:\n",
    "            df['UserAmountStd'] = df[c]\n",
    "            break\n",
    "\n",
    "# Amount anomaly score (z-score) - guard divide-by-zero and missing std\n",
    "df['UserAmountStd'] = df['UserAmountStd'].replace(0, np.nan)\n",
    "df['AmtZScore'] = (df['Amount'] - df['UserAmountMean']) / df['UserAmountStd']\n",
    "df['AmtZScore'] = df['AmtZScore'].fillna(0)\n",
    "\n",
    "# device-country mismatch flag: assume user primary country is mode country (safe mode extraction)\n",
    "UserCountry = df.groupby('UserId')['Country'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan).rename('UserPrimaryCountry')\n",
    "df = df.merge(UserCountry, on='UserId', how='left')\n",
    "# use the correct merged column name 'UserPrimaryCountry'\n",
    "df['DeviceCountryMismatch'] = (df['Country'] != df['UserPrimaryCountry']).astype(int)\n",
    "\n",
    "# Flagging for night transactions (between 12 AM to 6 AM) -- correct boolean logic\n",
    "df['IsNightTransaction'] = df['hour'].apply(lambda x: 1 if (x > 0 and x < 6) else 0)\n",
    "\n",
    "# Short-term rolling features - ensure feature list matches created columns\n",
    "FeatCots = ['Amount', 'AmountLog', 'hour', 'weekday', 'UserTxCount1h', 'UserTxCount24h',\n",
    "            'UserAmountMean','AmtZScore','DeviceType','MerchantCategory','Country',\n",
    "            'UsdZarReturn','UsdZarReturn7DayVol','VixIndex','UsdZar7DayMean','RepoRate','MarketStress','IsNightTransaction','DeviceCountryMismatch']\n",
    "\n",
    "len(df), df[FeatCots].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91e420d",
   "metadata": {},
   "source": [
    " #5 Prepare training dataset\n",
    "- Choose modeling features.\n",
    "- Encode categoricals using a ColumnTransformer pipeline.\n",
    "- Train/test split stratified by label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "367317b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (16000, 22) Test shape: (4000, 22)\n",
      "Fraud rate in train: 0.001625 in test: 0.00175\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell: 6 - prepare dataset for modeling\n",
    "LabelCol='IsFradud'\n",
    "#Some datasets have different naming conventions for the label column.\n",
    "if LabelCol not in df.columns:\n",
    "    LabelCol='Class' if 'Class' in df.columns else 'is_fraud'\n",
    "    Y=df[LabelCol].astype(int)\n",
    "#Select feautures and target\n",
    "NumFeatures=['Amount', 'AmountLog', 'hour', 'weekday', 'UserTxCount1h', 'UserTxCount24h', \n",
    "             'UserAmountMean','AmtZScore','DeviceType','MerchantCategory','Country',\n",
    "             'UsdZarReturn','UsdZarReturn7DayVol','VixIndex','UsdZar7DayMean','RepoRate','MarketStress','IsNightTransaction','DeviceCountryMismatch']\n",
    "CatFeatures=['DeviceType','MerchantCategory','Country']\n",
    "X=df[NumFeatures + CatFeatures].copy()\n",
    "\n",
    "#Handle infinites and NaNs\n",
    "X=X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "#Train-test split\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X,Y, test_size=0.2, stratify=Y, random_state=RandomSeed)\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "print(\"Fraud rate in train:\", Y_train.mean(), \"in test:\", Y_test.mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f3676b",
   "metadata": {},
   "source": [
    "Preprocessing pipeline\n",
    "- Standard scale numerical features\n",
    "- One-hot encode small-cardinality categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a008d458",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Tablet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m/var/folders/g6/ybd9b4f15qbbw80zvl_072ym0000gn/T/ipykernel_75599/2285398259.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     13\u001b[39m Preprocessor=ColumnTransformer(transformers=[('Num', NumericTransormer, NumFeatures),\n\u001b[32m     14\u001b[39m                                             (\u001b[33m'Cat'\u001b[39m, categorical_transformer, CatFeatures)])\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Fit-transform training set to create feature matrix for simple models\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m XtrainP=Preprocessor.fit_transform(X_train)\n\u001b[32m     18\u001b[39m XtestP=Preprocessor.transform(X_test)\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m print(\u001b[33m\"Transformed feauture shape:\"\u001b[39m, XtrainP.shape)\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/_set_output.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m     @wraps(f)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m wrapped(self, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         data_to_wrap = f(self, X, *args, **kwargs)\n\u001b[32m    317\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m isinstance(data_to_wrap, tuple):\n\u001b[32m    318\u001b[39m             \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m             return_tuple = (\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1332\u001b[39m                 skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m                     prefer_skip_nested_validation \u001b[38;5;28;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m                 )\n\u001b[32m   1335\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/compose/_column_transformer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    995\u001b[39m             routed_params = process_routing(self, \u001b[33m\"fit_transform\"\u001b[39m, **params)\n\u001b[32m    996\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    997\u001b[39m             routed_params = self._get_empty_routing()\n\u001b[32m    998\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m999\u001b[39m         result = self._call_func_on_transformers(\n\u001b[32m   1000\u001b[39m             X,\n\u001b[32m   1001\u001b[39m             y,\n\u001b[32m   1002\u001b[39m             _fit_transform_one,\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/compose/_column_transformer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, func, column_as_labels, routed_params)\u001b[39m\n\u001b[32m    903\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m ValueError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    904\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"Expected 2D array, got 1D array instead\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m str(e):\n\u001b[32m    905\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m ValueError(_ERR_MSG_1DCOLUMN) \u001b[38;5;28;01mfrom\u001b[39;00m e\n\u001b[32m    906\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/parallel.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     87\u001b[39m                 kwargs,\n\u001b[32m     88\u001b[39m             )\n\u001b[32m     89\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;28;01min\u001b[39;00m iterable\n\u001b[32m     90\u001b[39m         )\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m super().__call__(iterable_with_config_and_warning_filters)\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/joblib/parallel.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1982\u001b[39m             \u001b[38;5;66;03m# If n_jobs==1, run the computation sequentially and return\u001b[39;00m\n\u001b[32m   1983\u001b[39m             \u001b[38;5;66;03m# immediately to avoid overheads.\u001b[39;00m\n\u001b[32m   1984\u001b[39m             output = self._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m             next(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m self.return_generator \u001b[38;5;28;01melse\u001b[39;00m list(output)\n\u001b[32m   1987\u001b[39m \n\u001b[32m   1988\u001b[39m         \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m         \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/joblib/parallel.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1924\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1925\u001b[39m             self._running = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1926\u001b[39m             self._iterating = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1927\u001b[39m             self._original_iterator = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1928\u001b[39m             self.print_progress()\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/parallel.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    180\u001b[39m                             this_warning_filter_dict[special_key] = this_value.pattern\n\u001b[32m    181\u001b[39m \n\u001b[32m    182\u001b[39m                     warnings.filterwarnings(**this_warning_filter_dict, append=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    183\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.function(*args, **kwargs)\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/pipeline.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1480\u001b[39m     \"\"\"\n\u001b[32m   1481\u001b[39m     params = params \u001b[38;5;28;01mor\u001b[39;00m {}\n\u001b[32m   1482\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1483\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m hasattr(transformer, \u001b[33m\"fit_transform\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m             res = transformer.fit_transform(X, y, **params.get(\u001b[33m\"fit_transform\"\u001b[39m, {}))\n\u001b[32m   1485\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1486\u001b[39m             res = transformer.fit(X, y, **params.get(\"fit\", {})).transform(\n\u001b[32m   1487\u001b[39m                 X, **params.get(\u001b[33m\"transform\"\u001b[39m, {})\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1332\u001b[39m                 skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m                     prefer_skip_nested_validation \u001b[38;5;28;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m                 )\n\u001b[32m   1335\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/pipeline.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    685\u001b[39m                 step_params=routed_params[self.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    686\u001b[39m                 all_params=params,\n\u001b[32m    687\u001b[39m             )\n\u001b[32m    688\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m hasattr(last_step, \u001b[33m\"fit_transform\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m                 return last_step.fit_transform(\n\u001b[32m    690\u001b[39m                     Xt, y, **last_step_params[\u001b[33m\"fit_transform\"\u001b[39m]\n\u001b[32m    691\u001b[39m                 )\n\u001b[32m    692\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/_set_output.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m     @wraps(f)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m wrapped(self, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         data_to_wrap = f(self, X, *args, **kwargs)\n\u001b[32m    317\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m isinstance(data_to_wrap, tuple):\n\u001b[32m    318\u001b[39m             \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m             return_tuple = (\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    903\u001b[39m                 )\n\u001b[32m    904\u001b[39m \n\u001b[32m    905\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    906\u001b[39m             \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, **fit_params).transform(X)\n\u001b[32m    908\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    909\u001b[39m             \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    910\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/preprocessing/_data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    920\u001b[39m             Fitted scaler.\n\u001b[32m    921\u001b[39m         \"\"\"\n\u001b[32m    922\u001b[39m         \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    923\u001b[39m         self._reset()\n\u001b[32m--> \u001b[39m\u001b[32m924\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.partial_fit(X, y, sample_weight)\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1332\u001b[39m                 skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m                     prefer_skip_nested_validation \u001b[38;5;28;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m                 )\n\u001b[32m   1335\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/preprocessing/_data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    957\u001b[39m             Fitted scaler.\n\u001b[32m    958\u001b[39m         \"\"\"\n\u001b[32m    959\u001b[39m         xp, _, X_device = get_namespace_and_device(X)\n\u001b[32m    960\u001b[39m         first_call = \u001b[38;5;28;01mnot\u001b[39;00m hasattr(self, \u001b[33m\"n_samples_seen_\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m         X = validate_data(\n\u001b[32m    962\u001b[39m             self,\n\u001b[32m    963\u001b[39m             X,\n\u001b[32m    964\u001b[39m             accept_sparse=(\u001b[33m\"csr\"\u001b[39m, \u001b[33m\"csc\"\u001b[39m),\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2898\u001b[39m             out = y\n\u001b[32m   2899\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2900\u001b[39m             out = X, y\n\u001b[32m   2901\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2902\u001b[39m         out = check_array(X, input_name=\u001b[33m\"X\"\u001b[39m, **check_params)\n\u001b[32m   2903\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2904\u001b[39m         out = _check_y(y, **check_params)\n\u001b[32m   2905\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1019\u001b[39m                         )\n\u001b[32m   1020\u001b[39m                     array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1021\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1022\u001b[39m                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1024\u001b[39m                 raise ValueError(\n\u001b[32m   1025\u001b[39m                     \u001b[33m\"Complex data not supported\\n{}\\n\"\u001b[39m.format(array)\n\u001b[32m   1026\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m complex_warning\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/_array_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    874\u001b[39m         \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[32m    875\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    876\u001b[39m             array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    877\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m             array = numpy.asarray(array, order=order, dtype=dtype)\n\u001b[32m    879\u001b[39m \n\u001b[32m    880\u001b[39m         \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n",
      "\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2164\u001b[39m             )\n\u001b[32m   2165\u001b[39m         values = self._values\n\u001b[32m   2166\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2167\u001b[39m             \u001b[38;5;66;03m# Note: branch avoids `copy=None` for NumPy 1.x support\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2168\u001b[39m             arr = np.asarray(values, dtype=dtype)\n\u001b[32m   2169\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2170\u001b[39m             arr = np.array(values, dtype=dtype, copy=copy)\n\u001b[32m   2171\u001b[39m \n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'Tablet'"
     ]
    }
   ],
   "source": [
    "# Cell: 7 - preprocessing pipeline\n",
    "# Remove duplicate columns from X_train and X_test before preprocessing\n",
    "X_train = X_train.loc[:, ~X_train.columns.duplicated(keep='first')]\n",
    "X_test = X_test.loc[:, ~X_test.columns.duplicated(keep='first')]\n",
    "\n",
    "NumericTransormer=Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "Preprocessor=ColumnTransformer(transformers=[('Num', NumericTransormer, NumFeatures),\n",
    "                                            ('Cat', categorical_transformer, CatFeatures)])\n",
    "\n",
    "# Fit-transform training set to create feature matrix for simple models\n",
    "XtrainP=Preprocessor.fit_transform(X_train)\n",
    "XtestP=Preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Transformed feauture shape:\", XtrainP.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35ee5d",
   "metadata": {},
   "source": [
    "6) Handle class imbalance & Model training\n",
    "We will:\n",
    "- Train a LogisticRegression baseline with class weights.\n",
    "- Train a LightGBM model (primary).\n",
    "- Optionally use SMOTE for oversampling (toggle below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f498796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 8 - optional SMOTE toggle (use with caution)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "UseSmote=False\n",
    "\n",
    "# Ensure RandomSeed is available when cells are run out of order\n",
    "RandomSeed = globals().get(RandomSeed, 42)\n",
    "\n",
    "if UseSmote:\n",
    "    print(\"Applying SMOTE to balance the classes in the training set.\")\n",
    "    Sm = SMOTE(random_state=RandomSeed, n_jobs=-1)\n",
    "    XtrainPRes, YtrainPRes = Sm.fit_resample(XtrainP, Y_train)\n",
    "else:\n",
    "    XtrainPRes, YtrainPRes = XtrainP, Y_train\n",
    "\n",
    "print(\"Post-resample shape:\", XtrainPRes.shape, \"Fraud rate:\", YtrainPRes.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 9 - Logistic Regression baseline\n",
    "Lr=LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RandomSeed)\n",
    "Lr.fit(XtrainPRes, YtrainPRes)\n",
    "ProbLr=Lr.predict_proba(XtestP)[:,1]\n",
    "print(\"Logistic Regression AUC-ROC:\", roc_auc_score(Y_test, ProbLr))\n",
    "print(classification_report(Y_test, Lr.predict(XtestP)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0616ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 10 - LightGBM model (with sklearn wrapper)\n",
    "# We'll use plain lgb.Dataset approach via sklearn wrapper for convenience.\n",
    "lgbm=lgb.LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    objective='binary',\n",
    "    class_weight='balanced',\n",
    "    random_state=RandomSeed,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgbm.fit(XtrainPRes, YtrainPRes,\n",
    "         eval_set=[(XtestP, Y_test)],\n",
    "         eval_metric='auc',\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=100)\n",
    "\n",
    "ProbLgbm=lgbm.predict_proba(XtestP)[:,1]\n",
    "print(\"LightGBM AUC-ROC:\", roc_auc_score(Y_test, ProbLgbm))\n",
    "print(classification_report(Y_test, lgbm.predict(XtestP)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a567fd",
   "metadata": {},
   "source": [
    "## 7) Evaluation visuals & business-oriented metrics\n",
    "- ROC, PR curves\n",
    "- Confusion matrix at chosen threshold\n",
    "- Precision@K / Lift if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#ROC\n",
    "fpr, tpr, _=roc_curve(Y_test, ProbLgbm)\n",
    "roc_auc=auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f'LightGBM (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "#precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(Y_test, ProbLgbm)\n",
    "ap = average_precision_score(Y_test, ProbLgbm)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(recall, precision, label=f'LightGBM (AP = {ap:.3f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bca217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 12 - Confusion matrix at a chosen threshold\n",
    "def conf_matrix_at_threshold(y_true, probs, thresh=0.5):\n",
    "    Preds = (probs >= thresh).astype(int)\n",
    "    Cm = confusion_matrix(y_true, Preds)\n",
    "    tn, fp, fn, tp = Cm.ravel()\n",
    "    return {'tn':tn,'fp':fp,'fn':fn,'tp':tp,'cm':Cm}\n",
    "\n",
    "th = 0.5\n",
    "cm_stats = conf_matrix_at_threshold(Y_test, ProbLgbm, thresh=th)\n",
    "print(\"Threshold:\", th)\n",
    "print(cm_stats)\n",
    "sns.heatmap(cm_stats['Cm'], annot=True, fmt='d', cmap='Blues', xticklabels=['NotFraud','Fraud'], yticklabels=['NotFraud','Fraud'])\n",
    "plt.title(f'Confusion matrix @ threshold={th}')\n",
    "plt.show()\n",
    "\n",
    "# Show classification report for thresholded predictions\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "preds_th = (ProbLgbm >= th).astype(int)\n",
    "print(\"Precision:\", precision_score(Y_test, preds_th))\n",
    "print(\"Recall:\", recall_score(Y_test, preds_th))\n",
    "print(\"F1:\", f1_score(Y_test, preds_th))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1084a37",
   "metadata": {},
   "source": [
    " Business metric: Expected Loss Avoided (toy example)\n",
    "- Suppose average fraud amount is `avg_fraud_amt`.\n",
    "- False negative = missed fraud (cost = amount)\n",
    "- False positive = manual review cost = e.g., 5 USD\n",
    "Compute net saved loss vs baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63683c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 13 - toy business metric\n",
    "AvgFraudAmt=df.loc[df[LabelCol]==1, 'amount'].mean() if df[LabelCol].sum() >0 else 100\n",
    "ManuelReviewCost=5.0\n",
    "\n",
    "tn, fp, fn, tp = cm_stats['tn'], cm_stats['fp'], cm_stats['fn'], cm_stats['tp']\n",
    "ExpectedLossMissed=fn*AvgFraudAmt\n",
    "ExpectedManualReviewCost=(tp*fp) *ManuelReviewCost\n",
    "print(f\"Avg fraud amount: {AvgFraudAmt:.2f}\")\n",
    "print(f\"Missed frauds (fn): {fn}, cost approx: {ExpectedLossMissed:.2f}\")\n",
    "print(f\"Manual reviews (tp+fp): {(tp+fp)}, cost approx: {ExpectedManualReviewCost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a44eb",
   "metadata": {},
   "source": [
    "8) Explainability — SHAP\n",
    "SHAP helps explain LightGBM predictions (global and single prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d1a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 14 - SHAP explainability\n",
    "# Create a small sample to speed shap runtime\n",
    "import shap\n",
    "SampleIndices=np.random.choice(X_test.shape[0], size=min(1000, X_test.shape[0]), replace=False)\n",
    "XShapSample=X_test.iloc[SampleIndices]\n",
    "XShapSampleP=Preprocessor.transform(XShapSample)\n",
    "\n",
    "# Build feature names after preprocessing\n",
    "# numeric names + ohe names\n",
    "NumNames=NumFeatures\n",
    "ohe=Preprocessor.named_transformers_['Cat'].named_steps['ohe']\n",
    "oheNames=ohe.get_feature_names_out(CatFeatures).tolist()\n",
    "FeatureNames=NumNames + oheNames\n",
    "\n",
    "# Use TreeExplainer for LightGBM\n",
    "Explainer=shap.TreeExplainer(lgbm)\n",
    "ShapValues=Explainer.shap_values(XShapSampleP)\n",
    "\n",
    "# Summary plot (global importance)\n",
    "shap.summary_plot(ShapValues, XShapSampleP, feature_names=FeatureNames, plot_type='bar', show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a11f92",
   "metadata": {},
   "source": [
    "SHAP waterfall for a single example\n",
    "Pick a high-risk transaction from the test set and show local explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: 15 - SHAP waterfall for single transaction (fixed)\n",
    "# pick the highest predicted risk transaction in the test set\n",
    "\n",
    "HighRiskIdx = np.argmax(ProbLgbm)\n",
    "\n",
    "# Use the original (preprocessed) pandas row from X_test, then transform\n",
    "SingleRow_df = X_test.iloc[[HighRiskIdx]]\n",
    "SingleRowP = Preprocessor.transform(SingleRow_df)  # shape (1, n_features)\n",
    "\n",
    "# Compute SHAP values for the single transformed row\n",
    "SingleShap = Explainer.shap_values(SingleRowP)\n",
    "\n",
    "# Handle binary-class vs single-array shap output: pick positive-class contributions\n",
    "if isinstance(SingleShap, list) and len(SingleShap) > 1:\n",
    "    shap_vals = SingleShap[1][0]  # positive class, first (and only) sample\n",
    "else:\n",
    "    # SingleShap could be array of shape (1, n) or (n,) depending on shap version\n",
    "    shap_vals = SingleShap[0] if getattr(SingleShap, 'ndim', 1) == 2 else SingleShap\n",
    "\n",
    "# Determine base value for positive class if provided as array\n",
    "ev = Explainer.expected_value\n",
    "base_value = ev[1] if (isinstance(ev, (list, np.ndarray)) and len(ev) > 1) else ev\n",
    "\n",
    "print(\"Predicted fraud probability for the highest risk transaction:\", ProbLgbm[HighRiskIdx])\n",
    "\n",
    "# Build Explanation and plot waterfall (data is the 1D feature vector)\n",
    "shap.waterfall_plot(\n",
    "    shap.Explanation(values=shap_vals,\n",
    "                     base_values=base_value,\n",
    "                     data=SingleRowP[0],\n",
    "                     feature_names=FeatureNames),\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345cdb66",
   "metadata": {},
   "source": [
    " 9) Save model & preprocessing pipeline\n",
    "Save both the trained LightGBM model and the preprocessing pipeline for later deployment (Streamlit or API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell: 16 -  Save the model and preprocessor\n",
    "OutDIr=\"models\"\n",
    "os.makedirs(OutDIr, exist_ok=True)\n",
    "\n",
    "joblib.dump(Preprocessor, os.path.join(OutDIr, \"preprocessor.joblib\"))\n",
    "joblib.dump(lgbm, os.path.join(OutDIr, \"lgbm_model.joblib\"))\n",
    "joblib.dump(Lr, os.path.join(OutDIr, \"logistic_regression_model.joblib\"))\n",
    "\n",
    "print(\"Saved preprocessor and models to ./models/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
